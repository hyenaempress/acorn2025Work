오늘 수업 테마 - 과적합(Overfitting) 방지하기 위한 학습법

1교시
Decision Tree?
- 머신러닝에서 학습법의 일종으로, 분류(Classification)과 회귀(Regression) 모두 가능한 지도 학습 모델 중 하나
	스무고개 하듯이 예/아니오 질문을 이어가며 학습한다. 
	그니까 영역 나눠놓고 둘 중 하나에 속하는걸 옳은 방식으로 학습하도록 하나봐

Decision tree에서의 무질서도(Entropy)
- 역학에서의 "무질서도"와 같은 맥락, 하지만 머신러닝에서는 "불순도"로 보고 있어
	고양이/개 판단 레이어에서 고양이가 개 바운더리 내로 포함됐을 때 엔트로피(불순도)가 증가하는거지
	구하는 공식은 간단하진 않은데 뭔지만 알고 있자

deci2_iris.py 파일은
모델 선언 부분만 바꿧어 근데도 원하는 그림 뽑는데 문제가 없엇어

2교시
# 이거 graphviz 깔아야돼 깔고 패스 설정까지 해줘야된다 핖 인스톨만으로 안된다더라
# https://www.npackd.org/p/org.graphviz.Graphviz/2.38 여기서 다운받으면 돼
# 이걸 설명하는 카페 게시글은 13번 게시글

3교시 - deci3_over.py
과적합을 해결하기 위한 교차검증방법
- gridsearchcv, kfold
https://zeuskwon-ds.tistory.com/34 여길 참고하자
머신러닝에선 언더피팅보다 오버피팅이 엄청 문제래 그걸 해결하는 그건가봐
언더피팅은 그냥 조금 더 학습시키면 된대

검증하는 순서는
1 ㅣ 2 ㅣ 3 ㅣ 4 
뎁스가 4인거에서
4번 먼저, 다음 3번 이런식으로

4교시 - deci3_over.py

5교시 - deci3_over.py

앙상블 기법이란?
- 예측력을 높이기 위해 "여러가지 학습 알고리즘이 어울려" 성능을 높이는 방식
1) Voting
- 여러 모델로부터 나온 후보 결과들 중 다수가 나온 결과를 채택하는 방식
2) Bagging
- 한 샘플링 데이터를 그냥 계속 쓰는거래
주구장창 같은 모델을 사용하지만 훈련세트에서 중복을 허용한 여러 부분 집합을 샘플링하여 학습함
3) Boosting
- 약한 개별 모델들을 연결하여 이전 모델이 생성한 가중치 
또는 오차를 기준으로 성능을 보완해 나가면서 학습하는 방법
(다만 이 성능 검증할 때 샘플의 전체를 대상으로 검증하지 않고 일부분만 검증한다)
하지만 결과적으로 완성된 알고리즘은 아주 신뢰할만한 알고리즘으로 재탄생된다
------------------------------------지피티 부연설명---------------------------------------------
[앙상블 기법(Ensemble Methods)]
→ 여러 모델을 종합하여 예측 성능을 높이는 방법

1) Voting
- 여러 다른 모델을 학습시킨 뒤, 예측 결과를 종합하여 최종 결과를 결정
- 분류(Classification): 다수결(majority voting)
- 회귀(Regression): 평균(average)

2) Bagging (Bootstrap Aggregating)
- 훈련 데이터에서 중복을 허용한 부분집합(bootstrap sample)을 여러 개 생성
- 각 부분집합마다 같은 모델(주로 Decision Tree)을 학습
- 최종적으로 결과를 평균(회귀) 또는 투표(분류)로 합침
- 과적합 방지 효과 있음 (예: Random Forest)

3) Boosting
- 여러 개의 약한 학습기(weak learner)를 순차적으로 학습
- 이전 모델이 잘못 맞춘 샘플에 더 큰 가중치를 두어 다음 모델이 보완
- 전체 데이터를 사용하되, 오차가 큰 샘플에 집중
- 대표 알고리즘: AdaBoost, Gradient Boosting, XGBoost, LightGBM
- 성능이 뛰어나지만 과적합 위험과 학습 속도 문제가 있을 수 있음

Hard Voting
- 하드 보팅은 예측한 결괏값들중 다수의 분류기가 결정한 예측값을 최종 보팅 결괏값으로 선정하는 방식이다
다수결의 원칙이라고 생각하면 된다









