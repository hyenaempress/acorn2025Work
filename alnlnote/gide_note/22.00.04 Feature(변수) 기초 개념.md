# 22.00.04 Feature(변수) 기초 개념

📍 **위치**: 22.00 통계 입문 - 데이터 분석과 AI의 기초  
🎯 **목적**: 통계 분석과 데이터 사이언스의 기본 단위인 Feature 완전 이해  
📚 **연결**: 22.01 기술통계 → 22.02 추론통계로 이어지는 기초 개념

---

## 🎯 Feature란 무엇인가?

### 📊 Feature의 정의

**Feature(피처)**는 데이터에서 관찰하고 측정할 수 있는 개별적인 특성이나 속성입니다. 통계학에서는 **변수(Variable)**라고 하며, 데이터베이스에서는 컬럼(Column), 일반적으로는 **속성(Attribute)**이라고도 합니다.

```python
# 실제 데이터 예시: 학생 정보
student_data = {
    'name': '김파이썬',        # Feature 1: 이름 (명목척도)
    'age': 25,               # Feature 2: 나이 (연속형)
    'gender': '남',          # Feature 3: 성별 (명목척도)
    'grade': 'A',            # Feature 4: 학점 (순서척도)
    'height': 175.5,         # Feature 5: 키 (연속형)
    'score': 92              # Feature 6: 점수 (연속형)
}

print("📊 하나의 관측치(행)에 6개의 Feature(열)가 있습니다")
```

### 🔤 Feature의 여러 이름들

| 분야 | 용어 | 의미 | 예시 |
|------|------|------|------|
| 통계학 | Variable (변수) | 관찰되는 특성 | 키, 몸무게, 성별 |
| 머신러닝 | Feature (피처) | 모델 입력 변수 | 온도, 습도, 풍속 |
| 데이터베이스 | Column (컬럼) | 테이블의 열 | 이름, 나이, 주소 |
| 일반 | Attribute (속성) | 객체의 특징 | 색깔, 크기, 무게 |

---

## 📏 측정 척도별 Feature 분류

### 🎯 4단계 측정 척도 (Stevens의 분류)

**명목척도(Nominal Scale) 정의**
```python
def nominal_scale_features():
    """명목척도: 분류만 가능, 순서 없음"""
    return {
        "특징": "분류만 가능, 순서 없음",
        "연산": "= ≠ (같다/다르다)",
        "예시": ["성별(남/여)", "혈액형(A/B/O/AB)", "색깔(빨강/파랑/노랑)"],
        "통계량": "최빈값, 빈도",
        "Python": "df['성별'].value_counts()"
    }
```

**순서척도(Ordinal Scale) 정의**
```python
def ordinal_scale_features():
    """순서척도: 순서 있음, 간격 불규칙"""
    return {
        "특징": "순서 있음, 간격 불규칙",
        "연산": "= ≠ > < (크다/작다)",
        "예시": ["학점(A/B/C/D/F)", "만족도(1-5점)", "소득수준(상/중/하)"],
        "통계량": "중앙값, 사분위수",
        "Python": "df['학점'].median()"
    }
```

**구간척도(Interval Scale) 정의**
```python
def interval_scale_features():
    """구간척도: 일정한 간격, 절대영점 없음"""
    return {
        "특징": "일정한 간격, 절대영점 없음",
        "연산": "= ≠ > < + - (더하기/빼기)",
        "예시": ["온도(℃)", "연도", "IQ점수"],
        "통계량": "평균, 표준편차",
        "Python": "df['온도'].mean()"
    }
```

**비율척도(Ratio Scale) 정의**
```python
def ratio_scale_features():
    """비율척도: 일정한 간격, 절대영점 있음"""
    return {
        "특징": "일정한 간격, 절대영점 있음",
        "연산": "= ≠ > < + - × ÷ (모든 연산)",
        "예시": ["키", "몸무게", "나이", "소득"],
        "통계량": "모든 통계량 가능",
        "Python": "df['키'].describe()"
    }
```

---

## 🔢 실무적 분류: 범주형 vs 수치형

### 📊 범주형(Categorical) 변수

**명목형 처리**
```python
def nominal_processing():
    """명목형: 순서가 없는 카테고리"""
    return {
        "특징": "순서가 없는 카테고리",
        "예시": ["성별", "혈액형", "거주지", "브랜드"],
        "처리방법": "더미변수, 원핫인코딩",
        "Python": "pd.get_dummies(df['성별'])"
    }
```

**순서형 처리**
```python
def ordinal_processing():
    """순서형: 순서가 있는 카테고리"""
    return {
        "특징": "순서가 있는 카테고리",
        "예시": ["학점", "만족도", "크기(S/M/L/XL)"],
        "처리방법": "라벨인코딩, 순서보존",
        "Python": "df['학점'].map({'A':4, 'B':3, 'C':2, 'D':1})"
    }
```

### 🔢 수치형(Numerical) 변수

**이산형 처리**
```python
def discrete_processing():
    """이산형: 셀 수 있는 정수값"""
    return {
        "특징": "셀 수 있는 정수값",
        "예시": ["자녀수", "방문횟수", "판매량"],
        "처리방법": "그대로 사용 또는 구간화",
        "Python": "df['자녀수'].value_counts()"
    }
```

**연속형 처리**
```python
def continuous_processing():
    """연속형: 측정 가능한 실수값"""
    return {
        "특징": "측정 가능한 실수값",
        "예시": ["키", "몸무게", "온도", "시간"],
        "처리방법": "정규화, 표준화",
        "Python": "df['키'].describe()"
    }
```

---

## 🎯 분석에서 Feature의 역할

### 📊 독립변수 vs 종속변수

**독립변수(Independent Variable) 정의**
```python
def independent_variable():
    """독립변수: 다른 변수에 영향을 주는 변수"""
    return {
        "기호": "X, 원인변수, 설명변수",
        "정의": "다른 변수에 영향을 주는 변수",
        "예시": ["광고비 → 매출", "공부시간 → 성적", "운동량 → 체중감소"],
        "특징": "모델의 입력(Input), 예측에 사용",
        "Python": "X = df[['광고비', '홍보횟수', '할인율']]"
    }
```

**종속변수(Dependent Variable) 정의**
```python
def dependent_variable():
    """종속변수: 다른 변수에 의해 영향받는 변수"""
    return {
        "기호": "Y, 결과변수, 반응변수",
        "정의": "다른 변수에 의해 영향받는 변수",
        "예시": ["매출액", "합격여부", "고객만족도"],
        "특징": "모델의 출력(Output), 예측하려는 대상",
        "Python": "y = df['매출액']"
    }
```

**통제변수(Control Variable) 정의**
```python
def control_variable():
    """통제변수: 영향을 제거하거나 일정하게 유지하는 변수"""
    return {
        "기호": "Z, 공변량",
        "정의": "영향을 제거하거나 일정하게 유지하는 변수",
        "예시": ["나이를 통제한 성별-소득 관계 분석"],
        "특징": "혼란변수 제거, 순수한 관계 파악",
        "Python": "# 나이 그룹별로 분석"
    }
```

**매개변수(Mediator) 정의**
```python
def mediator_variable():
    """매개변수: 독립변수와 종속변수 사이를 매개하는 변수"""
    return {
        "기호": "M",
        "정의": "독립변수와 종속변수 사이를 매개하는 변수",
        "예시": ["교육 → 지식습득 → 소득증가"],
        "특징": "인과관계의 메커니즘 설명",
        "Python": "# 경로분석 또는 구조방정식"
    }
```

### 🔄 변수 간 관계 유형

**선형관계 분석**
```python
def linear_relationship():
    """선형관계: 일정한 비율로 증가/감소"""
    return {
        "특징": "일정한 비율로 증가/감소",
        "예시": "키와 몸무게, 공부시간과 성적",
        "분석방법": "상관분석, 선형회귀",
        "Python": "df.corr(), sns.regplot()"
    }
```

**비선형관계 분석**
```python
def nonlinear_relationship():
    """비선형관계: 곡선형태의 관계"""
    return {
        "특징": "곡선형태의 관계",
        "예시": "나이와 학습능력, 가격과 수요",
        "분석방법": "다항회귀, 비선형회귀",
        "Python": "np.polyfit(), 다항특성 생성"
    }
```

**역관계 분석**
```python
def inverse_relationship():
    """역관계: 한 변수 증가 시 다른 변수 감소"""
    return {
        "특징": "한 변수 증가 시 다른 변수 감소",
        "예시": "가격과 수요, 온도와 난방비",
        "분석방법": "음의 상관관계 확인",
        "Python": "correlation < 0"
    }
```

**독립관계 분석**
```python
def independent_relationship():
    """독립관계: 변수 간 관련성이 없음"""
    return {
        "특징": "변수 간 관련성이 없음",
        "예시": "키와 지능, 혈액형과 성격",
        "분석방법": "상관계수 0에 가까움",
        "Python": "correlation ≈ 0"
    }
```

---

## 🛠️ Feature Engineering 기초

### 🔧 Feature Engineering 정의

Feature Engineering은 원시 데이터로부터 머신러닝 모델의 성능을 향상시킬 수 있는 새로운 Feature를 생성하거나 기존 Feature를 변환하는 과정입니다.

### 📊 Feature 생성 기법

**파생변수 생성**
```python
def derived_variables():
    """기존 변수들의 조합으로 새 변수 생성"""
    return {
        "설명": "기존 변수들의 조합으로 새 변수 생성",
        "예시": "BMI = 몸무게 / (키²)",
        "Python": "df['BMI'] = df['몸무게'] / (df['키']/100)**2"
    }
```

**집계변수 생성**
```python
def aggregated_variables():
    """그룹별 통계량으로 새 변수 생성"""
    return {
        "설명": "그룹별 통계량으로 새 변수 생성",
        "예시": "지역별 평균소득, 고객별 구매횟수",
        "Python": "df['지역평균소득'] = df.groupby('지역')['소득'].transform('mean')"
    }
```

### 🔄 Feature 변환 기법

**정규화(Normalization)**
```python
def min_max_scaling():
    """Min-Max Scaling: 0-1 범위로 스케일 조정"""
    return {
        "설명": "0-1 범위로 스케일 조정",
        "예시": "Min-Max Scaling",
        "Python": "(df['값'] - df['값'].min()) / (df['값'].max() - df['값'].min())"
    }
```

**표준화(Standardization)**
```python
def z_score_standardization():
    """Z-score 변환: 평균 0, 표준편차 1로 변환"""
    return {
        "설명": "평균 0, 표준편차 1로 변환",
        "예시": "Z-score 변환",
        "Python": "(df['값'] - df['값'].mean()) / df['값'].std()"
    }
```

**로그변환(Log Transformation)**
```python
def log_transformation():
    """로그변환: 왜곡된 분포를 정규분포에 가깝게"""
    return {
        "설명": "왜곡된 분포를 정규분포에 가깝게",
        "예시": "소득, 인구수 등 치우친 데이터",
        "Python": "np.log(df['소득'] + 1)"
    }
```

### 🔠 범주형 변수 처리

**더미변수 생성**
```python
def dummy_variables():
    """더미변수: 범주를 0/1 이진변수로 변환"""
    return {
        "설명": "범주를 0/1 이진변수로 변환",
        "예시": "성별 → 남성(0/1), 여성(0/1)",
        "Python": "pd.get_dummies(df['성별'])"
    }
```

**라벨인코딩**
```python
def label_encoding():
    """라벨인코딩: 범주를 숫자로 변환"""
    return {
        "설명": "범주를 숫자로 변환",
        "예시": "학점 A→4, B→3, C→2, D→1",
        "Python": "df['학점'].map({'A':4, 'B':3, 'C':2, 'D':1})"
    }
```

---

## 📈 Feature 중요도와 선택

### 📊 Feature 중요도 평가 방법

**상관분석**
```python
def correlation_analysis():
    """Feature 간 선형관계 측정"""
    return {
        "목적": "Feature 간 선형관계 측정",
        "범위": "-1 ≤ r ≤ 1",
        "해석": "±0.7 이상: 강한 관계, ±0.3-0.7: 보통, ±0.3 미만: 약한 관계",
        "Python": "df.corr(), sns.heatmap(df.corr())"
    }
```

**분산분석**
```python
def anova_analysis():
    """범주형 변수와 연속형 변수 관계"""
    return {
        "목적": "범주형 변수와 연속형 변수 관계",
        "통계량": "F-statistic, p-value",
        "해석": "p < 0.05면 통계적으로 유의한 관계",
        "Python": "from scipy.stats import f_oneway"
    }
```

**카이제곱검정**
```python
def chi_square_test():
    """범주형 변수 간 독립성 검정"""
    return {
        "목적": "범주형 변수 간 독립성 검정",
        "통계량": "Chi-square, p-value",
        "해석": "p < 0.05면 두 변수는 관련이 있음",
        "Python": "from scipy.stats import chi2_contingency"
    }
```

**정보이득**
```python
def information_gain():
    """분류에서 Feature의 중요도"""
    return {
        "목적": "분류에서 Feature의 중요도",
        "개념": "엔트로피 감소량",
        "활용": "의사결정트리에서 분할 기준",
        "Python": "sklearn.feature_selection.mutual_info_classif"
    }
```

---

## 💡 실무에서 Feature 다루기

### 🔍 Feature 탐색 및 이해

**기본 정보 파악**
```python
# 데이터 기본 정보
print("📊 데이터 기본 정보")
print(f"데이터 크기: {df.shape}")
print(f"결측치: {df.isnull().sum().sum()}개")

# Feature 타입 확인
print("\n🔢 Feature 타입별 분류")
print("수치형 Feature:", df.select_dtypes(include=[np.number]).columns.tolist())
print("범주형 Feature:", df.select_dtypes(include=['object']).columns.tolist())

# 기술통계량 확인
print("\n📈 수치형 Feature 기술통계량")
print(df.describe())

# 범주형 Feature 분포 확인
for col in df.select_dtypes(include=['object']).columns:
    print(f"\n📊 {col} 분포:")
    print(df[col].value_counts())

# 상관관계 분석
print("\n🔄 Feature 간 상관관계")
correlation_matrix = df.corr()
print(correlation_matrix)
```

**시각화 탐색**
```python
import matplotlib.pyplot as plt
import seaborn as sns

# 히스토그램
df.hist(figsize=(15, 10))
plt.show()

# 상관관계 히트맵
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.show()
```

---

## ⚠️ Feature 사용 시 주의사항

### 🚨 주요 문제점과 해결책

**다중공선성(Multicollinearity)**
```python
def multicollinearity_check():
    """독립변수들 간 높은 상관관계 문제"""
    return {
        "문제": "독립변수들 간 높은 상관관계",
        "영향": "회귀계수 불안정, 해석 어려움",
        "해결": "VIF 확인, 주성분분석, Feature 제거",
        "기준": "VIF > 10이면 다중공선성 의심"
    }
```

**데이터 누출(Data Leakage)**
```python
def data_leakage_prevention():
    """미래 정보가 현재 예측에 포함되는 문제"""
    return {
        "문제": "미래 정보가 현재 예측에 포함",
        "예시": "내일 주가로 오늘 주가 예측",
        "영향": "과적합, 실제 성능 저하",
        "방지": "시간 순서 고려, 전처리 순서 주의"
    }
```

**차원의 저주(Curse of Dimensionality)**
```python
def curse_of_dimensionality():
    """Feature 수가 데이터 수보다 많은 문제"""
    return {
        "문제": "Feature 수가 데이터 수보다 많음",
        "영향": "과적합, 계산 복잡도 증가",
        "해결": "Feature 선택, 차원 축소",
        "경험칙": "Feature 수 < 데이터 수 / 10"
    }
```

**결측치 처리**
```python
def missing_values_handling():
    """빈 값이 있는 Feature 처리"""
    return {
        "문제": "빈 값이 있는 Feature",
        "방법": "제거, 대체(평균/중앙값/최빈값), 예측",
        "주의": "무작정 제거하면 정보 손실",
        "원칙": "결측 패턴 먼저 분석"
    }
```

---

## 📊 Feature와 통계 분석의 연결

### 🔗 Feature 타입별 적합한 통계 기법

#### 단일 Feature 분석

**범주형 Feature 분석**
```python
def categorical_single_analysis():
    """범주형 변수 단일 분석"""
    return {
        "기법": "빈도표, 막대그래프, 파이차트",
        "통계량": "최빈값, 엔트로피",
        "Python": "df['변수'].value_counts(), plt.bar()"
    }
```

**수치형 Feature 분석**
```python
def numerical_single_analysis():
    """수치형 변수 단일 분석"""
    return {
        "기법": "히스토그램, 상자그림, 기술통계량",
        "통계량": "평균, 중앙값, 표준편차, 사분위수",
        "Python": "df['변수'].describe(), plt.hist()"
    }
```

#### 두 Feature 관계 분석

**범주형 vs 범주형**
```python
def categorical_vs_categorical():
    """두 범주형 변수 관계 분석"""
    return {
        "기법": "교차표, 카이제곱검정",
        "통계량": "카이제곱 통계량, p-value",
        "Python": "pd.crosstab(), chi2_contingency()"
    }
```

**범주형 vs 수치형**
```python
def categorical_vs_numerical():
    """범주형과 수치형 변수 관계 분석"""
    return {
        "기법": "그룹별 평균 비교, t검정, ANOVA",
        "통계량": "그룹별 평균, F통계량",
        "Python": "df.groupby('범주')['수치'].mean(), ttest_ind()"
    }
```

**수치형 vs 수치형**
```python
def numerical_vs_numerical():
    """두 수치형 변수 관계 분석"""
    return {
        "기법": "산점도, 상관분석, 회귀분석",
        "통계량": "상관계수, 회귀계수",
        "Python": "df.corr(), sns.scatterplot(), linregress()"
    }
```

---

## 🎯 다음 학습 단계

### 📚 22.01 기술통계로의 연결

Feature의 기본 개념을 이해했다면, 이제 22.01 기술통계에서 각 Feature의 분포와 특성을 자세히 분석하는 방법을 배우게 됩니다:

#### 학습 경로
- **22.01 기술통계**: Feature별 기술통계량 계산, 분포의 모양과 특성 파악, 이상치 탐지와 처리, Feature 간 관계 시각화
- **22.02 추론통계**: 표본 Feature로 모집단 추론, Feature 간 관계 가설검정, 신뢰구간과 예측구간, 통계적 유의성 판단
- **고급 분석**: 다변량 통계 분석, 머신러닝 Feature 중요도, 차원 축소 기법, Feature 선택 알고리즘

---

## 📝 핵심 정리

### 🎯 Feature 완전 정복 체크리스트

#### ✅ 기본 개념
- [ ] Feature = Variable = Column = Attribute 이해
- [ ] 독립변수와 종속변수 구분
- [ ] 4단계 측정 척도 (명목, 순서, 구간, 비율) 숙지

#### ✅ 타입 분류
- [ ] 범주형(명목/순서) vs 수치형(이산/연속) 판별
- [ ] 각 타입별 적절한 통계기법 선택
- [ ] Python으로 Feature 타입 확인

#### ✅ 실무 활용
- [ ] Feature Engineering 기본 기법 적용
- [ ] Feature 중요도 평가 방법 이해
- [ ] 다중공선성, 데이터 누출 등 주의사항 숙지

#### ✅ 통계 연결
- [ ] Feature 타입별 적합한 분석 방법 선택
- [ ] 시각화 기법과 통계량 적절히 활용
- [ ] 분석 결과의 해석과 의미 도출

---

## 💡 핵심 메시지

🏆 **모든 항목을 체크했다면 Feature 마스터!**  
🚀 **이제 22.01 기술통계로 넘어갈 준비 완료!**

**Feature는 데이터 분석의 알파벳과 같습니다. 이 기초를 탄탄히 다졌으니, 어떤 복잡한 통계 분석도 자신 있게 도전할 수 있을 것입니다!** 🎯✨
