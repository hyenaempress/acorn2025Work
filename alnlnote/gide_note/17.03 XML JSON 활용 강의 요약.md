# 웹 스크래핑과 데이터 처리 강의 요약

## 📍 1. 웹 스크래핑 기본 개념

### 🔧 필수 라이브러리
```python
import requests
import urllib.request
from bs4 import BeautifulSoup
import pandas as pd
import time
```

### ⚠️ 스크래핑 주의사항
1. **속도 제한**: 너무 빠르게 요청하면 IP 차단 가능
   - 0.5초 이상 간격으로 요청
   - `time.sleep(0.5)` 사용

2. **예외 처리**: 네트워크 작업은 반드시 try-except 사용
```python
try:
    response = requests.get(url)
    response.raise_for_status()
except Exception as e:
    print(f"오류 발생: {e}")
```

3. **사이트 구조 변경**: 스크래핑 코드는 언제든 작동하지 않을 수 있음

---

## 📍 2. BeautifulSoup 활용

### 🎯 HTML 파싱 예제
```python
# 웹페이지 요청
url = "https://example.com"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# CSS 선택자 사용
items = soup.select('dl.txt dt')  # 클래스가 txt인 dl 하위의 dt 태그
prices = soup.select('p.money strong')  # 가격 정보 추출

# 텍스트 추출 및 정제
menu_names = [tag.text.strip() for tag in items]
```

### 🔍 DOM 구조 이해
- **부모-자식 관계**: `parent` → `child`
- **형제 관계**: `sibling`
- **루트 엘리먼트**: 하나만 존재

---

## 📍 3. 실습 예제: 치킨 메뉴 스크래핑

### 📊 교촌치킨 메뉴 및 가격 수집
```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

# 웹페이지 요청
url = "교촌치킨 메뉴 페이지"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# 메뉴명 추출
menu_items = soup.select('dl.txt dt')
menu_names = [item.text.strip() for item in menu_items]

# 가격 추출 및 정제
price_items = soup.select('p.money strong')
prices = []
for price in price_items:
    # 쉼표 제거하고 숫자만 추출
    clean_price = price.text.replace(',', '').replace('원', '')
    prices.append(int(clean_price))

# 데이터프레임 생성
df = pd.DataFrame({
    '상품명': menu_names,
    '가격': prices
})

# 기본 통계 계산
print(f"평균 가격: {df['가격'].mean():.2f}원")
print(f"표준편차: {df['가격'].std():.2f}원")
print(f"최저가: {df['가격'].min()}원")
print(f"최고가: {df['가격'].max()}원")
```

---

## 📍 4. XML 데이터 처리

### 🌡️ 기상청 날씨 데이터 예제
```python
import xml.etree.ElementTree as ET
import requests
from bs4 import BeautifulSoup

# XML 파일 읽기
with open('my.xml', 'r', encoding='utf-8') as f:
    soup = BeautifulSoup(f.read(), 'xml')

# 아이템 요소 찾기
items = soup.find_all('item')

# 데이터 추출
data = []
for item in items:
    name = item.find('name').text
    tel = item.find('tel').text
    exam = item.find('exam').text
    data.append([name, tel, exam])

# 데이터프레임으로 변환
df = pd.DataFrame(data, columns=['이름', '전화번호', '시험'])
```

### 🔄 XML의 특징
- 부모-자식 구조가 명확
- JSON보다 복잡하지만 구조화된 데이터에 적합
- 요즘은 JSON을 더 선호하는 추세

---

## 📍 5. JSON 데이터 처리

### 🔄 JSON 인코딩/디코딩
```python
import json

# 딕셔너리를 JSON 문자열로 변환 (인코딩)
data = {'name': 'tom', 'age': 33, 'score': [90, 80, 100]}
json_string = json.dumps(data)
print(type(json_string))  # <class 'str'>

# JSON 문자열을 딕셔너리로 변환 (디코딩)
json_data = json.loads(json_string)
print(type(json_data))  # <class 'dict'>
print(json_data['name'])  # 'tom'
```

### 🌐 웹에서 JSON 데이터 읽기
```python
import urllib.request
import json

# 서울시 도서관 데이터 예제
url = "https://raw.githubusercontent.com/pykwon/python/master/seoullibtime5.json"

# 데이터 읽기
response = urllib.request.urlopen(url)
plain_text = response.read().decode('utf-8')

# JSON으로 변환
json_data = json.loads(plain_text)

# 데이터 추출
lib_data = json_data['seoulLibraryTime']['row']
for lib in lib_data:
    print(lib['LBRRY_NAME'], lib['TEL'], lib['ADRES'])

# 데이터프레임으로 변환
df = pd.DataFrame(lib_data, columns=['LBRRY_NAME', 'TEL', 'ADRES'])
```

---

## 📍 6. 데이터 저장 방법

### 💾 다양한 저장 형식
```python
# CSV 저장
df.to_csv('weather.csv', index=False, encoding='utf-8')

# Excel 저장
df.to_excel('weather.xlsx', index=False)

# JSON 저장
df.to_json('weather.json', orient='records')

# 데이터베이스 저장
df.to_sql('weather_table', connection, if_exists='replace')
```

---

## 📍 7. 데이터 분석 워크플로우

### 🔄 완전한 프로세스
```python
# 1. 데이터 수집 (웹 스크래핑)
url = "데이터 소스 URL"
soup = BeautifulSoup(requests.get(url).text, 'html.parser')

# 2. 데이터 추출 및 정제
raw_data = soup.select('원하는 선택자')
clean_data = [item.text.strip() for item in raw_data]

# 3. 데이터프레임 생성
df = pd.DataFrame(clean_data, columns=['컬럼명'])

# 4. 기본 분석
print(df.info())
print(df.describe())

# 5. 조건 필터링
filtered_df = df[df['온도'] >= 30]

# 6. 정렬
sorted_df = df.sort_values(by='온도', ascending=False)

# 7. 결과 저장
df.to_csv('result.csv', index=False)
```

---

## 🎯 핵심 메시지

### 💡 강의의 핵심 포인트
1. **웹에서 데이터를 가져오는 것이 목표**
2. **최종적으로는 모든 데이터를 pandas DataFrame으로 변환**
3. **DataFrame에만 넣으면 pandas의 모든 기능 활용 가능**
4. **데이터 수집(ETL)이 데이터 분석에서 가장 시간과 비용이 많이 드는 부분**

### 🔮 다음 단계
- **시각화**: matplotlib을 이용한 데이터 시각화
- **고급 분석**: 통계 분석, 머신러닝
- **웹 개발**: Django를 이용한 웹 애플리케이션 구축

### 🚀 실무 적용
- 실시간 데이터 수집 시스템 구축
- 정기적인 데이터 모니터링
- 자동화된 리포트 생성
- 비즈니스 인사이트 도출

---

## ⚡ 요약

이 강의는 **웹 스크래핑 → 데이터 정제 → pandas 처리 → 분석/저장**의 완전한 데이터 파이프라인을 다루며, 실무에서 바로 활용할 수 있는 실전 기술을 제공합니다.
