# 웹 크롤링과 pandas 파일 저장/읽기 실습 정리

## 📋 개요
BeautifulSoup를 이용한 웹 스크래핑과 pandas의 파일 I/O 기능을 결합한 실무 중심의 데이터 수집 및 처리 방법을 다룹니다.

---

## 🔧 필수 라이브러리 설치 및 import

```python
# 웹 크롤링용 라이브러리
import requests
import urllib.request as req
from bs4 import BeautifulSoup

# 데이터 처리용 라이브러리
import pandas as pd
import csv
import re

# 정규표현식 라이브러리 (데이터 정제용)
import re
```

---

## 🌐 웹 스크래핑 기본 구조

### 1. 기본 웹 페이지 읽기 (위키백과 예제)

```python
# 위키백과 이순신 페이지 읽기
url = "https://ko.wikipedia.org/wiki/이순신"

# 방법 1: urllib 사용
wiki = req.urlopen(url)
soup = BeautifulSoup(wiki, "html.parser")

# 방법 2: requests 사용 (권장)
response = requests.get(url)
response.raise_for_status()  # 오류 발생 시 예외 처리
soup = BeautifulSoup(response.text, "html.parser")

# 특정 내용 추출 (CSS 셀렉터 사용)
content = soup.select("#mw-content-text > div.mw-parser-output > p")
for p in content:
    print(p.get_text())
```

### 2. 선택자(Selector) 활용

```python
# CSS 선택자 vs find 메서드
# CSS 선택자 방식 (권장)
rows = soup.select('table.type_2 tbody tr')

# find 방식
table = soup.find('table', attrs={'class': 'type_2'})
tbody = table.find('tbody')
rows = tbody.find_all('tr')

# 두 방식 모두 유효하며, 상황에 따라 선택 사용
```

---

## 📈 네이버 코스피 데이터 수집 실습

### 1. 프로젝트 설정

```python
# 기본 설정
url_template = "https://finance.naver.com/sise/sise_market_sum.naver?page={}"
csv_filename = '네이버코스피.csv'

# CSV 파일 헤더 정의
headers = ['종목명', '현재가', '전일비', '등락률', '거래량', 
           '매수호가', '매도호가', '시가총액', '상장주식수', '외국인비율', 'PER', 'ROE']
```

### 2. CSV 파일 생성 및 헤더 작성

```python
# CSV 파일 생성 및 헤더 작성
with open(csv_filename, 'w', encoding='utf-8', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(headers)
```

### 3. 다중 페이지 데이터 수집

```python
# 여러 페이지 데이터 수집 (1~3페이지)
with open(csv_filename, 'a', encoding='utf-8', newline='') as f:
    writer = csv.writer(f)
    
    for page in range(1, 4):  # 1페이지부터 3페이지까지
        url = url_template.format(page)
        print(f"페이지 {page} 처리 중: {url}")
        
        # 웹 페이지 요청
        response = requests.get(url)
        response.raise_for_status()
        
        # BeautifulSoup 객체 생성
        soup = BeautifulSoup(response.text, "html.parser")
        
        # 테이블 데이터 추출
        rows = soup.select('table.type_2 tbody tr')
        
        for row in rows:
            cols = row.find_all('td')
            
            # 데이터가 충분한 행만 처리
            if len(cols) < len(headers):
                print(f"[스킵됨] 열 수 부족: {len(cols)} < {len(headers)}")
                continue
            
            # 데이터 정제 (공백, 줄바꿈 제거)
            row_data = []
            for col in cols:
                text = re.sub(r'[\n\t]+', '', col.get_text().strip())
                row_data.append(text)
            
            writer.writerow(row_data)

print('CSV 저장 완료!')
```

---

## 🔍 데이터 정제 및 정규표현식 활용

### 정규표현식을 이용한 텍스트 정제

```python
# 정규표현식 패턴
import re

def clean_text(text):
    """텍스트 데이터 정제 함수"""
    # 줄바꿈, 탭 문자 제거
    text = re.sub(r'[\n\t]+', '', text)
    
    # 앞뒤 공백 제거
    text = text.strip()
    
    # 연속된 공백을 하나로 변경
    text = re.sub(r'\s+', ' ', text)
    
    return text

# 사용 예시
for col in cols:
    cleaned_text = clean_text(col.get_text())
    row_data.append(cleaned_text)
```

---

## 📊 pandas를 이용한 데이터 처리

### 1. 기본 pandas 파일 저장/읽기

```python
import pandas as pd
import numpy as np

# 딕셔너리에서 DataFrame 생성
items = {
    'apple': {'count': 10, 'price': 1500},
    'orange': {'count': 4, 'price': 700},
}

df = pd.DataFrame(items)
print(df)
```

### 2. 다양한 저장 형식

```python
# 1. CSV 저장 (기본)
df.to_csv('result.csv', sep=',', index=False)

# 2. CSV 저장 (헤더 없이)
df.to_csv('result.csv', sep=',', index=False, header=False)

# 3. Excel 저장
df.to_excel('result.xlsx', index=False, sheet_name='Sheet1')

# 4. 전치(Transpose) 후 저장
data_transposed = df.T
data_transposed.to_csv('result_transposed.csv', sep=',', index=False)

# 5. JSON 저장
print(df.to_json())  # 기본 형태
print(df.to_json(orient='records'))  # 레코드 형태

# 6. HTML 저장
print(df.to_html())
df.to_html('table.html', index=False)

# 7. 클립보드로 복사
df.to_clipboard(index=False)
```

### 3. 파일 읽기

```python
# CSV 읽기
df_read = pd.read_csv('result.csv')
print(df_read)

# Excel 읽기
df_excel = pd.read_excel('result.xlsx')
print(df_excel)

# 웹에서 직접 테이블 읽기
url = "https://ko.wikipedia.org/wiki/이순신"
tables = pd.read_html(url)
print(f"총 {len(tables)}개의 테이블 발견")
```

---

## ⚠️ 웹 크롤링 주의사항

### 1. 법적/윤리적 고려사항

```python
# robots.txt 확인
# https://example.com/robots.txt

# 요청 간격 조절 (서버 부하 방지)
import time

for page in range(1, 10):
    # 데이터 수집 코드
    time.sleep(1)  # 1초 대기
```

### 2. 예외 처리

```python
import requests
from requests.exceptions import RequestException

def safe_request(url, max_retries=3):
    """안전한 웹 요청 함수"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response
        except RequestException as e:
            print(f"요청 실패 (시도 {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)  # 재시도 전 대기
            else:
                raise
    return None
```

### 3. 구조 변경 대응

```python
def verify_page_structure(soup):
    """페이지 구조 확인 함수"""
    target_table = soup.select('table.type_2')
    if not target_table:
        print("경고: 예상된 테이블 구조를 찾을 수 없습니다.")
        return False
    return True

# 사용 예시
if verify_page_structure(soup):
    # 데이터 추출 진행
    rows = soup.select('table.type_2 tbody tr')
else:
    print("페이지 구조가 변경되었습니다. 코드를 확인하세요.")
```

---

## 🚀 완전한 실습 예제

```python
import pandas as pd
import requests
from bs4 import BeautifulSoup
import csv
import re
import time

def naver_stock_crawler():
    """네이버 코스피 데이터 크롤링 함수"""
    
    # 설정
    url_template = "https://finance.naver.com/sise/sise_market_sum.naver?page={}"
    csv_filename = '네이버코스피.csv'
    headers = ['종목명', '현재가', '전일비', '등락률', '거래량', 
               '매수호가', '매도호가', '시가총액']
    
    # CSV 파일 초기화
    with open(csv_filename, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(headers)
    
    # 데이터 수집
    with open(csv_filename, 'a', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        
        for page in range(1, 4):  # 1-3페이지
            try:
                url = url_template.format(page)
                print(f"페이지 {page} 처리 중...")
                
                response = requests.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, "html.parser")
                rows = soup.select('table.type_2 tbody tr')
                
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) >= len(headers):
                        row_data = [re.sub(r'[\n\t\s]+', ' ', col.get_text()).strip() 
                                   for col in cols[:len(headers)]]
                        writer.writerow(row_data)
                
                time.sleep(1)  # 서버 부하 방지
                
            except Exception as e:
                print(f"페이지 {page} 처리 중 오류: {e}")
    
    # pandas로 데이터 확인
    df = pd.read_csv(csv_filename)
    print(f"\n수집 완료: {len(df)}개 종목")
    print(df.head())
    
    return df

# 실행
if __name__ == "__main__":
    df = naver_stock_crawler()
```

---

## 💡 실무 팁

### 1. 크롤링 vs 스크래핑
- **크롤링(Crawling)**: 여러 페이지를 자동으로 순회하며 데이터 수집
- **스크래핑(Scraping)**: 특정 페이지에서 원하는 데이터만 추출

### 2. 파서 종류별 특징
```python
# HTML 파서 종류
parsers = {
    'html.parser': '기본 내장, 빠름',
    'lxml': '가장 빠름, 외부 라이브러리 필요',
    'xml': 'XML 전용',
    'html5lib': '가장 정확, 느림'
}

# 사용 예시
soup = BeautifulSoup(html_content, "lxml")  # 가장 빠른 파서
```

### 3. 데이터 품질 관리
```python
def validate_data(df):
    """데이터 품질 검사"""
    print(f"총 행 수: {len(df)}")
    print(f"결측값: {df.isnull().sum().sum()}")
    print(f"중복값: {df.duplicated().sum()}")
    
    # 숫자 컬럼 타입 확인
    numeric_cols = ['현재가', '거래량']
    for col in numeric_cols:
        if col in df.columns:
            print(f"{col} 데이터 타입: {df[col].dtype}")

# 사용
df = pd.read_csv('네이버코스피.csv')
validate_data(df)
```

이 실습을 통해 웹에서 데이터를 수집하고 pandas로 효율적으로 처리하는 전체 워크플로우를 학습할 수 있습니다! 🎯