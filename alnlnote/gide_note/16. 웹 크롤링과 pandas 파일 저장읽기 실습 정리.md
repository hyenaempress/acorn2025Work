# ì›¹ í¬ë¡¤ë§ê³¼ pandas íŒŒì¼ ì €ì¥/ì½ê¸° ì‹¤ìŠµ ì •ë¦¬

## ğŸ“‹ ê°œìš”
BeautifulSoupë¥¼ ì´ìš©í•œ ì›¹ ìŠ¤í¬ë˜í•‘ê³¼ pandasì˜ íŒŒì¼ I/O ê¸°ëŠ¥ì„ ê²°í•©í•œ ì‹¤ë¬´ ì¤‘ì‹¬ì˜ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

---

## ğŸ”§ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° import

```python
# ì›¹ í¬ë¡¤ë§ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests
import urllib.request as req
from bs4 import BeautifulSoup

# ë°ì´í„° ì²˜ë¦¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import csv
import re

# ì •ê·œí‘œí˜„ì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ë°ì´í„° ì •ì œìš©)
import re
```

---

## ğŸŒ ì›¹ ìŠ¤í¬ë˜í•‘ ê¸°ë³¸ êµ¬ì¡°

### 1. ê¸°ë³¸ ì›¹ í˜ì´ì§€ ì½ê¸° (ìœ„í‚¤ë°±ê³¼ ì˜ˆì œ)

```python
# ìœ„í‚¤ë°±ê³¼ ì´ìˆœì‹  í˜ì´ì§€ ì½ê¸°
url = "https://ko.wikipedia.org/wiki/ì´ìˆœì‹ "

# ë°©ë²• 1: urllib ì‚¬ìš©
wiki = req.urlopen(url)
soup = BeautifulSoup(wiki, "html.parser")

# ë°©ë²• 2: requests ì‚¬ìš© (ê¶Œì¥)
response = requests.get(url)
response.raise_for_status()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
soup = BeautifulSoup(response.text, "html.parser")

# íŠ¹ì • ë‚´ìš© ì¶”ì¶œ (CSS ì…€ë ‰í„° ì‚¬ìš©)
content = soup.select("#mw-content-text > div.mw-parser-output > p")
for p in content:
    print(p.get_text())
```

### 2. ì„ íƒì(Selector) í™œìš©

```python
# CSS ì„ íƒì vs find ë©”ì„œë“œ
# CSS ì„ íƒì ë°©ì‹ (ê¶Œì¥)
rows = soup.select('table.type_2 tbody tr')

# find ë°©ì‹
table = soup.find('table', attrs={'class': 'type_2'})
tbody = table.find('tbody')
rows = tbody.find_all('tr')

# ë‘ ë°©ì‹ ëª¨ë‘ ìœ íš¨í•˜ë©°, ìƒí™©ì— ë”°ë¼ ì„ íƒ ì‚¬ìš©
```

---

## ğŸ“ˆ ë„¤ì´ë²„ ì½”ìŠ¤í”¼ ë°ì´í„° ìˆ˜ì§‘ ì‹¤ìŠµ

### 1. í”„ë¡œì íŠ¸ ì„¤ì •

```python
# ê¸°ë³¸ ì„¤ì •
url_template = "https://finance.naver.com/sise/sise_market_sum.naver?page={}"
csv_filename = 'ë„¤ì´ë²„ì½”ìŠ¤í”¼.csv'

# CSV íŒŒì¼ í—¤ë” ì •ì˜
headers = ['ì¢…ëª©ëª…', 'í˜„ì¬ê°€', 'ì „ì¼ë¹„', 'ë“±ë½ë¥ ', 'ê±°ë˜ëŸ‰', 
           'ë§¤ìˆ˜í˜¸ê°€', 'ë§¤ë„í˜¸ê°€', 'ì‹œê°€ì´ì•¡', 'ìƒì¥ì£¼ì‹ìˆ˜', 'ì™¸êµ­ì¸ë¹„ìœ¨', 'PER', 'ROE']
```

### 2. CSV íŒŒì¼ ìƒì„± ë° í—¤ë” ì‘ì„±

```python
# CSV íŒŒì¼ ìƒì„± ë° í—¤ë” ì‘ì„±
with open(csv_filename, 'w', encoding='utf-8', newline='') as f:
    writer = csv.writer(f)
    writer.writerow(headers)
```

### 3. ë‹¤ì¤‘ í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘

```python
# ì—¬ëŸ¬ í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘ (1~3í˜ì´ì§€)
with open(csv_filename, 'a', encoding='utf-8', newline='') as f:
    writer = csv.writer(f)
    
    for page in range(1, 4):  # 1í˜ì´ì§€ë¶€í„° 3í˜ì´ì§€ê¹Œì§€
        url = url_template.format(page)
        print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘: {url}")
        
        # ì›¹ í˜ì´ì§€ ìš”ì²­
        response = requests.get(url)
        response.raise_for_status()
        
        # BeautifulSoup ê°ì²´ ìƒì„±
        soup = BeautifulSoup(response.text, "html.parser")
        
        # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
        rows = soup.select('table.type_2 tbody tr')
        
        for row in rows:
            cols = row.find_all('td')
            
            # ë°ì´í„°ê°€ ì¶©ë¶„í•œ í–‰ë§Œ ì²˜ë¦¬
            if len(cols) < len(headers):
                print(f"[ìŠ¤í‚µë¨] ì—´ ìˆ˜ ë¶€ì¡±: {len(cols)} < {len(headers)}")
                continue
            
            # ë°ì´í„° ì •ì œ (ê³µë°±, ì¤„ë°”ê¿ˆ ì œê±°)
            row_data = []
            for col in cols:
                text = re.sub(r'[\n\t]+', '', col.get_text().strip())
                row_data.append(text)
            
            writer.writerow(row_data)

print('CSV ì €ì¥ ì™„ë£Œ!')
```

---

## ğŸ” ë°ì´í„° ì •ì œ ë° ì •ê·œí‘œí˜„ì‹ í™œìš©

### ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ì •ì œ

```python
# ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
import re

def clean_text(text):
    """í…ìŠ¤íŠ¸ ë°ì´í„° ì •ì œ í•¨ìˆ˜"""
    # ì¤„ë°”ê¿ˆ, íƒ­ ë¬¸ì ì œê±°
    text = re.sub(r'[\n\t]+', '', text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€ê²½
    text = re.sub(r'\s+', ' ', text)
    
    return text

# ì‚¬ìš© ì˜ˆì‹œ
for col in cols:
    cleaned_text = clean_text(col.get_text())
    row_data.append(cleaned_text)
```

---

## ğŸ“Š pandasë¥¼ ì´ìš©í•œ ë°ì´í„° ì²˜ë¦¬

### 1. ê¸°ë³¸ pandas íŒŒì¼ ì €ì¥/ì½ê¸°

```python
import pandas as pd
import numpy as np

# ë”•ì…”ë„ˆë¦¬ì—ì„œ DataFrame ìƒì„±
items = {
    'apple': {'count': 10, 'price': 1500},
    'orange': {'count': 4, 'price': 700},
}

df = pd.DataFrame(items)
print(df)
```

### 2. ë‹¤ì–‘í•œ ì €ì¥ í˜•ì‹

```python
# 1. CSV ì €ì¥ (ê¸°ë³¸)
df.to_csv('result.csv', sep=',', index=False)

# 2. CSV ì €ì¥ (í—¤ë” ì—†ì´)
df.to_csv('result.csv', sep=',', index=False, header=False)

# 3. Excel ì €ì¥
df.to_excel('result.xlsx', index=False, sheet_name='Sheet1')

# 4. ì „ì¹˜(Transpose) í›„ ì €ì¥
data_transposed = df.T
data_transposed.to_csv('result_transposed.csv', sep=',', index=False)

# 5. JSON ì €ì¥
print(df.to_json())  # ê¸°ë³¸ í˜•íƒœ
print(df.to_json(orient='records'))  # ë ˆì½”ë“œ í˜•íƒœ

# 6. HTML ì €ì¥
print(df.to_html())
df.to_html('table.html', index=False)

# 7. í´ë¦½ë³´ë“œë¡œ ë³µì‚¬
df.to_clipboard(index=False)
```

### 3. íŒŒì¼ ì½ê¸°

```python
# CSV ì½ê¸°
df_read = pd.read_csv('result.csv')
print(df_read)

# Excel ì½ê¸°
df_excel = pd.read_excel('result.xlsx')
print(df_excel)

# ì›¹ì—ì„œ ì§ì ‘ í…Œì´ë¸” ì½ê¸°
url = "https://ko.wikipedia.org/wiki/ì´ìˆœì‹ "
tables = pd.read_html(url)
print(f"ì´ {len(tables)}ê°œì˜ í…Œì´ë¸” ë°œê²¬")
```

---

## âš ï¸ ì›¹ í¬ë¡¤ë§ ì£¼ì˜ì‚¬í•­

### 1. ë²•ì /ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­

```python
# robots.txt í™•ì¸
# https://example.com/robots.txt

# ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
import time

for page in range(1, 10):
    # ë°ì´í„° ìˆ˜ì§‘ ì½”ë“œ
    time.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
```

### 2. ì˜ˆì™¸ ì²˜ë¦¬

```python
import requests
from requests.exceptions import RequestException

def safe_request(url, max_retries=3):
    """ì•ˆì „í•œ ì›¹ ìš”ì²­ í•¨ìˆ˜"""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            return response
        except RequestException as e:
            print(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            else:
                raise
    return None
```

### 3. êµ¬ì¡° ë³€ê²½ ëŒ€ì‘

```python
def verify_page_structure(soup):
    """í˜ì´ì§€ êµ¬ì¡° í™•ì¸ í•¨ìˆ˜"""
    target_table = soup.select('table.type_2')
    if not target_table:
        print("ê²½ê³ : ì˜ˆìƒëœ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    return True

# ì‚¬ìš© ì˜ˆì‹œ
if verify_page_structure(soup):
    # ë°ì´í„° ì¶”ì¶œ ì§„í–‰
    rows = soup.select('table.type_2 tbody tr')
else:
    print("í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. ì½”ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
```

---

## ğŸš€ ì™„ì „í•œ ì‹¤ìŠµ ì˜ˆì œ

```python
import pandas as pd
import requests
from bs4 import BeautifulSoup
import csv
import re
import time

def naver_stock_crawler():
    """ë„¤ì´ë²„ ì½”ìŠ¤í”¼ ë°ì´í„° í¬ë¡¤ë§ í•¨ìˆ˜"""
    
    # ì„¤ì •
    url_template = "https://finance.naver.com/sise/sise_market_sum.naver?page={}"
    csv_filename = 'ë„¤ì´ë²„ì½”ìŠ¤í”¼.csv'
    headers = ['ì¢…ëª©ëª…', 'í˜„ì¬ê°€', 'ì „ì¼ë¹„', 'ë“±ë½ë¥ ', 'ê±°ë˜ëŸ‰', 
               'ë§¤ìˆ˜í˜¸ê°€', 'ë§¤ë„í˜¸ê°€', 'ì‹œê°€ì´ì•¡']
    
    # CSV íŒŒì¼ ì´ˆê¸°í™”
    with open(csv_filename, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(headers)
    
    # ë°ì´í„° ìˆ˜ì§‘
    with open(csv_filename, 'a', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        
        for page in range(1, 4):  # 1-3í˜ì´ì§€
            try:
                url = url_template.format(page)
                print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
                
                response = requests.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, "html.parser")
                rows = soup.select('table.type_2 tbody tr')
                
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) >= len(headers):
                        row_data = [re.sub(r'[\n\t\s]+', ' ', col.get_text()).strip() 
                                   for col in cols[:len(headers)]]
                        writer.writerow(row_data)
                
                time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                
            except Exception as e:
                print(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # pandasë¡œ ë°ì´í„° í™•ì¸
    df = pd.read_csv(csv_filename)
    print(f"\nìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê°œ ì¢…ëª©")
    print(df.head())
    
    return df

# ì‹¤í–‰
if __name__ == "__main__":
    df = naver_stock_crawler()
```

---

## ğŸ’¡ ì‹¤ë¬´ íŒ

### 1. í¬ë¡¤ë§ vs ìŠ¤í¬ë˜í•‘
- **í¬ë¡¤ë§(Crawling)**: ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ìë™ìœ¼ë¡œ ìˆœíšŒí•˜ë©° ë°ì´í„° ìˆ˜ì§‘
- **ìŠ¤í¬ë˜í•‘(Scraping)**: íŠ¹ì • í˜ì´ì§€ì—ì„œ ì›í•˜ëŠ” ë°ì´í„°ë§Œ ì¶”ì¶œ

### 2. íŒŒì„œ ì¢…ë¥˜ë³„ íŠ¹ì§•
```python
# HTML íŒŒì„œ ì¢…ë¥˜
parsers = {
    'html.parser': 'ê¸°ë³¸ ë‚´ì¥, ë¹ ë¦„',
    'lxml': 'ê°€ì¥ ë¹ ë¦„, ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”',
    'xml': 'XML ì „ìš©',
    'html5lib': 'ê°€ì¥ ì •í™•, ëŠë¦¼'
}

# ì‚¬ìš© ì˜ˆì‹œ
soup = BeautifulSoup(html_content, "lxml")  # ê°€ì¥ ë¹ ë¥¸ íŒŒì„œ
```

### 3. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬
```python
def validate_data(df):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
    print(f"ì´ í–‰ ìˆ˜: {len(df)}")
    print(f"ê²°ì¸¡ê°’: {df.isnull().sum().sum()}")
    print(f"ì¤‘ë³µê°’: {df.duplicated().sum()}")
    
    # ìˆ«ì ì»¬ëŸ¼ íƒ€ì… í™•ì¸
    numeric_cols = ['í˜„ì¬ê°€', 'ê±°ë˜ëŸ‰']
    for col in numeric_cols:
        if col in df.columns:
            print(f"{col} ë°ì´í„° íƒ€ì…: {df[col].dtype}")

# ì‚¬ìš©
df = pd.read_csv('ë„¤ì´ë²„ì½”ìŠ¤í”¼.csv')
validate_data(df)
```

ì´ ì‹¤ìŠµì„ í†µí•´ ì›¹ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  pandasë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ¯