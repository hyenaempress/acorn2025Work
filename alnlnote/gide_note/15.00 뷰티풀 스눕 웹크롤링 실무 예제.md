# 15.00 뷰티풀 스눕 웹크롤링 실무 예제

## 📋 개요
BeautifulSoup를 이용한 웹 크롤링의 실무 활용 예제입니다. 페이지 순환을 통한 대용량 데이터 수집과 통계 분석까지 다룹니다.

---

## 🚀 실전 예제: 팝마트 상품 크롤링

### 1. 라이브러리 import

```python
from bs4 import BeautifulSoup
import urllib.request as req
import re
import numpy as np
```

### 2. 페이지 순환 스크래핑 구현

```python
# 페이지 순환 스크래핑 예제 
base_url = "https://m.popmart.co.kr/product/list.html?cate_no=392"
page = 1

all_names = []
all_prices = []

while True:  # 페이지 순환 루프 추가 
    url = f"{base_url}&page={page}"
    response = req.urlopen(url)
    html = response.read().decode('utf-8')
    soup = BeautifulSoup(html, 'html.parser')

    # 상품명과 가격 추출
    names = [tag.text.strip() for tag in soup.select('p.name')]
    price_texts = [tag.text.strip() for tag in soup.select('span.price')]

    # 페이지에 더 이상 상품이 없으면 종료
    if not names:
        print(f"페이지 {page}에서 더 이상 상품을 찾을 수 없어 크롤링을 중단합니다.")
        break

    # 가격 데이터 정제 (숫자만 추출)
    price_nums = []
    for txt in price_texts:
        num = re.sub(r'[^0-9]', '', txt)  # 숫자가 아닌 문자 제거
        if num:
            price_nums.append(int(num))
    
    # 데이터 품질 검사
    if len(price_nums) != len(names):
        print(f"경고: 페이지 {page}, 상품명과 가격 수가 달라요 ({len(names)}명 vs {len(price_nums)}개)")

    # 전체 리스트에 추가
    all_names.extend(names)
    all_prices.extend(price_nums)

    print(f"페이지 {page} 완료: 상품 {len(names)}개 수집됨.")
    page += 1
```

### 3. 수집된 데이터 분석

```python
# 수집된 전체 결과 출력
print(f"\n총 수집된 상품 수: {len(all_names)}개")

if all_prices:
    arr = np.array(all_prices)
    
    # 기본 통계량 계산
    print(f"평균 가격: {arr.mean():,.0f}원")
    print(f"가격 표준편차: {arr.std():,.0f}원")
    print(f"최고 가격: {arr.max():,}원")
    
    # 최고가 상품 찾기
    print("최고가 상품:")
    for name, price in zip(all_names, all_prices):
        if price == arr.max():
            print(f" - {name}")
```

---

## 🔍 코드 핵심 포인트 분석

### 1. 페이지 순환 크롤링

**무한 루프 + 조건부 종료:**
```python
while True:
    # 데이터 수집 시도
    if not names:  # 데이터가 없으면 종료
        break
    page += 1  # 다음 페이지로
```

**장점:**
- 전체 페이지 수를 미리 몰라도 됨
- 사이트 구조 변경에 유연하게 대응
- 대용량 데이터 수집 가능

### 2. 정규표현식을 이용한 데이터 정제

```python
# 가격에서 숫자만 추출
num = re.sub(r'[^0-9]', '', txt)
```

**처리 과정:**
- `"25,000원"` → `"25000"`
- `"무료배송"` → `""` (빈 문자열)
- `"19,900"` → `"19900"`

### 3. 데이터 품질 관리

```python
# 상품명과 가격 개수 일치 확인
if len(price_nums) != len(names):
    print(f"경고: 페이지 {page}, 상품명과 가격 수가 달라요")
```

**중요성:**
- 데이터 수집 과정에서 발생할 수 있는 오류 감지
- 크롤링 결과의 신뢰성 확보
- 디버깅 시 문제점 파악 용이

### 4. numpy를 이용한 통계 분석

```python
arr = np.array(all_prices)
print(f"평균 가격: {arr.mean():,.0f}원")
print(f"가격 표준편차: {arr.std():,.0f}원")
```

**분석 가능한 지표:**
- 평균, 중간값, 최빈값
- 표준편차, 분산
- 사분위수, 백분위수
- 가격대별 상품 분포

---

## 💡 실무 활용 팁

### 1. 에러 처리 강화

```python
import time
import random

while True:
    try:
        url = f"{base_url}&page={page}"
        response = req.urlopen(url)
        html = response.read().decode('utf-8')
        
        # 성공적으로 데이터 수집
        
    except Exception as e:
        print(f"페이지 {page} 오류: {e}")
        time.sleep(5)  # 5초 대기 후 재시도
        continue
    
    # 랜덤 지연으로 서버 부하 방지
    time.sleep(random.uniform(0.5, 2.0))
    page += 1
```

### 2. 데이터 중간 저장

```python
import pandas as pd

# 10페이지마다 중간 저장
if page % 10 == 0:
    df_temp = pd.DataFrame({
        '상품명': all_names,
        '가격': all_prices[:len(all_names)]
    })
    df_temp.to_csv(f'backup_page_{page}.csv', index=False)
    print(f"페이지 {page}까지 백업 저장 완료")
```

### 3. 고급 분석 예제

```python
# 가격대별 상품 분포
price_ranges = {
    '1만원 미만': sum(1 for p in all_prices if p < 10000),
    '1-2만원': sum(1 for p in all_prices if 10000 <= p < 20000),
    '2-3만원': sum(1 for p in all_prices if 20000 <= p < 30000),
    '3만원 이상': sum(1 for p in all_prices if p >= 30000)
}

for range_name, count in price_ranges.items():
    percentage = (count / len(all_prices)) * 100
    print(f"{range_name}: {count}개 ({percentage:.1f}%)")
```

---

## ⚠️ 주의사항 및 권장사항

### 1. 법적/윤리적 고려
- **robots.txt 확인**: `https://도메인/robots.txt`
- **이용약관 준수**: 사이트의 크롤링 정책 확인
- **개인정보 보호**: 민감한 정보 수집 금지

### 2. 기술적 고려
- **요청 간격 조절**: 너무 빠른 요청은 IP 차단 위험
- **User-Agent 설정**: 봇으로 인식되지 않도록 주의
- **세션 관리**: 로그인이 필요한 사이트의 경우

### 3. 성능 최적화
- **멀티스레딩**: 동시 여러 페이지 처리
- **캐싱**: 중복 요청 방지
- **메모리 관리**: 대용량 데이터 처리 시 주의

---

## 🎯 학습 효과

이 예제를 통해 학습할 수 있는 내용:

1. **실전 크롤링 패턴**: 페이지 순환과 종료 조건
2. **데이터 정제**: 정규표현식과 문자열 처리
3. **품질 관리**: 수집 과정에서의 오류 감지
4. **통계 분석**: numpy를 이용한 기본 분석
5. **확장성**: 대용량 데이터 처리 방법

실무에서 바로 활용할 수 있는 완성도 높은 코드로, 다른 쇼핑몰이나 사이트에도 응용할 수 있습니다! 🚀