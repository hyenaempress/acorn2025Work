# 22.03.01 ê°€ì„¤ê²€ì • ì œ1ì¢…Â·ì œ2ì¢… ì˜¤ë¥˜ - ê³ ë„í™” ë²„ì „

## ğŸ¯ í•™ìŠµ ëª©í‘œ
ì´ ë¬¸ì„œëŠ” ê¸°ì¡´ ê°€ì„¤ê²€ì • ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì œ1ì¢…Â·ì œ2ì¢… ì˜¤ë¥˜ì˜ ì‹¤ë¬´ì  ì´í•´**ì™€ **ê³ ê¸‰ ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬**ë¥¼ ì™„ì „íˆ ë§ˆìŠ¤í„°í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

---

## ğŸ“š ëª©ì°¨
1. [ì œ1ì¢…Â·ì œ2ì¢… ì˜¤ë¥˜ ì™„ì „ ì´í•´](#-ì œ1ì¢…ì œ2ì¢…-ì˜¤ë¥˜-ì™„ì „-ì´í•´)
2. [ì˜¤ë¥˜ì™€ ì˜ì‚¬ê²°ì •ì˜ ì² í•™](#-ì˜¤ë¥˜ì™€-ì˜ì‚¬ê²°ì •ì˜-ì² í•™)
3. [ê²€ì •ë ¥ê³¼ íš¨ê³¼í¬ê¸°](#-ê²€ì •ë ¥ê³¼-íš¨ê³¼í¬ê¸°)
4. [ë‹¤ì¤‘ê²€ì • ë¬¸ì œì™€ í•´ê²°ì±…](#-ë‹¤ì¤‘ê²€ì •-ë¬¸ì œì™€-í•´ê²°ì±…)
5. [ë² ì´ì§€ì•ˆ ê´€ì ì—ì„œì˜ ê°€ì„¤ê²€ì •](#-ë² ì´ì§€ì•ˆ-ê´€ì ì—ì„œì˜-ê°€ì„¤ê²€ì •)
6. [ì‹¤ë¬´ ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬](#-ì‹¤ë¬´-ì˜ì‚¬ê²°ì •-í”„ë ˆì„ì›Œí¬)
7. [ê³ ê¸‰ ì‹œê°í™”ì™€ í•´ì„](#-ê³ ê¸‰-ì‹œê°í™”ì™€-í•´ì„)

---

## âš ï¸ ì œ1ì¢…Â·ì œ2ì¢… ì˜¤ë¥˜ ì™„ì „ ì´í•´

### ğŸ² ì˜ì‚¬ê²°ì • ë§¤íŠ¸ë¦­ìŠ¤ì˜ ì‹¬í™” ì´í•´

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

def comprehensive_error_analysis():
    """ì œ1ì¢…Â·ì œ2ì¢… ì˜¤ë¥˜ì˜ ì¢…í•©ì  ë¶„ì„"""
    
    print("âš ï¸ í†µê³„ì  ì˜ì‚¬ê²°ì •ì˜ 4ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤")
    print("=" * 60)
    
    # ì˜ì‚¬ê²°ì • ë§¤íŠ¸ë¦­ìŠ¤ (í™•ì¥ ë²„ì „)
    decision_scenarios = {
        "Hâ‚€ê°€ ì°¸ & Hâ‚€ ì±„íƒ": {
            "ìƒí™©": "ì˜¬ë°”ë¥¸ ê²°ì • (Correct Decision)",
            "í™•ë¥ ": "1 - Î±",
            "ì˜ë¯¸": "ì°¸ì¸ ê°€ì„¤ì„ ì°¸ì´ë¼ê³  ì˜¬ë°”ë¥´ê²Œ íŒë‹¨",
            "ì˜ˆì‹œ": "íš¨ê³¼ ì—†ëŠ” ì•½ì„ 'íš¨ê³¼ ì—†ë‹¤'ê³  ì˜¬ë°”ë¥´ê²Œ ê²°ë¡ ",
            "ë¹„ì¦ˆë‹ˆìŠ¤": "ğŸ’° ìì› ë‚­ë¹„ ë°©ì§€, ì˜¬ë°”ë¥¸ í˜„ìƒìœ ì§€"
        },
        
        "Hâ‚€ê°€ ì°¸ & Hâ‚€ ê¸°ê°": {
            "ìƒí™©": "ì œ1ì¢… ì˜¤ë¥˜ (Type I Error, Î±)",
            "í™•ë¥ ": "Î± (ìœ ì˜ìˆ˜ì¤€)",
            "ì˜ë¯¸": "ì°¸ì¸ ê°€ì„¤ì„ ê±°ì§“ì´ë¼ê³  ì˜ëª» íŒë‹¨",
            "ì˜ˆì‹œ": "íš¨ê³¼ ì—†ëŠ” ì•½ì„ 'íš¨ê³¼ ìˆë‹¤'ê³  ì˜ëª» ê²°ë¡ ",
            "ë¹„ì¦ˆë‹ˆìŠ¤": "ğŸ’¸ ë¶ˆí•„ìš”í•œ íˆ¬ì, ì˜ëª»ëœ í˜ì‹ "
        },
        
        "Hâ‚€ê°€ ê±°ì§“ & Hâ‚€ ì±„íƒ": {
            "ìƒí™©": "ì œ2ì¢… ì˜¤ë¥˜ (Type II Error, Î²)",
            "í™•ë¥ ": "Î²",
            "ì˜ë¯¸": "ê±°ì§“ì¸ ê°€ì„¤ì„ ì°¸ì´ë¼ê³  ì˜ëª» íŒë‹¨",
            "ì˜ˆì‹œ": "íš¨ê³¼ ìˆëŠ” ì•½ì„ 'íš¨ê³¼ ì—†ë‹¤'ê³  ì˜ëª» ê²°ë¡ ",
            "ë¹„ì¦ˆë‹ˆìŠ¤": "ğŸ“‰ ê¸°íšŒ ìƒì‹¤, ì˜ëª»ëœ í˜„ìƒìœ ì§€"
        },
        
        "Hâ‚€ê°€ ê±°ì§“ & Hâ‚€ ê¸°ê°": {
            "ìƒí™©": "ì˜¬ë°”ë¥¸ ê²°ì • (Statistical Power)",
            "í™•ë¥ ": "1 - Î² (ê²€ì •ë ¥)",
            "ì˜ë¯¸": "ê±°ì§“ì¸ ê°€ì„¤ì„ ê±°ì§“ì´ë¼ê³  ì˜¬ë°”ë¥´ê²Œ íŒë‹¨",
            "ì˜ˆì‹œ": "íš¨ê³¼ ìˆëŠ” ì•½ì„ 'íš¨ê³¼ ìˆë‹¤'ê³  ì˜¬ë°”ë¥´ê²Œ ê²°ë¡ ",
            "ë¹„ì¦ˆë‹ˆìŠ¤": "ğŸš€ ì˜¬ë°”ë¥¸ í˜ì‹ , ì„±ê³µì  ì˜ì‚¬ê²°ì •"
        }
    }
    
    # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ ìƒì„¸ ë¶„ì„
    for scenario, details in decision_scenarios.items():
        print(f"\nğŸ“Š {scenario}")
        print("-" * 50)
        for key, value in details.items():
            print(f"   {key}: {value}")
    
    return decision_scenarios

comprehensive_error_analysis()
```

### ğŸ¯ ì‹¤ë¬´ ê´€ì ì—ì„œì˜ ì˜¤ë¥˜ ë¹„ìš© ë¶„ì„

```python
def business_error_cost_analysis():
    """ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œì˜ ì˜¤ë¥˜ ë¹„ìš© ë¶„ì„"""
    
    business_scenarios = {
        "ì‹ ì•½ ê°œë°œ": {
            "ì œ1ì¢… ì˜¤ë¥˜ ë¹„ìš©": {
                "ì‹œë‚˜ë¦¬ì˜¤": "íš¨ê³¼ ì—†ëŠ” ì•½ì„ ì¶œì‹œ",
                "ì§ì ‘ë¹„ìš©": "ì„ìƒì‹œí—˜ ë¹„ìš© (ìˆ˜ë°±ì–µ)",
                "ê°„ì ‘ë¹„ìš©": "íšŒì‚¬ ì‹ ë¢°ë„ í•˜ë½, ë²•ì  ë¦¬ìŠ¤í¬",
                "ì‹¬ê°ë„": "ğŸ”´ ë§¤ìš° ë†’ìŒ"
            },
            "ì œ2ì¢… ì˜¤ë¥˜ ë¹„ìš©": {
                "ì‹œë‚˜ë¦¬ì˜¤": "íš¨ê³¼ ìˆëŠ” ì•½ì„ í¬ê¸°",
                "ì§ì ‘ë¹„ìš©": "ê¸°íšŒë¹„ìš© (ì ì¬ ìˆ˜ìµ ìƒì‹¤)",
                "ê°„ì ‘ë¹„ìš©": "ê²½ìŸì‚¬ì—ê²Œ ì‹œì¥ ì„ ì  í—ˆìš©",
                "ì‹¬ê°ë„": "ğŸŸ¡ ì¤‘ê°„"
            },
            "ì¶”ì²œ ì „ëµ": "ì œ1ì¢… ì˜¤ë¥˜ë¥¼ ë” ì—„ê²©íˆ í†µì œ (Î± = 0.01)"
        },
        
        "ë§ˆì¼€íŒ… ìº í˜ì¸": {
            "ì œ1ì¢… ì˜¤ë¥˜ ë¹„ìš©": {
                "ì‹œë‚˜ë¦¬ì˜¤": "íš¨ê³¼ ì—†ëŠ” ìº í˜ì¸ì„ ì‹¤í–‰",
                "ì§ì ‘ë¹„ìš©": "ê´‘ê³ ë¹„ ì†ì‹¤ (ìˆ˜ì²œë§Œ~ìˆ˜ì–µ)",
                "ê°„ì ‘ë¹„ìš©": "ë¸Œëœë“œ ì´ë¯¸ì§€ ì†ìƒ",
                "ì‹¬ê°ë„": "ğŸŸ¡ ì¤‘ê°„"
            },
            "ì œ2ì¢… ì˜¤ë¥˜ ë¹„ìš©": {
                "ì‹œë‚˜ë¦¬ì˜¤": "íš¨ê³¼ ìˆëŠ” ìº í˜ì¸ì„ í¬ê¸°",
                "ì§ì ‘ë¹„ìš©": "ë§¤ì¶œ ì¦ê°€ ê¸°íšŒ ìƒì‹¤",
                "ê°„ì ‘ë¹„ìš©": "ê²½ìŸ ìš°ìœ„ ìƒì‹¤",
                "ì‹¬ê°ë„": "ğŸŸ¡ ì¤‘ê°„"
            },
            "ì¶”ì²œ ì „ëµ": "ê· í˜•ì¡íŒ ì ‘ê·¼ (Î± = 0.05)"
        },
        
        "í’ˆì§ˆ ê´€ë¦¬": {
            "ì œ1ì¢… ì˜¤ë¥˜ ë¹„ìš©": {
                "ì‹œë‚˜ë¦¬ì˜¤": "ì •ìƒ ì œí’ˆì„ ë¶ˆëŸ‰ìœ¼ë¡œ íŒë‹¨",
                "ì§ì ‘ë¹„ìš©": "ì œí’ˆ íê¸° ë¹„ìš©",
                "ê°„ì ‘ë¹„ìš©": "ìƒì‚°ì„± ì €í•˜, ë‚©ê¸° ì§€ì—°",
                "ì‹¬ê°ë„": "ğŸŸ¡ ì¤‘ê°„"
            },
            "ì œ2ì¢… ì˜¤ë¥˜ ë¹„ìš©": {
                "ì‹œë‚˜ë¦¬ì˜¤": "ë¶ˆëŸ‰ ì œí’ˆì„ ì •ìƒìœ¼ë¡œ íŒë‹¨",
                "ì§ì ‘ë¹„ìš©": "ë¦¬ì½œ ë¹„ìš©, ë°°ìƒ",
                "ê°„ì ‘ë¹„ìš©": "ê³ ê° ì‹ ë¢° ìƒì‹¤, ë¸Œëœë“œ ìœ„í—˜",
                "ì‹¬ê°ë„": "ğŸ”´ ë§¤ìš° ë†’ìŒ"
            },
            "ì¶”ì²œ ì „ëµ": "ì œ2ì¢… ì˜¤ë¥˜ë¥¼ ë” ì—„ê²©íˆ í†µì œ (Î² ìµœì†Œí™”)"
        }
    }
    
    print("ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ë³„ ì˜¤ë¥˜ ë¹„ìš© ë¶„ì„")
    print("=" * 50)
    
    for business, analysis in business_scenarios.items():
        print(f"\nğŸ¢ {business}")
        print("-" * 30)
        
        for error_type, details in analysis.items():
            if error_type == "ì¶”ì²œ ì „ëµ":
                print(f"   âœ… {error_type}: {details}")
            else:
                print(f"\n   âš ï¸ {error_type}:")
                for key, value in details.items():
                    print(f"     {key}: {value}")
    
    return business_scenarios

business_error_cost_analysis()
```

---

## ğŸ§  ì˜¤ë¥˜ì™€ ì˜ì‚¬ê²°ì •ì˜ ì² í•™

### ğŸ’­ ì˜¤ë¥˜ì˜ ì² í•™ì  ë°°ê²½

```python
def philosophical_perspective_on_errors():
    """ì˜¤ë¥˜ì— ëŒ€í•œ ì² í•™ì  ê´€ì """
    
    philosophical_frameworks = {
        "ë³´ìˆ˜ì£¼ì˜ (Conservatism)": {
            "í•µì‹¬ ì² í•™": "ê¸°ì¡´ ì§ˆì„œ ìœ ì§€, ë³€í™”ì— ì‹ ì¤‘",
            "ì˜¤ë¥˜ ì„ í˜¸": "ì œ1ì¢… ì˜¤ë¥˜ë¥¼ ë” ê²½ê³„ (Î±ë¥¼ ì‘ê²Œ)",
            "ì˜ì‚¬ê²°ì •": "í™•ì‹¤í•œ ì¦ê±°ê°€ ìˆì„ ë•Œë§Œ ë³€í™”",
            "ì ìš© ë¶„ì•¼": "ì˜ë£Œ, ì•ˆì „, ë²•ë¥ ",
            "ì¥ì ": "ì•ˆì •ì„±, ì‹ ë¢°ì„±",
            "ë‹¨ì ": "í˜ì‹  ê¸°íšŒ ìƒì‹¤"
        },
        
        "ì§„ë³´ì£¼ì˜ (Progressivism)": {
            "í•µì‹¬ ì² í•™": "ì ê·¹ì  í˜ì‹ , ê¸°íšŒ í¬ì°©",
            "ì˜¤ë¥˜ ì„ í˜¸": "ì œ2ì¢… ì˜¤ë¥˜ë¥¼ ë” ê²½ê³„ (Î²ë¥¼ ì‘ê²Œ)",
            "ì˜ì‚¬ê²°ì •": "ë¶ˆí™•ì‹¤í•´ë„ ì‹œë„í•˜ë©° í•™ìŠµ",
            "ì ìš© ë¶„ì•¼": "ìŠ¤íƒ€íŠ¸ì—…, R&D, íƒí—˜ì  ì—°êµ¬",
            "ì¥ì ": "í˜ì‹  ê°€ëŠ¥ì„±, ë¹ ë¥¸ í•™ìŠµ",
            "ë‹¨ì ": "ë†’ì€ ì‹¤íŒ¨ ìœ„í—˜"
        },
        
        "ê· í˜•ì£¼ì˜ (Balanced Approach)": {
            "í•µì‹¬ ì² í•™": "ìƒí™©ì— ë”°ë¥¸ ìœ ì—°í•œ ì ‘ê·¼",
            "ì˜¤ë¥˜ ì„ í˜¸": "ë§¥ë½ì— ë”°ë¼ Î±, Î² ì¡°ì •",
            "ì˜ì‚¬ê²°ì •": "ë¹„ìš©-í¸ìµ ë¶„ì„ ê¸°ë°˜",
            "ì ìš© ë¶„ì•¼": "ì¼ë°˜ì  ë¹„ì¦ˆë‹ˆìŠ¤, ì •ì±…",
            "ì¥ì ": "ìƒí™© ì ì‘ì„±",
            "ë‹¨ì ": "ëª…í™•í•œ ê¸°ì¤€ ë¶€ì¡±"
        }
    }
    
    print("ğŸ¤” ì˜¤ë¥˜ì— ëŒ€í•œ ì² í•™ì  ê´€ì ")
    print("=" * 40)
    
    for philosophy, details in philosophical_frameworks.items():
        print(f"\nğŸ“š {philosophy}")
        print("-" * 25)
        for aspect, description in details.items():
            print(f"   {aspect}: {description}")
    
    # ì² í•™ë³„ ìœ ì˜ìˆ˜ì¤€ ì„¤ì • ê°€ì´ë“œ
    print(f"\nğŸ¯ ì² í•™ë³„ ìœ ì˜ìˆ˜ì¤€ ì„¤ì • ê°€ì´ë“œ:")
    philosophy_alpha = {
        "ë³´ìˆ˜ì£¼ì˜": "Î± = 0.01 (1% ìœ„í—˜ë§Œ í—ˆìš©)",
        "ê· í˜•ì£¼ì˜": "Î± = 0.05 (5% ìœ„í—˜ í—ˆìš©, í‘œì¤€)",
        "ì§„ë³´ì£¼ì˜": "Î± = 0.10 (10% ìœ„í—˜ í—ˆìš©, ê´€ëŒ€)"
    }
    
    for philosophy, alpha_setting in philosophy_alpha.items():
        print(f"   {philosophy}: {alpha_setting}")
    
    return philosophical_frameworks

philosophical_perspective_on_errors()
```

### ğŸ­ ì—­í• ë³„ ê´€ì ì˜ ì°¨ì´

```python
def stakeholder_perspectives():
    """ì´í•´ê´€ê³„ìë³„ ì˜¤ë¥˜ì— ëŒ€í•œ ê´€ì """
    
    stakeholder_views = {
        "ì—°êµ¬ì (Researcher)": {
            "ì£¼ìš” ê´€ì‹¬": "í•™ë¬¸ì  ì—„ë°€ì„±, ì¬í˜„ ê°€ëŠ¥ì„±",
            "ì„ í˜¸í•˜ëŠ” Î±": "0.05 (ì „í†µì  ê¸°ì¤€)",
            "ì œ1ì¢… ì˜¤ë¥˜ ìš°ë ¤": "ì˜ëª»ëœ ë°œê²¬ ë°œí‘œë¡œ ì¸í•œ ì‹ ë¢°ë„ ì†ìƒ",
            "ì œ2ì¢… ì˜¤ë¥˜ ìš°ë ¤": "ì§„ì§œ ë°œê²¬ì„ ë†“ì¹  ê°€ëŠ¥ì„±",
            "íŠ¹ì§•": "ê· í˜•ì¡íŒ ì ‘ê·¼, í•™ê³„ ê´€ìŠµ ì¤€ìˆ˜"
        },
        
        "ê²½ì˜ì§„ (Management)": {
            "ì£¼ìš” ê´€ì‹¬": "ROI, ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³¼",
            "ì„ í˜¸í•˜ëŠ” Î±": "ìƒí™©ì— ë”°ë¼ ìœ ì—° (0.01~0.10)",
            "ì œ1ì¢… ì˜¤ë¥˜ ìš°ë ¤": "ë¶ˆí•„ìš”í•œ íˆ¬ìë¡œ ì¸í•œ ì†ì‹¤",
            "ì œ2ì¢… ì˜¤ë¥˜ ìš°ë ¤": "ê¸°íšŒ ìƒì‹¤ë¡œ ì¸í•œ ê²½ìŸ ì—´ì„¸",
            "íŠ¹ì§•": "ë¹„ìš©-í¸ìµ ì¤‘ì‹¬ì˜ ì˜ì‚¬ê²°ì •"
        },
        
        "ê·œì œê¸°ê´€ (Regulator)": {
            "ì£¼ìš” ê´€ì‹¬": "ê³µê³µ ì•ˆì „, ì‚¬íšŒì  ì±…ì„",
            "ì„ í˜¸í•˜ëŠ” Î±": "0.01 ì´í•˜ (ë§¤ìš° ë³´ìˆ˜ì )",
            "ì œ1ì¢… ì˜¤ë¥˜ ìš°ë ¤": "ìœ„í—˜í•œ ì œí’ˆ/ì„œë¹„ìŠ¤ ìŠ¹ì¸",
            "ì œ2ì¢… ì˜¤ë¥˜ ìš°ë ¤": "ìœ ìš©í•œ í˜ì‹  ì°¨ë‹¨",
            "íŠ¹ì§•": "ì•ˆì „ ìš°ì„ , ì˜ˆë°© ì›ì¹™"
        },
        
        "íˆ¬ìì (Investor)": {
            "ì£¼ìš” ê´€ì‹¬": "ìˆ˜ìµë¥ , ë¦¬ìŠ¤í¬ ê´€ë¦¬",
            "ì„ í˜¸í•˜ëŠ” Î±": "íˆ¬ì ì„±í–¥ì— ë”°ë¼ ì°¨ë“±",
            "ì œ1ì¢… ì˜¤ë¥˜ ìš°ë ¤": "ìˆ˜ìµì„± ì—†ëŠ” ì‚¬ì—…ì— íˆ¬ì",
            "ì œ2ì¢… ì˜¤ë¥˜ ìš°ë ¤": "ìˆ˜ìµì„± ë†’ì€ ê¸°íšŒ ë†“ì¹¨",
            "íŠ¹ì§•": "í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ì–‘í™”ë¡œ ë¦¬ìŠ¤í¬ ë¶„ì‚°"
        }
    }
    
    print("ğŸ‘¥ ì´í•´ê´€ê³„ìë³„ ì˜¤ë¥˜ ê´€ì ")
    print("=" * 35)
    
    for stakeholder, perspective in stakeholder_views.items():
        print(f"\nğŸ­ {stakeholder}")
        print("-" * 20)
        for aspect, view in perspective.items():
            print(f"   {aspect}: {view}")
    
    return stakeholder_views

stakeholder_perspectives()
```

---

## ğŸ”‹ ê²€ì •ë ¥ê³¼ íš¨ê³¼í¬ê¸°

### âš¡ ê²€ì •ë ¥(Statistical Power) ì™„ì „ ë¶„ì„

```python
def statistical_power_analysis():
    """ê²€ì •ë ¥ì˜ ì¢…í•©ì  ë¶„ì„"""
    
    print("âš¡ ê²€ì •ë ¥(Statistical Power) ì™„ì „ ë¶„ì„")
    print("=" * 50)
    
    power_concepts = {
        "ì •ì˜": "ì‹¤ì œë¡œ ì°¨ì´ê°€ ìˆì„ ë•Œ ì´ë¥¼ ì˜¬ë°”ë¥´ê²Œ íƒì§€í•  í™•ë¥ ",
        "ìˆ˜ì‹": "ê²€ì •ë ¥ = 1 - Î² (ì œ2ì¢… ì˜¤ë¥˜ í™•ë¥ )",
        "ì˜ë¯¸": "ê±°ì§“ì¸ ê·€ë¬´ê°€ì„¤ì„ ì˜¬ë°”ë¥´ê²Œ ê¸°ê°í•  í™•ë¥ ",
        "ë²”ìœ„": "0 â‰¤ Power â‰¤ 1 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)",
        "ì¼ë°˜ì  ê¸°ì¤€": "0.80 ì´ìƒ (80% ì´ìƒ)"
    }
    
    print("ğŸ“Š ê²€ì •ë ¥ ê¸°ë³¸ ê°œë…:")
    for concept, explanation in power_concepts.items():
        print(f"   {concept}: {explanation}")
    
    # ê²€ì •ë ¥ì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì¸ë“¤
    power_factors = {
        "í‘œë³¸ í¬ê¸° (n)": {
            "ê´€ê³„": "í‘œë³¸ì´ í´ìˆ˜ë¡ ê²€ì •ë ¥ ì¦ê°€",
            "ì´ìœ ": "í‘œì¤€ì˜¤ì°¨ ê°ì†Œë¡œ ì°¨ì´ íƒì§€ ëŠ¥ë ¥ í–¥ìƒ",
            "ì‹¤ë¬´ í™œìš©": "A/B í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì‚¬ì´ì¦ˆ ì„¤ê³„"
        },
        
        "ìœ ì˜ìˆ˜ì¤€ (Î±)": {
            "ê´€ê³„": "Î±ê°€ í´ìˆ˜ë¡ ê²€ì •ë ¥ ì¦ê°€",
            "ì´ìœ ": "ê¸°ê°ì—­ì´ ë„“ì–´ì ¸ ì°¨ì´ íƒì§€ ì‰¬ì›Œì§",
            "íŠ¸ë ˆì´ë“œì˜¤í”„": "ì œ1ì¢… ì˜¤ë¥˜ ìœ„í—˜ ì¦ê°€"
        },
        
        "íš¨ê³¼í¬ê¸° (Effect Size)": {
            "ê´€ê³„": "ì‹¤ì œ ì°¨ì´ê°€ í´ìˆ˜ë¡ ê²€ì •ë ¥ ì¦ê°€",
            "ì´ìœ ": "í° ì°¨ì´ëŠ” íƒì§€í•˜ê¸° ì‰¬ì›€",
            "ì‹¤ë¬´ ì˜ë¯¸": "ì‹¤ì§ˆì  ì˜ë¯¸ê°€ ìˆëŠ” ì°¨ì´ ì •ì˜"
        },
        
        "ì¸¡ì • ì •ë°€ë„": {
            "ê´€ê³„": "ì¸¡ì •ì´ ì •í™•í• ìˆ˜ë¡ ê²€ì •ë ¥ ì¦ê°€",
            "ì´ìœ ": "ë…¸ì´ì¦ˆ ê°ì†Œë¡œ ì‹ í˜¸ íƒì§€ í–¥ìƒ",
            "ì‹¤ë¬´ í™œìš©": "ì¸¡ì • ë„êµ¬ ê°œì„ , ì‹¤í—˜ ì„¤ê³„ ìµœì í™”"
        }
    }
    
    print(f"\nğŸ¯ ê²€ì •ë ¥ ì˜í–¥ ìš”ì¸:")
    for factor, details in power_factors.items():
        print(f"\n   ğŸ“ˆ {factor}:")
        for aspect, description in details.items():
            print(f"     {aspect}: {description}")
    
    return power_concepts, power_factors

statistical_power_analysis()
```

### ğŸ“ íš¨ê³¼í¬ê¸°(Effect Size) ì‹¤ë¬´ ê°€ì´ë“œ

```python
def effect_size_practical_guide():
    """íš¨ê³¼í¬ê¸°ì˜ ì‹¤ë¬´ì  ì´í•´ì™€ í™œìš©"""
    
    print("ğŸ“ íš¨ê³¼í¬ê¸°(Effect Size) ì‹¤ë¬´ ê°€ì´ë“œ")
    print("=" * 45)
    
    effect_size_types = {
        "Cohen's d (ë‘ ê·¸ë£¹ í‰ê·  ì°¨ì´)": {
            "ìˆ˜ì‹": "d = (Î¼â‚ - Î¼â‚‚) / Ïƒ",
            "í•´ì„ ê¸°ì¤€": {
                "ì‘ì€ íš¨ê³¼": "d = 0.20 (ì•½ê°„ì˜ ì°¨ì´)",
                "ì¤‘ê°„ íš¨ê³¼": "d = 0.50 (ë³´í†µ ì°¨ì´)",
                "í° íš¨ê³¼": "d = 0.80 (ëª…í™•í•œ ì°¨ì´)"
            },
            "ì‹¤ë¬´ ì˜ˆì‹œ": {
                "d = 0.2": "ë‚¨ë…€ í‚¤ ì°¨ì´ (ì‹¤ì œ ì•½ 3-4cm)",
                "d = 0.5": "êµìœ¡ í”„ë¡œê·¸ë¨ íš¨ê³¼ (ë³´í†µ ìˆ˜ì¤€)",
                "d = 0.8": "ì‹ ì•½ vs ìœ„ì•½ íš¨ê³¼ (í° ì°¨ì´)"
            }
        },
        
        "ìƒê´€ê³„ìˆ˜ (r)": {
            "ìˆ˜ì‹": "r = Pearson ìƒê´€ê³„ìˆ˜",
            "í•´ì„ ê¸°ì¤€": {
                "ì‘ì€ íš¨ê³¼": "r = 0.10 (ì•½í•œ ìƒê´€)",
                "ì¤‘ê°„ íš¨ê³¼": "r = 0.30 (ë³´í†µ ìƒê´€)",
                "í° íš¨ê³¼": "r = 0.50 (ê°•í•œ ìƒê´€)"
            },
            "ì‹¤ë¬´ ì˜ˆì‹œ": {
                "r = 0.1": "í‚¤ì™€ IQì˜ ìƒê´€ê´€ê³„",
                "r = 0.3": "êµìœ¡ ìˆ˜ì¤€ê³¼ ì†Œë“ì˜ ìƒê´€ê´€ê³„",
                "r = 0.5": "ì—°ìŠµ ì‹œê°„ê³¼ ì‹¤ë ¥ì˜ ìƒê´€ê´€ê³„"
            }
        },
        
        "ê²°ì •ê³„ìˆ˜ (RÂ²)": {
            "ìˆ˜ì‹": "RÂ² = ì„¤ëª…ëœ ë¶„ì‚° / ì „ì²´ ë¶„ì‚°",
            "í•´ì„ ê¸°ì¤€": {
                "ì‘ì€ íš¨ê³¼": "RÂ² = 0.01 (1% ì„¤ëª…)",
                "ì¤‘ê°„ íš¨ê³¼": "RÂ² = 0.09 (9% ì„¤ëª…)",
                "í° íš¨ê³¼": "RÂ² = 0.25 (25% ì„¤ëª…)"
            },
            "ì‹¤ë¬´ í™œìš©": "íšŒê·€ ëª¨ë¸ì˜ ì„¤ëª…ë ¥ í‰ê°€"
        }
    }
    
    for measure, details in effect_size_types.items():
        print(f"\nğŸ“Š {measure}")
        print("-" * 30)
        print(f"   ìˆ˜ì‹: {details['ìˆ˜ì‹']}")
        
        if 'í•´ì„ ê¸°ì¤€' in details:
            print(f"   í•´ì„ ê¸°ì¤€:")
            for level, criterion in details['í•´ì„ ê¸°ì¤€'].items():
                print(f"     â€¢ {level}: {criterion}")
        
        if 'ì‹¤ë¬´ ì˜ˆì‹œ' in details:
            print(f"   ì‹¤ë¬´ ì˜ˆì‹œ:")
            for example, description in details['ì‹¤ë¬´ ì˜ˆì‹œ'].items():
                print(f"     â€¢ {example}: {description}")
    
    return effect_size_types

effect_size_practical_guide()
```

### ğŸ¯ ê²€ì •ë ¥ ë¶„ì„ì„ ìœ„í•œ ì‹¤ë¬´ ë„êµ¬

```python
def power_analysis_toolkit():
    """ê²€ì •ë ¥ ë¶„ì„ ì‹¤ë¬´ ë„êµ¬"""
    
    import numpy as np
    from scipy import stats
    
    def calculate_sample_size(effect_size, alpha=0.05, power=0.80):
        """í•„ìš”í•œ í‘œë³¸ í¬ê¸° ê³„ì‚°"""
        # Cohen's conventionì„ ì‚¬ìš©í•œ ê·¼ì‚¬ ê³„ì‚°
        z_alpha = stats.norm.ppf(1 - alpha/2)  # ì–‘ì¸¡ê²€ì •
        z_beta = stats.norm.ppf(power)
        
        n = 2 * ((z_alpha + z_beta) / effect_size) ** 2
        return int(np.ceil(n))
    
    def calculate_power(n, effect_size, alpha=0.05):
        """ì£¼ì–´ì§„ ì¡°ê±´ì—ì„œ ê²€ì •ë ¥ ê³„ì‚°"""
        z_alpha = stats.norm.ppf(1 - alpha/2)
        z_score = effect_size * np.sqrt(n/2) - z_alpha
        power = stats.norm.cdf(z_score)
        return power
    
    print("ğŸ¯ ê²€ì •ë ¥ ë¶„ì„ ì‹¤ë¬´ ë„êµ¬")
    print("=" * 35)
    
    # ì‹¤ë¬´ ì‹œë‚˜ë¦¬ì˜¤ë³„ í‘œë³¸ í¬ê¸° ê³„ì‚°
    scenarios = {
        "A/B í…ŒìŠ¤íŠ¸ (ì‘ì€ íš¨ê³¼)": {"effect_size": 0.2, "power": 0.80},
        "ë§ˆì¼€íŒ… ìº í˜ì¸ (ì¤‘ê°„ íš¨ê³¼)": {"effect_size": 0.5, "power": 0.80},
        "ì‹ ì œí’ˆ í…ŒìŠ¤íŠ¸ (í° íš¨ê³¼)": {"effect_size": 0.8, "power": 0.80},
        "ë†’ì€ ê²€ì •ë ¥ í•„ìš”": {"effect_size": 0.5, "power": 0.90}
    }
    
    print(f"\nğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ë³„ í•„ìš” í‘œë³¸ í¬ê¸°:")
    for scenario, params in scenarios.items():
        n_required = calculate_sample_size(
            params["effect_size"], 
            power=params["power"]
        )
        print(f"   {scenario}: {n_required}ëª… (ê° ê·¸ë£¹)")
    
    # í‘œë³¸ í¬ê¸°ë³„ ê²€ì •ë ¥ ê³„ì‚°
    print(f"\nâš¡ í‘œë³¸ í¬ê¸°ë³„ ê²€ì •ë ¥ (íš¨ê³¼í¬ê¸° 0.5 ê¸°ì¤€):")
    sample_sizes = [30, 50, 100, 200, 500]
    for n in sample_sizes:
        power = calculate_power(n, 0.5)
        print(f"   n={n}: ê²€ì •ë ¥ {power:.3f} ({power*100:.1f}%)")
    
    return calculate_sample_size, calculate_power

sample_size_calc, power_calc = power_analysis_toolkit()
```

---

## ğŸ”„ ë‹¤ì¤‘ê²€ì • ë¬¸ì œì™€ í•´ê²°ì±…

### âš ï¸ ë‹¤ì¤‘ê²€ì • ë¬¸ì œì˜ ì‹¬ê°ì„±

```python
def multiple_testing_problem():
    """ë‹¤ì¤‘ê²€ì • ë¬¸ì œì˜ ì´í•´ì™€ ì‹œë®¬ë ˆì´ì…˜"""
    
    import numpy as np
    from scipy import stats
    
    print("ğŸ”„ ë‹¤ì¤‘ê²€ì • ë¬¸ì œ (Multiple Testing Problem)")
    print("=" * 50)
    
    # ë‹¤ì¤‘ê²€ì • ì‹œ ì œ1ì¢… ì˜¤ë¥˜ í™•ë¥  ê³„ì‚°
    def family_wise_error_rate(num_tests, alpha=0.05):
        """ì „ì²´ ì‹¤í—˜ì—ì„œ ìµœì†Œ í•˜ë‚˜ì˜ ì œ1ì¢… ì˜¤ë¥˜ê°€ ë°œìƒí•  í™•ë¥ """
        return 1 - (1 - alpha) ** num_tests
    
    print("ğŸ“Š ê²€ì • íšŸìˆ˜ë³„ ì œ1ì¢… ì˜¤ë¥˜ ë°œìƒ í™•ë¥ :")
    test_counts = [1, 5, 10, 20, 50, 100]
    
    for m in test_counts:
        fwer = family_wise_error_rate(m)
        print(f"   {m}ë²ˆ ê²€ì • ì‹œ: {fwer:.3f} ({fwer*100:.1f}%)")
    
    # ì‹¤ì œ ì‹œë®¬ë ˆì´ì…˜
    def simulate_multiple_testing(num_tests=20, num_simulations=10000, alpha=0.05):
        """ë‹¤ì¤‘ê²€ì • ì‹œë®¬ë ˆì´ì…˜"""
        false_discoveries = 0
        
        for _ in range(num_simulations):
            # ê·€ë¬´ê°€ì„¤ì´ ëª¨ë‘ ì°¸ì¸ ìƒí™© (ì°¨ì´ê°€ ì—†ìŒ)
            p_values = []
            for _ in range(num_tests):
                # ë‘ ê·¸ë£¹ ëª¨ë‘ ë™ì¼í•œ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
                group1 = np.random.normal(0, 1, 30)
                group2 = np.random.normal(0, 1, 30)
                _, p_value = stats.ttest_ind(group1, group2)
                p_values.append(p_value)
            
            # í•˜ë‚˜ë¼ë„ ìœ ì˜í•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ë©´ ê±°ì§“ ë°œê²¬
            if any(p < alpha for p in p_values):
                false_discoveries += 1
        
        return false_discoveries / num_simulations
    
    print(f"\nğŸ² ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼:")
    simulated_fwer = simulate_multiple_testing()
    theoretical_fwer = family_wise_error_rate(20)
    
    print(f"   ì´ë¡ ì  FWER: {theoretical_fwer:.3f}")
    print(f"   ì‹œë®¬ë ˆì´ì…˜ FWER: {simulated_fwer:.3f}")
    print(f"   ì°¨ì´: {abs(theoretical_fwer - simulated_fwer):.3f}")
    
    return family_wise_error_rate, simulate_multiple_testing

fwer_calc, simulate_mt = multiple_testing_problem()
```

### ğŸ›¡ï¸ ë‹¤ì¤‘ê²€ì • ë³´ì • ë°©ë²•ë“¤

```python
def multiple_testing_corrections():
    """ë‹¤ì¤‘ê²€ì • ë³´ì • ë°©ë²•ë“¤ì˜ ë¹„êµ"""
    
    import numpy as np
    from scipy import stats
    
    correction_methods = {
        "Bonferroni ë³´ì •": {
            "ë°©ë²•": "Î±_adjusted = Î± / m",
            "íŠ¹ì§•": "ê°€ì¥ ë³´ìˆ˜ì , ê°•í•œ í†µì œ",
            "ì¥ì ": "ê°„ë‹¨í•˜ê³  í™•ì‹¤í•œ ë³´ì •",
            "ë‹¨ì ": "ë„ˆë¬´ ë³´ìˆ˜ì , ê²€ì •ë ¥ í¬ê²Œ ê°ì†Œ",
            "ì ìš©": "ì¤‘ìš”í•œ ì˜ë£Œ/ì•ˆì „ ì—°êµ¬"
        },
        
        "Holm ë°©ë²•": {
            "ë°©ë²•": "ìˆœì°¨ì  ë³´ì • (Step-down)",
            "íŠ¹ì§•": "Bonferronië³´ë‹¤ ëœ ë³´ìˆ˜ì ",
            "ì¥ì ": "ê²€ì •ë ¥ì´ Bonferronië³´ë‹¤ ë†’ìŒ",
            "ë‹¨ì ": "ì—¬ì „íˆ ë³´ìˆ˜ì ",
            "ì ìš©": "ì¼ë°˜ì ì¸ ë‹¤ì¤‘ ë¹„êµ"
        },
        
        "FDR (Benjamini-Hochberg)": {
            "ë°©ë²•": "ê±°ì§“ ë°œê²¬ìœ¨ í†µì œ",
            "íŠ¹ì§•": "í˜„ëŒ€ì  ì ‘ê·¼, ì‹¤ìš©ì ",
            "ì¥ì ": "ê²€ì •ë ¥ ë†’ìŒ, ì‹¤ë¬´ ì¹œí™”ì ",
            "ë‹¨ì ": "ê°œë… ì´í•´ í•„ìš”",
            "ì ìš©": "íƒìƒ‰ì  ë°ì´í„° ë¶„ì„, ë°”ì´ì˜¤ì¸í¬ë§¤í‹±ìŠ¤"
        }
    }
    
    print("ğŸ›¡ï¸ ë‹¤ì¤‘ê²€ì • ë³´ì • ë°©ë²• ë¹„êµ")
    print("=" * 40)
    
    for method, details in correction_methods.items():
        print(f"\nğŸ“‹ {method}")
        print("-" * 20)
        for aspect, description in details.items():
            print(f"   {aspect}: {description}")
    
    # ì‹¤ì œ ë³´ì • ì˜ˆì‹œ
    def apply_corrections(p_values, alpha=0.05):
        """ë‹¤ì–‘í•œ ë³´ì • ë°©ë²• ì ìš©"""
        p_values = np.array(p_values)
        m = len(p_values)
        
        # 1. Bonferroni ë³´ì •
        bonferroni_alpha = alpha / m
        bonferroni_significant = p_values < bonferroni_alpha
        
        # 2. Holm ë°©ë²•
        sorted_indices = np.argsort(p_values)
        holm_significant = np.zeros(m, dtype=bool)
        
        for i, idx in enumerate(sorted_indices):
            holm_alpha = alpha / (m - i)
            if p_values[idx] < holm_alpha:
                holm_significant[idx] = True
            else:
                break
        
        # 3. FDR (Benjamini-Hochberg)
        sorted_p = np.sort(p_values)
        fdr_significant = np.zeros(m, dtype=bool)
        
        for i in range(m-1, -1, -1):
            if sorted_p[i] <= (i+1) / m * alpha:
                threshold = sorted_p[i]
                fdr_significant = p_values <= threshold
                break
        
        return {
            "original": p_values < alpha,
            "bonferroni": bonferroni_significant,
            "holm": holm_significant,
            "fdr": fdr_significant
        }
    
    # ì˜ˆì‹œ ë°ì´í„°ë¡œ ë³´ì • ë°©ë²• ë¹„êµ
    example_p_values = [0.001, 0.01, 0.03, 0.04, 0.06, 0.08, 0.12, 0.15, 0.25, 0.45]
    results = apply_corrections(example_p_values)
    
    print(f"\nğŸ§ª ë³´ì • ë°©ë²• ë¹„êµ (ì˜ˆì‹œ ë°ì´í„°):")
    print(f"P-values: {example_p_values}")
    
    for method, significant in results.items():
        count = np.sum(significant)
        print(f"   {method}: {count}ê°œ ìœ ì˜ {list(significant)}")
    
    return correction_methods, apply_corrections

corrections, apply_corr = multiple_testing_corrections()
```

---

## ğŸ”® ë² ì´ì§€ì•ˆ ê´€ì ì—ì„œì˜ ê°€ì„¤ê²€ì •

### ğŸ¯ ë¹ˆë„ì£¼ì˜ vs ë² ì´ì§€ì•ˆ íŒ¨ëŸ¬ë‹¤ì„

```python
def frequentist_vs_bayesian():
    """ë¹ˆë„ì£¼ì˜ì™€ ë² ì´ì§€ì•ˆ ê´€ì ì˜ ë¹„êµ"""
    
    paradigm_comparison = {
        "í™•ë¥ ì˜ ì˜ë¯¸": {
            "ë¹ˆë„ì£¼ì˜": "ì¥ê¸°ì  ë¹ˆë„ (ê°ê´€ì  í™•ë¥ )",
            "ë² ì´ì§€ì•ˆ": "ë¯¿ìŒì˜ ì •ë„ (ì£¼ê´€ì  í™•ë¥ )",
            "ì˜ˆì‹œ": "ë™ì „ ì•ë©´ í™•ë¥  50%ì˜ ì˜ë¯¸"
        },
        
        "ëª¨ìˆ˜ì— ëŒ€í•œ ê´€ì ": {
            "ë¹ˆë„ì£¼ì˜": "ëª¨ìˆ˜ëŠ” ê³ ì •ëœ ìƒìˆ˜ (ì•Œ ìˆ˜ ì—†ìŒ)",
            "ë² ì´ì§€ì•ˆ": "ëª¨ìˆ˜ëŠ” í™•ë¥ ë³€ìˆ˜ (ë¶„í¬ë¥¼ ê°€ì§)",
            "ì‹¤ë¬´ ì°¨ì´": "ì ì¶”ì • vs êµ¬ê°„ì¶”ì •ì˜ í•´ì„"
        },
        
        "ê°€ì„¤ê²€ì • ë°©ë²•": {
            "ë¹ˆë„ì£¼ì˜": "P-value, ìœ ì˜ì„± ê²€ì •",
            "ë² ì´ì§€ì•ˆ": "Bayes Factor, ì‚¬í›„í™•ë¥ ",
            "ì¥ì ": "ê°ê´€ì„± vs ì§ê´€ì„±"
        },
        
        "ì‚¬ì „ ì •ë³´ í™œìš©": {
            "ë¹ˆë„ì£¼ì˜": "ì‚¬ì „ ì •ë³´ ë¬´ì‹œ",
            "ë² ì´ì§€ì•ˆ": "ì‚¬ì „ ì •ë³´ ì ê·¹ í™œìš©",
            "ì‹¤ë¬´ ê°€ì¹˜": "ì „ë¬¸ê°€ ì§€ì‹ ë°˜ì˜ ê°€ëŠ¥"
        }
    }
    
    print("ğŸ”® ë¹ˆë„ì£¼ì˜ vs ë² ì´ì§€ì•ˆ íŒ¨ëŸ¬ë‹¤ì„")
    print("=" * 45)
    
    for aspect, comparison in paradigm_comparison.items():
        print(f"\nğŸ“Š {aspect}:")
        for paradigm, description in comparison.items():
            print(f"   {paradigm}: {description}")
    
    return paradigm_comparison

paradigm_comp = frequentist_vs_bayesian()
```

### ğŸ² ë² ì´ì§€ì•ˆ ê°€ì„¤ê²€ì • ì‹¤ìŠµ

```python
def bayesian_hypothesis_testing():
    """ë² ì´ì§€ì•ˆ ê°€ì„¤ê²€ì •ì˜ ì‹¤ì œ ì ìš©"""
    
    import numpy as np
    from scipy import stats
    
    print("ğŸ² ë² ì´ì§€ì•ˆ ê°€ì„¤ê²€ì • ì‹¤ìŠµ")
    print("=" * 35)
    
    # ì˜ˆì‹œ: ë™ì „ì˜ ê³µì •ì„± ê²€ì •
    def bayesian_coin_test(heads, total_flips, prior_alpha=1, prior_beta=1):
        """ë² ì´ì§€ì•ˆ ë™ì „ ê³µì •ì„± ê²€ì •"""
        
        # ì‚¬ì „ë¶„í¬: Beta(Î±, Î²)
        # ì‚¬í›„ë¶„í¬: Beta(Î± + heads, Î² + tails)
        posterior_alpha = prior_alpha + heads
        posterior_beta = prior_beta + (total_flips - heads)
        
        # ì‚¬í›„ í‰ê·  (ì¶”ì •ê°’)
        posterior_mean = posterior_alpha / (posterior_alpha + posterior_beta)
        
        # 95% ì‹ ë¢°êµ¬ê°„
        credible_interval = stats.beta.interval(
            0.95, posterior_alpha, posterior_beta
        )
        
        # Hâ‚€: p = 0.5ì— ëŒ€í•œ ì‚¬í›„í™•ë¥ 
        # 0.5 ì£¼ë³€ ì‘ì€ êµ¬ê°„ì˜ í™•ë¥ ë¡œ ê·¼ì‚¬
        epsilon = 0.01
        prob_null = stats.beta.cdf(0.5 + epsilon, posterior_alpha, posterior_beta) - \
                   stats.beta.cdf(0.5 - epsilon, posterior_alpha, posterior_beta)
        
        return {
            "posterior_mean": posterior_mean,
            "credible_interval": credible_interval,
            "prob_around_null": prob_null / (2 * epsilon)  # ë°€ë„ë¡œ ë³€í™˜
        }
    
    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„ì„
    scenarios = {
        "ê³µì •í•œ ë™ì „": {"heads": 50, "total": 100},
        "í¸í–¥ëœ ë™ì „": {"heads": 70, "total": 100},
        "ì ì€ ë°ì´í„°": {"heads": 7, "total": 10}
    }
    
    print("ğŸª™ ë™ì „ ê³µì •ì„± ë² ì´ì§€ì•ˆ ê²€ì •:")
    
    for scenario, data in scenarios.items():
        result = bayesian_coin_test(data["heads"], data["total"])
        
        print(f"\n   ğŸ“Š {scenario} ({data['heads']}/{data['total']}):")
        print(f"      ì‚¬í›„ í‰ê· : {result['posterior_mean']:.3f}")
        print(f"      95% ì‹ ë¢°êµ¬ê°„: ({result['credible_interval'][0]:.3f}, {result['credible_interval'][1]:.3f})")
        
        # ê³µì •ì„± íŒë‹¨
        if 0.5 >= result['credible_interval'][0] and 0.5 <= result['credible_interval'][1]:
            print(f"      ê²°ë¡ : ê³µì •í•  ê°€ëŠ¥ì„± ìˆìŒ")
        else:
            print(f"      ê²°ë¡ : í¸í–¥ë˜ì—ˆì„ ê°€ëŠ¥ì„± ë†’ìŒ")
    
    return bayesian_coin_test

bayes_coin_test = bayesian_hypothesis_testing()
```

---

## ğŸ’¼ ì‹¤ë¬´ ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬

### ğŸ¯ í†µê³„ì  ì˜ì‚¬ê²°ì • ì²´í¬ë¦¬ìŠ¤íŠ¸

```python
def statistical_decision_framework():
    """ì‹¤ë¬´ìš© í†µê³„ì  ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬"""
    
    framework = {
        "1ë‹¨ê³„: ë¬¸ì œ ì •ì˜": {
            "í•µì‹¬ ì§ˆë¬¸": [
                "í•´ê²°í•˜ë ¤ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œê°€ ë¬´ì—‡ì¸ê°€?",
                "í†µê³„ì  ê²€ì •ì´ ì •ë§ í•„ìš”í•œê°€?",
                "ì˜ì‚¬ê²°ì •ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ì–¼ë§ˆë‚˜ í°ê°€?"
            ],
            "ì²´í¬í¬ì¸íŠ¸": [
                "âœ… ëª…í™•í•œ ë¬¸ì œ ì •ì˜",
                "âœ… ì´í•´ê´€ê³„ì ì‹ë³„",
                "âœ… ì˜ì‚¬ê²°ì • ê¸°ì¤€ ì„¤ì •"
            ]
        },
        
        "2ë‹¨ê³„: ì˜¤ë¥˜ ë¹„ìš© ë¶„ì„": {
            "í•µì‹¬ ì§ˆë¬¸": [
                "ì œ1ì¢… ì˜¤ë¥˜ì˜ ë¹„ìš©ì€ ì–¼ë§ˆë‚˜ í°ê°€?",
                "ì œ2ì¢… ì˜¤ë¥˜ì˜ ë¹„ìš©ì€ ì–¼ë§ˆë‚˜ í°ê°€?",
                "ì–´ë–¤ ì˜¤ë¥˜ê°€ ë” ì¹˜ëª…ì ì¸ê°€?"
            ],
            "ì²´í¬í¬ì¸íŠ¸": [
                "âœ… ì˜¤ë¥˜ë³„ ë¹„ìš© ì •ëŸ‰í™”",
                "âœ… ë¦¬ìŠ¤í¬ í—ˆìš© ìˆ˜ì¤€ ê²°ì •",
                "âœ… ìœ ì˜ìˆ˜ì¤€ ì„¤ì • ê·¼ê±° ëª…í™•í™”"
            ]
        },
        
        "3ë‹¨ê³„: ê²€ì • ì„¤ê³„": {
            "í•µì‹¬ ì§ˆë¬¸": [
                "ì ì ˆí•œ ê²€ì • ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€?",
                "í•„ìš”í•œ í‘œë³¸ í¬ê¸°ëŠ” ì–¼ë§ˆì¸ê°€?",
                "ì‹¤ë¬´ì ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” íš¨ê³¼í¬ê¸°ëŠ” ì–¼ë§ˆì¸ê°€?"
            ],
            "ì²´í¬í¬ì¸íŠ¸": [
                "âœ… ê²€ì •ë ¥ ë¶„ì„ ìˆ˜í–‰",
                "âœ… í‘œë³¸ í¬ê¸° ê³„ì‚°",
                "âœ… ì‹¤í—˜ ì„¤ê³„ ìµœì í™”"
            ]
        },
        
        "4ë‹¨ê³„: ê²°ê³¼ í•´ì„": {
            "í•µì‹¬ ì§ˆë¬¸": [
                "í†µê³„ì  ìœ ì˜ì„±ì´ ì‹¤ë¬´ì  ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ê°€?",
                "íš¨ê³¼í¬ê¸°ê°€ ì‹¤ì§ˆì ìœ¼ë¡œ ì¤‘ìš”í•œê°€?",
                "ê²°ê³¼ê°€ ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ì—ì„œ í•©ë¦¬ì ì¸ê°€?"
            ],
            "ì²´í¬í¬ì¸íŠ¸": [
                "âœ… P-valueì™€ íš¨ê³¼í¬ê¸° í•¨ê»˜ ê³ ë ¤",
                "âœ… ì‹ ë¢°êµ¬ê°„ í•´ì„",
                "âœ… ì‹¤ë¬´ì  ê¶Œê³ ì‚¬í•­ ë„ì¶œ"
            ]
        },
        
        "5ë‹¨ê³„: ì˜ì‚¬ê²°ì •ê³¼ í›„ì†ì¡°ì¹˜": {
            "í•µì‹¬ ì§ˆë¬¸": [
                "ì´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ í–‰ë™ì„ ì·¨í•  ê²ƒì¸ê°€?",
                "ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•œê°€?",
                "ëª¨ë‹ˆí„°ë§ ê³„íšì€ ë¬´ì—‡ì¸ê°€?"
            ],
            "ì²´í¬í¬ì¸íŠ¸": [
                "âœ… ëª…í™•í•œ í–‰ë™ ê³„íš",
                "âœ… ìœ„í—˜ ê´€ë¦¬ ë°©ì•ˆ",
                "âœ… ì„±ê³¼ ì¶”ì  ê³„íš"
            ]
        }
    }
    
    print("ğŸ’¼ ì‹¤ë¬´ìš© í†µê³„ì  ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬")
    print("=" * 50)
    
    for stage, details in framework.items():
        print(f"\nğŸ“‹ {stage}")
        print("-" * 30)
        
        print("   ğŸ” í•µì‹¬ ì§ˆë¬¸:")
        for question in details["í•µì‹¬ ì§ˆë¬¸"]:
            print(f"     â€¢ {question}")
        
        print("   âœ… ì²´í¬í¬ì¸íŠ¸:")
        for checkpoint in details["ì²´í¬í¬ì¸íŠ¸"]:
            print(f"     {checkpoint}")
    
    return framework

decision_framework = statistical_decision_framework()
```

### ğŸ¢ ì‚°ì—…ë³„ ì ìš© ê°€ì´ë“œ

```python
def industry_specific_guidelines():
    """ì‚°ì—…ë³„ ê°€ì„¤ê²€ì • ì ìš© ê°€ì´ë“œ"""
    
    industry_guides = {
        "ì œì•½/ì˜ë£Œ": {
            "íŠ¹ì§•": "ìƒëª…ê³¼ ì§ê²°, ê·œì œ ê°•í™”",
            "ê¶Œì¥ Î±": "0.01 ì´í•˜",
            "ì˜¤ë¥˜ ìš°ì„ ìˆœìœ„": "ì œ1ì¢… ì˜¤ë¥˜ ìµœì†Œí™”",
            "ì¶”ê°€ ê³ ë ¤ì‚¬í•­": [
                "ë‹¤ì¤‘ ì¢…ë£Œì  ë³´ì •",
                "ì„ìƒì‹œí—˜ í”„ë¡œí† ì½œ ì¤€ìˆ˜",
                "ê·œì œê¸°ê´€ ê°€ì´ë“œë¼ì¸ í™•ì¸"
            ],
            "ì‹¤ë¬´ íŒ": "ì•ˆì „ì„± ë¨¼ì €, íš¨ê³¼ì„± ë‚˜ì¤‘"
        },
        
        "ê¸ˆìœµ/íˆ¬ì": {
            "íŠ¹ì§•": "ë†’ì€ ë³€ë™ì„±, ë¹ ë¥¸ ì˜ì‚¬ê²°ì •",
            "ê¶Œì¥ Î±": "0.05 (ìƒí™©ì— ë”°ë¼ ì¡°ì •)",
            "ì˜¤ë¥˜ ìš°ì„ ìˆœìœ„": "ìƒí™©ë³„ ì°¨ë“± ì ìš©",
            "ì¶”ê°€ ê³ ë ¤ì‚¬í•­": [
                "ì‹œì¥ ë³€í™”ì— ë”°ë¥¸ ë™ì  ì¡°ì •",
                "í¬íŠ¸í´ë¦¬ì˜¤ ì°¨ì›ì˜ ë¦¬ìŠ¤í¬ ê´€ë¦¬",
                "ë°±í…ŒìŠ¤íŒ… ë° out-of-sample ê²€ì¦"
            ],
            "ì‹¤ë¬´ íŒ": "ë¦¬ìŠ¤í¬-ë¦¬í„´ íŠ¸ë ˆì´ë“œì˜¤í”„ ê³ ë ¤"
        },
        
        "í…Œí¬/IT": {
            "íŠ¹ì§•": "ë¹ ë¥¸ ì‹¤í—˜, ë°ì´í„° í’ë¶€",
            "ê¶Œì¥ Î±": "0.05~0.10 (A/B í…ŒìŠ¤íŠ¸)",
            "ì˜¤ë¥˜ ìš°ì„ ìˆœìœ„": "ì œ2ì¢… ì˜¤ë¥˜ ìµœì†Œí™”",
            "ì¶”ê°€ ê³ ë ¤ì‚¬í•­": [
                "ë‹¤ì¤‘ ì§€í‘œ ìµœì í™”",
                "ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¶„ì„",
                "ì¥ê¸° íš¨ê³¼ vs ë‹¨ê¸° íš¨ê³¼"
            ],
            "ì‹¤ë¬´ íŒ": "ë¹ ë¥¸ ì‹¤í—˜, ë¹ ë¥¸ í•™ìŠµ"
        },
        
        "ì œì¡°/í’ˆì§ˆê´€ë¦¬": {
            "íŠ¹ì§•": "ê³µì • ì•ˆì •ì„±, í’ˆì§ˆ ë³´ì¦",
            "ê¶Œì¥ Î±": "ìƒí™©ë³„ (ë¶ˆëŸ‰í’ˆ íƒì§€ vs ì •ìƒí’ˆ íê¸°)",
            "ì˜¤ë¥˜ ìš°ì„ ìˆœìœ„": "ê³µì •ì— ë”°ë¼ ë‹¤ë¦„",
            "ì¶”ê°€ ê³ ë ¤ì‚¬í•­": [
                "SPC(í†µê³„ì  ê³µì •ê´€ë¦¬) ì—°ê³„",
                "ìƒ˜í”Œë§ ì „ëµ ìµœì í™”",
                "ë¹„ìš©-í’ˆì§ˆ ê· í˜•"
            ],
            "ì‹¤ë¬´ íŒ": "ì˜ˆë°©ì´ ì¹˜ë£Œë³´ë‹¤ ê²½ì œì "
        }
    }
    
    print("ğŸ¢ ì‚°ì—…ë³„ ê°€ì„¤ê²€ì • ì ìš© ê°€ì´ë“œ")
    print("=" * 45)
    
    for industry, guide in industry_guides.items():
        print(f"\nğŸ­ {industry}")
        print("-" * 20)
        
        for aspect, details in guide.items():
            if isinstance(details, list):
                print(f"   {aspect}:")
                for item in details:
                    print(f"     â€¢ {item}")
            else:
                print(f"   {aspect}: {details}")
    
    return industry_guides

industry_guides = industry_specific_guidelines()
```

---

## ğŸ“Š ê³ ê¸‰ ì‹œê°í™”ì™€ í•´ì„

### ğŸ¨ ì‹œê°í™” ë„êµ¬í‚·

```python
def advanced_visualization_toolkit():
    """ê³ ê¸‰ í†µê³„ ì‹œê°í™” ë„êµ¬í‚·"""
    
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np
    from scipy import stats
    
    # í•œê¸€ í°íŠ¸ ì„¤ì •
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['axes.unicode_minus'] = False
    
    def plot_error_types_diagram():
        """ì œ1ì¢…Â·ì œ2ì¢… ì˜¤ë¥˜ ì‹œê°í™”"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ë¶„í¬ ìƒì„±
        x = np.linspace(-4, 8, 1000)
        null_dist = stats.norm.pdf(x, 0, 1)  # Hâ‚€ê°€ ì°¸ì¼ ë•Œ
        alt_dist = stats.norm.pdf(x, 3, 1)   # Hâ‚ì´ ì°¸ì¼ ë•Œ
        
        # ì„ê³„ê°’ ì„¤ì •
        critical_value = stats.norm.ppf(0.95, 0, 1)
        
        # ì²« ë²ˆì§¸ í”Œë¡¯: ì œ1ì¢… ì˜¤ë¥˜
        ax1.plot(x, null_dist, 'b-', label='Hâ‚€ê°€ ì°¸ (Î¼=0)', linewidth=2)
        ax1.fill_between(x[x >= critical_value], null_dist[x >= critical_value], 
                        alpha=0.3, color='red', label='ì œ1ì¢… ì˜¤ë¥˜ (Î±)')
        ax1.axvline(critical_value, color='red', linestyle='--', 
                   label=f'ì„ê³„ê°’ = {critical_value:.2f}')
        ax1.set_title('ì œ1ì¢… ì˜¤ë¥˜ (Type I Error)', fontsize=14, fontweight='bold')
        ax1.set_xlabel('ê²€ì • í†µê³„ëŸ‰')
        ax1.set_ylabel('í™•ë¥  ë°€ë„')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # ë‘ ë²ˆì§¸ í”Œë¡¯: ì œ2ì¢… ì˜¤ë¥˜
        ax2.plot(x, null_dist, 'b-', label='Hâ‚€ ë¶„í¬ (Î¼=0)', linewidth=2)
        ax2.plot(x, alt_dist, 'g-', label='Hâ‚ ë¶„í¬ (Î¼=3)', linewidth=2)
        ax2.fill_between(x[x < critical_value], alt_dist[x < critical_value], 
                        alpha=0.3, color='orange', label='ì œ2ì¢… ì˜¤ë¥˜ (Î²)')
        ax2.fill_between(x[x >= critical_value], alt_dist[x >= critical_value], 
                        alpha=0.3, color='lightgreen', label='ê²€ì •ë ¥ (1-Î²)')
        ax2.axvline(critical_value, color='red', linestyle='--', 
                   label=f'ì„ê³„ê°’ = {critical_value:.2f}')
        ax2.set_title('ì œ2ì¢… ì˜¤ë¥˜ & ê²€ì •ë ¥', fontsize=14, fontweight='bold')
        ax2.set_xlabel('ê²€ì • í†µê³„ëŸ‰')
        ax2.set_ylabel('í™•ë¥  ë°€ë„')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        return fig
    
    def plot_power_analysis():
        """ê²€ì •ë ¥ ë¶„ì„ ì‹œê°í™”"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. í‘œë³¸ í¬ê¸°ì™€ ê²€ì •ë ¥ì˜ ê´€ê³„
        sample_sizes = np.arange(10, 201, 10)
        powers = [1 - stats.norm.cdf(1.96 - 0.5 * np.sqrt(n/2)) for n in sample_sizes]
        
        ax1.plot(sample_sizes, powers, 'b-', linewidth=2, marker='o')
        ax1.axhline(y=0.8, color='r', linestyle='--', label='ê¶Œì¥ ê²€ì •ë ¥ (0.8)')
        ax1.set_title('í‘œë³¸ í¬ê¸° vs ê²€ì •ë ¥', fontweight='bold')
        ax1.set_xlabel('í‘œë³¸ í¬ê¸°')
        ax1.set_ylabel('ê²€ì •ë ¥')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # 2. íš¨ê³¼í¬ê¸°ì™€ ê²€ì •ë ¥ì˜ ê´€ê³„
        effect_sizes = np.linspace(0.1, 1.5, 50)
        powers_effect = [1 - stats.norm.cdf(1.96 - es * np.sqrt(50/2)) for es in effect_sizes]
        
        ax2.plot(effect_sizes, powers_effect, 'g-', linewidth=2)
        ax2.axhline(y=0.8, color='r', linestyle='--', label='ê¶Œì¥ ê²€ì •ë ¥ (0.8)')
        ax2.axvline(x=0.5, color='orange', linestyle='--', label='ì¤‘ê°„ íš¨ê³¼í¬ê¸°')
        ax2.set_title('íš¨ê³¼í¬ê¸° vs ê²€ì •ë ¥', fontweight='bold')
        ax2.set_xlabel('íš¨ê³¼í¬ê¸° (Cohen\\'s d)')
        ax2.set_ylabel('ê²€ì •ë ¥')
        ax2.grid(True, alpha=0.3)
        ax2.legend()
        
        # 3. ìœ ì˜ìˆ˜ì¤€ê³¼ ê²€ì •ë ¥ì˜ ê´€ê³„
        alphas = np.linspace(0.01, 0.20, 50)
        powers_alpha = [1 - stats.norm.cdf(stats.norm.ppf(1-a/2) - 0.5 * np.sqrt(50/2)) 
                       for a in alphas]
        
        ax3.plot(alphas, powers_alpha, 'purple', linewidth=2)
        ax3.axvline(x=0.05, color='r', linestyle='--', label='ì¼ë°˜ì  Î± (0.05)')
        ax3.set_title('ìœ ì˜ìˆ˜ì¤€ vs ê²€ì •ë ¥', fontweight='bold')
        ax3.set_xlabel('ìœ ì˜ìˆ˜ì¤€ (Î±)')
        ax3.set_ylabel('ê²€ì •ë ¥')
        ax3.grid(True, alpha=0.3)
        ax3.legend()
        
        # 4. ROC ê³¡ì„  ìŠ¤íƒ€ì¼ì˜ ì˜¤ë¥˜ íŠ¸ë ˆì´ë“œì˜¤í”„
        alphas_roc = np.linspace(0.001, 0.5, 100)
        type1_errors = alphas_roc
        type2_errors = [stats.norm.cdf(stats.norm.ppf(1-a/2) - 0.5 * np.sqrt(50/2)) 
                       for a in alphas_roc]
        
        ax4.plot(type1_errors, type2_errors, 'red', linewidth=2)
        ax4.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='ëœë¤ ì„ íƒ')
        ax4.set_title('ì œ1ì¢… vs ì œ2ì¢… ì˜¤ë¥˜ íŠ¸ë ˆì´ë“œì˜¤í”„', fontweight='bold')
        ax4.set_xlabel('ì œ1ì¢… ì˜¤ë¥˜ í™•ë¥  (Î±)')
        ax4.set_ylabel('ì œ2ì¢… ì˜¤ë¥˜ í™•ë¥  (Î²)')
        ax4.grid(True, alpha=0.3)
        ax4.legend()
        
        plt.tight_layout()
        return fig
    
    print("ğŸ¨ ê³ ê¸‰ í†µê³„ ì‹œê°í™” ë„êµ¬í‚·")
    print("=" * 35)
    
    # ì‹œê°í™” ìƒì„±
    error_fig = plot_error_types_diagram()
    power_fig = plot_power_analysis()
    
    print("âœ… ì œ1ì¢…Â·ì œ2ì¢… ì˜¤ë¥˜ ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì™„ë£Œ")
    print("âœ… ê²€ì •ë ¥ ë¶„ì„ ì°¨íŠ¸ ìƒì„± ì™„ë£Œ")
    
    return plot_error_types_diagram, plot_power_analysis

viz_toolkit = advanced_visualization_toolkit()
```

### ğŸ“ˆ ëŒ€ì‹œë³´ë“œ ìŠ¤íƒ€ì¼ ì¢…í•© ë¦¬í¬íŠ¸

```python
def comprehensive_hypothesis_testing_report():
    """ì¢…í•©ì ì¸ ê°€ì„¤ê²€ì • ë¦¬í¬íŠ¸ ìƒì„±"""
    
    def generate_report(data, test_type="t-test", alpha=0.05, **kwargs):
        """ê°€ì„¤ê²€ì • ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        
        import pandas as pd
        from scipy import stats
        import numpy as np
        
        print("ğŸ“Š ê°€ì„¤ê²€ì • ì¢…í•© ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        # 1. ë°ì´í„° ê¸°ë³¸ ì •ë³´
        print("ğŸ“‹ 1. ë°ì´í„° ê°œìš”")
        print("-" * 30)
        
        if isinstance(data, (list, np.ndarray)):
            data = np.array(data)
            n = len(data)
            mean = np.mean(data)
            std = np.std(data, ddof=1)
            
            print(f"   í‘œë³¸ í¬ê¸°: {n}")
            print(f"   í‰ê· : {mean:.4f}")
            print(f"   í‘œì¤€í¸ì°¨: {std:.4f}")
            print(f"   ë³€ë™ê³„ìˆ˜: {std/mean*100:.2f}%")
        
        # 2. ê²€ì • ì‹¤í–‰
        print(f"\nğŸ”¬ 2. ê²€ì • ê²°ê³¼ ({test_type})")
        print("-" * 30)
        
        if test_type == "t-test":
            test_value = kwargs.get('test_value', 0)
            t_stat, p_value = stats.ttest_1samp(data, test_value)
            
            print(f"   ê·€ë¬´ê°€ì„¤: Î¼ = {test_value}")
            print(f"   ëŒ€ë¦½ê°€ì„¤: Î¼ â‰  {test_value}")
            print(f"   t-í†µê³„ëŸ‰: {t_stat:.4f}")
            print(f"   ììœ ë„: {n-1}")
            print(f"   P-value: {p_value:.6f}")
            
            # íš¨ê³¼í¬ê¸° ê³„ì‚°
            cohens_d = (mean - test_value) / std
            print(f"   Cohen's d: {cohens_d:.4f}")
            
            # ì‹ ë¢°êµ¬ê°„
            ci = stats.t.interval(1-alpha, n-1, mean, std/np.sqrt(n))
            print(f"   {(1-alpha)*100}% ì‹ ë¢°êµ¬ê°„: ({ci[0]:.4f}, {ci[1]:.4f})")
        
        # 3. ê²°ê³¼ í•´ì„
        print(f"\nğŸ“Š 3. ê²°ê³¼ í•´ì„")
        print("-" * 30)
        
        if p_value < 0.001:
            significance = "ë§¤ìš° ê°•í•œ ì¦ê±° (***)"
            strength = "ğŸ”´"
        elif p_value < 0.01:
            significance = "ê°•í•œ ì¦ê±° (**)"
            strength = "ğŸŸ "
        elif p_value < alpha:
            significance = "ë³´í†µ ì¦ê±° (*)"
            strength = "ğŸŸ¡"
        elif p_value < 0.10:
            significance = "ì•½í•œ ì¦ê±° (Â·)"
            strength = "ğŸŸ¢"
        else:
            significance = "ì¦ê±° ì—†ìŒ (n.s.)"
            strength = "âšª"
        
        print(f"   í†µê³„ì  ìœ ì˜ì„±: {strength} {significance}")
        
        if p_value < alpha:
            decision = "Hâ‚€ ê¸°ê°"
            conclusion = "í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ìˆìŒ"
        else:
            decision = "Hâ‚€ ì±„íƒ"
            conclusion = "í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ì—†ìŒ"
        
        print(f"   ì˜ì‚¬ê²°ì •: {decision}")
        print(f"   ê²°ë¡ : {conclusion}")
        
        # 4. ì‹¤ë¬´ì  ê¶Œê³ 
        print(f"\nğŸ’¡ 4. ì‹¤ë¬´ì  ê¶Œê³ ì‚¬í•­")
        print("-" * 30)
        
        recommendations = []
        
        if abs(cohens_d) < 0.2:
            recommendations.append("íš¨ê³¼í¬ê¸°ê°€ ì‘ìŒ - ì‹¤ë¬´ì  ì˜ë¯¸ ì¬ê²€í†  í•„ìš”")
        elif abs(cohens_d) < 0.5:
            recommendations.append("ì¤‘ê°„ ì •ë„ íš¨ê³¼í¬ê¸° - ë¹„ìš©-í¸ìµ ë¶„ì„ ê¶Œì¥")
        else:
            recommendations.append("í° íš¨ê³¼í¬ê¸° - ì‹¤ë¬´ì ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ì°¨ì´")
        
        if n < 30:
            recommendations.append("í‘œë³¸ í¬ê¸°ê°€ ì‘ìŒ - ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ê³ ë ¤")
        
        if p_value < alpha and abs(cohens_d) < 0.2:
            recommendations.append("í†µê³„ì  ìœ ì˜í•˜ì§€ë§Œ íš¨ê³¼ ì‘ìŒ - ì‹¤ë¬´ ì ìš© ì‹ ì¤‘ ê²€í† ")
        
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        # 5. ê²€ì •ë ¥ ë¶„ì„
        print(f"\nâš¡ 5. ê²€ì •ë ¥ ë¶„ì„")
        print("-" * 30)
        
        # ì‚¬í›„ ê²€ì •ë ¥ ê³„ì‚° (ê·¼ì‚¬)
        if test_type == "t-test":
            ncp = abs(t_stat)  # ë¹„ì¤‘ì‹¬ëª¨ìˆ˜
            power = 1 - stats.t.cdf(stats.t.ppf(1-alpha/2, n-1), n-1, ncp)
            print(f"   ê´€ì°°ëœ ê²€ì •ë ¥: {power:.3f} ({power*100:.1f}%)")
            
            if power < 0.8:
                print("   âš ï¸ ê²€ì •ë ¥ì´ ë‚®ìŒ - í‘œë³¸ í¬ê¸° ì¦ê°€ ê²€í† ")
            else:
                print("   âœ… ì¶©ë¶„í•œ ê²€ì •ë ¥")
        
        return {
            "p_value": p_value,
            "effect_size": cohens_d if test_type == "t-test" else None,
            "decision": decision,
            "significance": significance,
            "power": power if test_type == "t-test" else None
        }
    
    # ì˜ˆì‹œ ë°ì´í„°ë¡œ ë¦¬í¬íŠ¸ ìƒì„±
    np.random.seed(42)
    sample_data = np.random.normal(152, 10, 50)
    
    print("ğŸ§ª ì˜ˆì‹œ: ìŒì‹ì  ë¶„ëŸ‰ ê²€ì¦ ë¦¬í¬íŠ¸")
    result = generate_report(sample_data, "t-test", test_value=150, alpha=0.05)
    
    return generate_report

report_generator = comprehensive_hypothesis_testing_report()
```

---

## ğŸ“ í•™ìŠµ ì™„ì„±ë„ ì²´í¬

### âœ… ë§ˆìŠ¤í„°ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

```python
def mastery_checklist():
    """ê°€ì„¤ê²€ì • ë§ˆìŠ¤í„°ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸"""
    
    checklist = {
        "ğŸŸ¢ ê¸°ì´ˆ ê°œë… (Basic Concepts)": {
            "ì œ1ì¢…Â·ì œ2ì¢… ì˜¤ë¥˜ ì •ì˜": "â“ ì™„ë²½íˆ ì´í•´í•˜ê³  ì„¤ëª…í•  ìˆ˜ ìˆëŠ”ê°€?",
            "P-value í•´ì„": "â“ ì •í™•í•œ ì˜ë¯¸ë¥¼ ì•„ëŠ”ê°€?",
            "ìœ ì˜ìˆ˜ì¤€ ì„¤ì •": "â“ ìƒí™©ì— ë§ëŠ” Î± ì„ íƒ ê°€ëŠ¥í•œê°€?",
            "íš¨ê³¼í¬ê¸° ê³„ì‚°": "â“ Cohen's dë¥¼ ê³„ì‚°í•˜ê³  í•´ì„í•  ìˆ˜ ìˆëŠ”ê°€?"
        },
        
        "ğŸŸ¡ ì¤‘ê¸‰ ì‘ìš© (Intermediate Application)": {
            "ê²€ì •ë ¥ ë¶„ì„": "â“ í‘œë³¸ í¬ê¸°ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ”ê°€?",
            "