# 22.00 í†µê³„ ì…ë¬¸ - ë°ì´í„° ë¶„ì„ê³¼ AIì˜ ê¸°ì´ˆ

> **ğŸ“‹ ì¶œì²˜**: ê±´ê°•ë³´í—˜ì‹¬ì‚¬í‰ê°€ì›, "íŒŒì´ì¬ì„ í™œìš©í•œ ë°ì´í„°Â·AI ë¶„ì„ ì‚¬ë¡€"  
> **ğŸŒ ì›¹ì‚¬ì´íŠ¸**: www.hira.or.kr  
> **ğŸ“ í¸ì§‘**: í•™ìŠµ ëª©ì ìœ¼ë¡œ ì¬êµ¬ì„± ë° í™•ì¥

## ğŸ¯ ì „ì²´ ê°œìš”
**í†µê³„í•™ì€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ì™€ AIì˜ ê·¼ê°„ì´ ë˜ëŠ” í•™ë¬¸ì…ë‹ˆë‹¤.** ë¹…ë°ì´í„° ì‹œëŒ€ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , ë¶„ì„í•˜ê³ , í•´ì„í•˜ì—¬ ê°€ì¹˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ëŠ” ëª¨ë“  ê³¼ì •ì˜ ê¸°ì´ˆê°€ ë°”ë¡œ í†µê³„í•™ì…ë‹ˆë‹¤.

> **"í‘œë³¸ í†µê³„ëŸ‰ì„ êµ¬í•´ì„œ ëª¨ì§‘ë‹¨ì´ ê·¸ëŸ¬í•  ê±°ë‹¤ë¼ê³  ì¶”ë¡ í•˜ëŠ” ê²ë‹ˆë‹¤. ì´ê²Œ ì¶”ë¡  í†µê³„ì…ë‹ˆë‹¤."**

---

## ğŸ“š í†µê³„ í•™ìŠµ ë¡œë“œë§µ

### ğŸ—ºï¸ **í•™ìŠµ ë‹¨ê³„ë³„ êµ¬ì„±**

```mermaid
graph TD
    A[22.00 í†µê³„ ì…ë¬¸] --> B[ë¹…ë°ì´í„° ê°œë…]
    A --> C[ë°ì´í„° ë¶„ì„ í•„ìˆ˜ì§€ì‹]
    A --> D[ëª¨ë¸í‰ê°€ ê¸°ë²•]
    
    B --> E[22.01 ê¸°ìˆ í†µê³„]
    C --> E
    D --> E
    
    E --> F[22.02 ì¶”ë¡ í†µê³„]
    
    F --> G[í†µê³„ ë¶„ì„ ê¸°ë²•]
    G --> H[AIë¶„ì„ëª¨í˜•]
    H --> I[ë”¥ëŸ¬ë‹]
    
    style A fill:#ff9999
    style E fill:#99ccff
    style F fill:#99ccff
    style G fill:#99ff99
    style H fill:#ffcc99
    style I fill:#ff99cc
```

---

## ğŸ“– **I. ë¹…ë°ì´í„°ë€?(What is Big Data?)**

### ğŸ” **ë¹…ë°ì´í„°ì˜ ì •ì˜**

ë¹…ë°ì´í„°ëŠ” ë‹¨ìˆœíˆ ë°ì´í„°ê°€ ë§ë‹¤ëŠ” ì˜ë¯¸ë¥¼ ë„˜ì–´ì„œ, **ê¸°ì¡´ì˜ ê´€ë¦¬ ë° ë¶„ì„ ì²´ê³„ë¡œëŠ” ê°ë‹¹í•  ìˆ˜ ì—†ì„ ì •ë„ì˜ ê±°ëŒ€í•œ ë°ì´í„°ì™€ ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ëª¨ë“  ê¸°ìˆ ì„ í¬ê´„**í•˜ëŠ” ê°œë…ì…ë‹ˆë‹¤.

#### **ê°€íŠ¸ë„ˆì˜ ë¹…ë°ì´í„° 5V**
```python
bigdata_5v = {
    "Volume (ê·œëª¨)": "ëŒ€ìš©ëŸ‰ì˜ ë°ì´í„°",
    "Velocity (ì†ë„)": "ì‹¤ì‹œê°„ ì²˜ë¦¬ ìš”êµ¬",
    "Variety (ë‹¤ì–‘ì„±)": "ì •í˜•/ë¹„ì •í˜• ë°ì´í„° í˜¼ì¬",
    "Veracity (ì •í™•ì„±)": "ë°ì´í„°ì˜ ì‹ ë¢°ì„±ê³¼ í’ˆì§ˆ",
    "Value (ê°€ì¹˜)": "ë¶„ì„ì„ í†µí•œ ì‹¤ì§ˆì  ê°€ì¹˜ ì°½ì¶œ"
}

for v, description in bigdata_5v.items():
    print(f"ğŸ“Š {v}: {description}")
```

### ğŸ“ˆ **DIKW í”¼ë¼ë¯¸ë“œ - ë°ì´í„°ì—ì„œ ì§€í˜œê¹Œì§€**

```python
def dikw_pyramid():
    """ë°ì´í„° â†’ ì •ë³´ â†’ ì§€ì‹ â†’ ì§€í˜œì˜ ë³€í™˜ ê³¼ì •"""
    
    dikw = {
        "ë°ì´í„°(Data)": {
            "ì •ì˜": "ê°€ê³µë˜ì§€ ì•Šì€ ìˆœìˆ˜í•œ ìˆ˜ì¹˜ë‚˜ ê¸°í˜¸",
            "ì˜ˆì‹œ": "Aë§ˆíŠ¸ëŠ” 100ì›, Bë§ˆíŠ¸ëŠ” 200ì›ì— ì—°í•„ íŒë§¤",
            "íŠ¹ì§•": "íƒ€ ë°ì´í„°ì™€ ìƒê´€ê´€ê³„ ì—†ëŠ” ì›ì‹œ ìë£Œ"
        },
        
        "ì •ë³´(Information)": {
            "ì •ì˜": "ì˜ë¯¸ ìˆê³  ìœ ìš©í•œ í˜•íƒœë¡œ ê°€ê³µëœ ë°ì´í„°",
            "ì˜ˆì‹œ": "Aë§ˆíŠ¸ì˜ ì—°í•„ì´ ë” ì‹¸ë‹¤",
            "íŠ¹ì§•": "ë°ì´í„° ê°„ ìƒê´€ê´€ê³„ ì´í•´ë¥¼ í†µí•œ íŒ¨í„´ ì¸ì‹"
        },
        
        "ì§€ì‹(Knowledge)": {
            "ì •ì˜": "ìƒí˜¸ ì—°ê²°ëœ ì •ë³´ íŒ¨í„´ì„ ì´í•´í•œ ì˜ˆì¸¡ ê²°ê³¼ë¬¼",
            "ì˜ˆì‹œ": "ìƒëŒ€ì ìœ¼ë¡œ ì €ë ´í•œ Aë§ˆíŠ¸ì—ì„œ ì—°í•„ì„ ì‚¬ì•¼ê² ë‹¤",
            "íŠ¹ì§•": "ì •ë³´ íŒ¨í„´ ê¸°ë°˜ì˜ ì˜ì‚¬ê²°ì • ëŠ¥ë ¥"
        },
        
        "ì§€í˜œ(Wisdom)": {
            "ì •ì˜": "ê·¼ë³¸ ì›ë¦¬ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì°½ì˜ì  ì•„ì´ë””ì–´",
            "ì˜ˆì‹œ": "Aë§ˆíŠ¸ì˜ ë‹¤ë¥¸ ìƒí’ˆë“¤ë„ Bë§ˆíŠ¸ë³´ë‹¤ ìŒ€ ê²ƒì´ë¼ íŒë‹¨",
            "íŠ¹ì§•": "ê¹Šì€ ì´í•´ì™€ í†µì°°ë ¥ ê¸°ë°˜ì˜ ì¼ë°˜í™”"
        }
    }
    
    print("ğŸ›ï¸ DIKW í”¼ë¼ë¯¸ë“œ - ë°ì´í„° ê°€ì¹˜ ì°½ì¶œ ê³¼ì •")
    print("=" * 60)
    
    for level, info in dikw.items():
        print(f"\nğŸ“Š {level}")
        print(f"   ì •ì˜: {info['ì •ì˜']}")
        print(f"   ì˜ˆì‹œ: {info['ì˜ˆì‹œ']}")
        print(f"   íŠ¹ì§•: {info['íŠ¹ì§•']}")

dikw_pyramid()
```

### ğŸ”„ **ë¹…ë°ì´í„°ê°€ ë§Œë“¤ì–´ë‚´ëŠ” ë³¸ì§ˆì  ë³€í™”**

#### **ë¶„ì„ íŒ¨ëŸ¬ë‹¤ì„ì˜ ì „í™˜**
| êµ¬ë¶„ | ê³¼ê±° ë°©ì‹ | ë¹…ë°ì´í„° ë°©ì‹ |
|------|----------|---------------|
| **ë°ì´í„° ì²˜ë¦¬** | ì‚¬ì „ì²˜ë¦¬ â†’ í•„ìš”í•œ ê²ƒë§Œ | ì‚¬í›„ì²˜ë¦¬ â†’ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ |
| **ì¡°ì‚¬ ë°©ë²•** | í‘œë³¸ì¡°ì‚¬ (Sampling) | ì „ìˆ˜ì¡°ì‚¬ (Population) |
| **í’ˆì§ˆ ì¤‘ì‹¬** | ì§ˆ(Quality) ìš°ì„  | ì–‘(Quantity) ìš°ì„  |
| **ë¶„ì„ ê´€ì ** | ì¸ê³¼ê´€ê³„ (Causation) | ìƒê´€ê´€ê³„ (Correlation) |

---

## ğŸ“Š **II. ë°ì´í„° ë¶„ì„ í•„ìˆ˜ì§€ì‹(Data Analysis Knowledge)**

### ğŸ“ˆ **ê¸°ìˆ í†µê³„ vs ì¶”ë¡ í†µê³„**

```python
def statistics_comparison():
    """ê¸°ìˆ í†µê³„ì™€ ì¶”ë¡ í†µê³„ì˜ ì°¨ì´ì  ì •ë¦¬"""
    
    comparison = {
        "ê¸°ìˆ í†µê³„(Descriptive)": {
            "ëª©ì ": "í˜„ì¬ ë°ì´í„° ìì²´ë¥¼ ì„¤ëª…",
            "ë°©ë²•": "í‰ê· , ë¶„ì‚°, í‘œì¤€í¸ì°¨, ìµœë¹ˆê°’, ì¤‘ì•™ê°’",
            "íŠ¹ì§•": "'ì´ë ‡ë‹¤' (í˜„í™© íŒŒì•…)",
            "ê²°ê³¼": "ë°ì´í„°ì˜ ê³„ëŸ‰í™”ëœ ìš”ì•½",
            "ì˜ˆì‹œ": "ì•¼êµ¬ì„ ìˆ˜ì˜ íƒ€ìœ¨, íˆ¬ìˆ˜ì˜ ë°©ì–´ìœ¨"
        },
        
        "ì¶”ë¡ í†µê³„(Inferential)": {
            "ëª©ì ": "í‘œë³¸ìœ¼ë¡œ ëª¨ì§‘ë‹¨ì„ ì¶”ë¡ ",
            "ë°©ë²•": "ê°€ì„¤ê²€ì •, íšŒê·€ë¶„ì„, ì˜ˆì¸¡ ëª¨ë¸ë§",
            "íŠ¹ì§•": "'ì´ëŸ´ ê²ƒì´ë‹¤' (ë¯¸ë˜ ì˜ˆì¸¡)",
            "ê²°ê³¼": "í†µê³„ì  ê²°ë¡  ë° ì˜ì‚¬ê²°ì •",
            "ì˜ˆì‹œ": "ì¶œêµ¬ì¡°ì‚¬ë¡œ ì„ ê±° ê²°ê³¼ ì˜ˆì¸¡"
        }
    }
    
    print("ğŸ“Š ê¸°ìˆ í†µê³„ vs ì¶”ë¡ í†µê³„ ì™„ì „ ë¹„êµ")
    print("=" * 50)
    
    for stat_type, details in comparison.items():
        print(f"\nğŸ” {stat_type}")
        for key, value in details.items():
            print(f"   {key}: {value}")

statistics_comparison()
```

### ğŸ“Š **ì£¼ìš” í†µê³„ëŸ‰ ì •ë¦¬**

#### **1ï¸âƒ£ ì¤‘ì‹¬ê²½í–¥ì¹˜ (Central Tendency)**
```python
import numpy as np
import pandas as pd

def central_tendency_examples():
    """ì¤‘ì‹¬ê²½í–¥ì¹˜ ê³„ì‚° ì˜ˆì œ"""
    
    # ì˜ˆì œ ë°ì´í„°: í•™ìƒë“¤ì˜ ì‹œí—˜ ì ìˆ˜
    scores = [85, 90, 78, 92, 88, 76, 95, 82, 89, 91]
    
    # ì‚°ìˆ í‰ê· 
    mean_score = np.mean(scores)
    
    # ì¤‘ì•™ê°’
    median_score = np.median(scores)
    
    # ìµœë¹ˆê°’ (pandas ì‚¬ìš©)
    df = pd.DataFrame({'scores': scores})
    mode_score = df['scores'].mode().iloc[0]
    
    print("ğŸ“Š ì¤‘ì‹¬ê²½í–¥ì¹˜ ê³„ì‚° ê²°ê³¼")
    print(f"ë°ì´í„°: {scores}")
    print(f"í‰ê· (Mean): {mean_score:.2f}ì ")
    print(f"ì¤‘ì•™ê°’(Median): {median_score:.2f}ì ")
    print(f"ìµœë¹ˆê°’(Mode): {mode_score}ì ")
    
    return mean_score, median_score, mode_score

central_tendency_examples()
```

#### **2ï¸âƒ£ ì‚°í¬ë„ (Dispersion)**
```python
def dispersion_measures():
    """ì‚°í¬ë„ ì¸¡ì • ì˜ˆì œ"""
    
    scores = [85, 90, 78, 92, 88, 76, 95, 82, 89, 91]
    
    # ë¶„ì‚° (í‘œë³¸ë¶„ì‚°)
    variance = np.var(scores, ddof=1)
    
    # í‘œì¤€í¸ì°¨
    std_dev = np.std(scores, ddof=1)
    
    # ë²”ìœ„
    data_range = max(scores) - min(scores)
    
    # ì‚¬ë¶„ìœ„ë²”ìœ„ (IQR)
    q1 = np.percentile(scores, 25)
    q3 = np.percentile(scores, 75)
    iqr = q3 - q1
    
    print("\nğŸ“Š ì‚°í¬ë„ ì¸¡ì • ê²°ê³¼")
    print(f"ë¶„ì‚°(Variance): {variance:.2f}")
    print(f"í‘œì¤€í¸ì°¨(Std Dev): {std_dev:.2f}")
    print(f"ë²”ìœ„(Range): {data_range}")
    print(f"ì‚¬ë¶„ìœ„ë²”ìœ„(IQR): {iqr:.2f}")
    
    return variance, std_dev, iqr

dispersion_measures()
```

### ğŸ² **í™•ë¥ ë¶„í¬ì™€ í‘œë³¸ì¶”ì¶œ**

#### **í‘œë³¸ì¶”ì¶œì˜ ì¢…ë¥˜**
```python
def sampling_methods():
    """ë‹¤ì–‘í•œ í‘œë³¸ì¶”ì¶œ ë°©ë²• ì‹œì—°"""
    
    # ëª¨ì§‘ë‹¨ ìƒì„± (1000ëª…ì˜ í•™ìƒ ì„±ì )
    np.random.seed(42)
    population = np.random.normal(75, 15, 1000)  # í‰ê·  75, í‘œì¤€í¸ì°¨ 15
    
    sampling_methods = {
        "ë‹¨ìˆœì„ì˜ì¶”ì¶œ": {
            "ë°©ë²•": "ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œ",
            "ì½”ë“œ": "np.random.choice(population, 100, replace=False)"
        },
        
        "ì¸µí™”ì„ì˜ì¶”ì¶œ": {
            "ë°©ë²•": "ê³„ì¸µë³„ë¡œ ë‚˜ëˆ„ì–´ ê°ê°ì—ì„œ ì¶”ì¶œ", 
            "ì„¤ëª…": "ì„±ë³„, í•™ë…„ ë“±ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¶”ì¶œ"
        },
        
        "ì²´ê³„ì ì¶”ì¶œ": {
            "ë°©ë²•": "ì¼ì •í•œ ê°„ê²©ìœ¼ë¡œ ì¶”ì¶œ",
            "ì„¤ëª…": "ì²« ë²ˆì§¸ëŠ” ì„ì˜, ì´í›„ëŠ” ë™ì¼ ê°„ê²©"
        },
        
        "ì§‘ë½ì¶”ì¶œ": {
            "ë°©ë²•": "í´ëŸ¬ìŠ¤í„° ì „ì²´ë¥¼ ì„ íƒ",
            "ì„¤ëª…": "íŠ¹ì • í•™êµ, ì§€ì—­ ì „ì²´ ì„ íƒ"
        }
    }
    
    print("ğŸ¯ í‘œë³¸ì¶”ì¶œ ë°©ë²•ë¡ ")
    print("=" * 40)
    
    for method, details in sampling_methods.items():
        print(f"\nğŸ“Š {method}")
        print(f"   ë°©ë²•: {details['ë°©ë²•']}")
        if 'ì½”ë“œ' in details:
            print(f"   ì½”ë“œ: {details['ì½”ë“œ']}")
        if 'ì„¤ëª…' in details:
            print(f"   ì„¤ëª…: {details['ì„¤ëª…']}")

sampling_methods()
```

---

## ğŸ“Š **III. ë°ì´í„° ë¶„ì„ ëª¨ë¸í‰ê°€(Data Analysis Model Assessment)**

### ğŸ” **ë¶„ë¥˜ì„±ëŠ¥í‰ê°€ - ì˜¤ì°¨í–‰ë ¬(Confusion Matrix)**

```python
def confusion_matrix_example():
    """ì˜¤ì°¨í–‰ë ¬ê³¼ ì„±ëŠ¥ì§€í‘œ ê³„ì‚° ì˜ˆì œ"""
    
    # ì˜ˆì œ: ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜ ê²°ê³¼
    # TP: ìŠ¤íŒ¸ì„ ìŠ¤íŒ¸ìœ¼ë¡œ ì •í™•íˆ ë¶„ë¥˜
    # TN: ì •ìƒì„ ì •ìƒìœ¼ë¡œ ì •í™•íˆ ë¶„ë¥˜  
    # FP: ì •ìƒì„ ìŠ¤íŒ¸ìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜
    # FN: ìŠ¤íŒ¸ì„ ì •ìƒìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜
    
    TP = 85  # ì§„ì–‘ì„±
    TN = 90  # ì§„ìŒì„±
    FP = 5   # ìœ„ì–‘ì„± (1ì¢… ì˜¤ë¥˜)
    FN = 10  # ìœ„ìŒì„± (2ì¢… ì˜¤ë¥˜)
    
    # ì„±ëŠ¥ì§€í‘œ ê³„ì‚°
    precision = TP / (TP + FP)      # ì •ë°€ë„
    recall = TP / (TP + FN)         # ë¯¼ê°ë„(ì¬í˜„ìœ¨)
    specificity = TN / (TN + FP)    # íŠ¹ì´ë„
    accuracy = (TP + TN) / (TP + TN + FP + FN)  # ì •í™•ë„
    f1_score = 2 * (precision * recall) / (precision + recall)  # F1 ì ìˆ˜
    
    print("ğŸ“Š ë¶„ë¥˜ì„±ëŠ¥í‰ê°€ ê²°ê³¼")
    print("=" * 30)
    print(f"ì •ë°€ë„(Precision): {precision:.3f}")
    print(f"ì¬í˜„ìœ¨(Recall/ë¯¼ê°ë„): {recall:.3f}")
    print(f"íŠ¹ì´ë„(Specificity): {specificity:.3f}")
    print(f"ì •í™•ë„(Accuracy): {accuracy:.3f}")
    print(f"F1 ì ìˆ˜: {f1_score:.3f}")
    
    # ì˜¤ì°¨í–‰ë ¬ ì‹œê°í™”
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    confusion_matrix = np.array([[TN, FP], [FN, TP]])
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues',
                xticklabels=['ì˜ˆì¸¡ ìŒì„±', 'ì˜ˆì¸¡ ì–‘ì„±'],
                yticklabels=['ì‹¤ì œ ìŒì„±', 'ì‹¤ì œ ì–‘ì„±'])
    plt.title('ì˜¤ì°¨í–‰ë ¬ (Confusion Matrix)')
    plt.ylabel('ì‹¤ì œê°’')
    plt.xlabel('ì˜ˆì¸¡ê°’')
    plt.show()
    
    return precision, recall, accuracy, f1_score

confusion_matrix_example()
```

### ğŸ“ˆ **ROC ê³¡ì„ ê³¼ AUC**

```python
def roc_curve_explanation():
    """ROC ê³¡ì„ ì˜ ì˜ë¯¸ì™€ í•´ì„"""
    
    roc_interpretation = {
        "AUC 0.9-1.0": "ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥",
        "AUC 0.8-0.9": "ì¢‹ì€ ì„±ëŠ¥", 
        "AUC 0.7-0.8": "ë³´í†µ ì„±ëŠ¥",
        "AUC 0.6-0.7": "ë‚˜ìœ ì„±ëŠ¥",
        "AUC 0.5": "ë¬´ì‘ìœ„ ë¶„ë¥˜ ìˆ˜ì¤€"
    }
    
    print("ğŸ“Š ROC-AUC ì„±ëŠ¥ í•´ì„ ê¸°ì¤€")
    print("=" * 35)
    
    for auc_range, performance in roc_interpretation.items():
        print(f"{auc_range}: {performance}")
    
    print("\nğŸ’¡ ROC ê³¡ì„  í•µì‹¬ í¬ì¸íŠ¸:")
    print("   â€¢ Xì¶•: ìœ„ì–‘ì„±ë¥  (1-íŠ¹ì´ë„)")
    print("   â€¢ Yì¶•: ì§„ì–‘ì„±ë¥  (ë¯¼ê°ë„)")
    print("   â€¢ ì™¼ìª½ ìœ„ë¡œ ê°ˆìˆ˜ë¡ ì¢‹ì€ ì„±ëŠ¥")
    print("   â€¢ ëŒ€ê°ì„ ì€ ë¬´ì‘ìœ„ ë¶„ë¥˜ê¸°")

roc_curve_explanation()
```

---

## ğŸ”§ **IV. ë°ì´í„° ë¶„ì„ ë„êµ¬ ì†Œê°œ: Python**

### ğŸ“š **í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœê³„**

```python
def python_datascience_ecosystem():
    """ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ë¥¼ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬"""
    
    libraries = {
        "ê¸°ë³¸ ë„êµ¬": {
            "NumPy": "ìˆ˜ì¹˜ ê³„ì‚°ì˜ ê¸°ë°˜",
            "Pandas": "ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„",
            "Matplotlib": "ê¸°ë³¸ ì‹œê°í™”",
            "Seaborn": "í†µê³„ ì‹œê°í™”"
        },
        
        "í†µê³„ ë¶„ì„": {
            "SciPy": "ê³¼í•™ ê³„ì‚° ë° í†µê³„",
            "Statsmodels": "í†µê³„ ëª¨ë¸ë§",
            "Pingouin": "ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ í†µê³„"
        },
        
        "ë¨¸ì‹ ëŸ¬ë‹": {
            "Scikit-learn": "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜",
            "XGBoost": "ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜",
            "LightGBM": "ê²½ëŸ‰ ë¶€ìŠ¤íŒ…"
        },
        
        "ë”¥ëŸ¬ë‹": {
            "TensorFlow": "êµ¬ê¸€ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬",
            "PyTorch": "í˜ì´ìŠ¤ë¶ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬",
            "Keras": "ê³ ìˆ˜ì¤€ ë”¥ëŸ¬ë‹ API"
        }
    }
    
    print("ğŸ íŒŒì´ì¬ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ìƒíƒœê³„")
    print("=" * 45)
    
    for category, libs in libraries.items():
        print(f"\nğŸ“¦ {category}:")
        for lib, description in libs.items():
            print(f"   â€¢ {lib}: {description}")

python_datascience_ecosystem()
```

---

## ğŸ“ˆ **V. í†µê³„ ë¶„ì„ ê¸°ë²•**

### ğŸ”— **ì—°ê´€ê·œì¹™ë¶„ì„ (Association Rule Analysis)**

```python
def association_rules_intro():
    """ì—°ê´€ê·œì¹™ë¶„ì„ ê°œë…ê³¼ ì§€í‘œ"""
    
    print("ğŸ›’ ì—°ê´€ê·œì¹™ë¶„ì„ - ì¥ë°”êµ¬ë‹ˆ ë¶„ì„")
    print("=" * 35)
    
    concepts = {
        "ì§€ì§€ë„(Support)": {
            "ì •ì˜": "ì „ì²´ ê±°ë˜ ì¤‘ íŠ¹ì • ìƒí’ˆì¡°í•©ì´ ë‚˜íƒ€ë‚˜ëŠ” ë¹„ìœ¨",
            "ê³µì‹": "P(A âˆ© B)",
            "ì˜ˆì‹œ": "ë¹µê³¼ ìš°ìœ ë¥¼ í•¨ê»˜ ì‚° ê±°ë˜ / ì „ì²´ ê±°ë˜"
        },
        
        "ì‹ ë¢°ë„(Confidence)": {
            "ì •ì˜": "Aë¥¼ ì‚° ì‚¬ëŒ ì¤‘ Bë„ ì‚° ì‚¬ëŒì˜ ë¹„ìœ¨",
            "ê³µì‹": "P(B|A) = P(A âˆ© B) / P(A)",
            "ì˜ˆì‹œ": "ë¹µì„ ì‚° ì‚¬ëŒ ì¤‘ ìš°ìœ ë„ ì‚° ë¹„ìœ¨"
        },
        
        "í–¥ìƒë„(Lift)": {
            "ì •ì˜": "Aì™€ Bì˜ ì—°ê´€ì„± ê°•ë„",
            "ê³µì‹": "P(B|A) / P(B)",
            "í•´ì„": "1ë³´ë‹¤ í¬ë©´ ì–‘ì˜ ìƒê´€ê´€ê³„"
        }
    }
    
    for metric, details in concepts.items():
        print(f"\nğŸ“Š {metric}")
        for key, value in details.items():
            print(f"   {key}: {value}")

association_rules_intro()
```

### ğŸ“Š **êµì°¨ë¶„ì„ (Cross-tabulation Analysis)**

```python
def crosstab_analysis_example():
    """êµì°¨ë¶„ì„ ì˜ˆì œì™€ ì¹´ì´ì œê³± ê²€ì •"""
    
    # ì˜ˆì œ ë°ì´í„°: ì„±ë³„ê³¼ ì œí’ˆ ì„ í˜¸ë„
    data = {
        'ì„±ë³„': ['ë‚¨', 'ë‚¨', 'ì—¬', 'ì—¬', 'ë‚¨', 'ì—¬', 'ë‚¨', 'ì—¬'] * 25,
        'ì„ í˜¸ë„': ['A', 'B', 'A', 'A', 'B', 'A', 'A', 'B'] * 25
    }
    
    df = pd.DataFrame(data)
    
    # êµì°¨í‘œ ìƒì„±
    crosstab = pd.crosstab(df['ì„±ë³„'], df['ì„ í˜¸ë„'], margins=True)
    
    print("ğŸ“Š êµì°¨ë¶„ì„ ì˜ˆì œ")
    print("=" * 25)
    print("êµì°¨í‘œ:")
    print(crosstab)
    
    # ì¹´ì´ì œê³± ê²€ì •
    from scipy.stats import chi2_contingency
    
    chi2, p_value, dof, expected = chi2_contingency(crosstab.iloc[:-1, :-1])
    
    print(f"\nğŸ“ˆ ì¹´ì´ì œê³± ê²€ì • ê²°ê³¼:")
    print(f"ì¹´ì´ì œê³± í†µê³„ëŸ‰: {chi2:.4f}")
    print(f"P-value: {p_value:.4f}")
    print(f"ììœ ë„: {dof}")
    
    if p_value < 0.05:
        print("âœ… ìœ ì˜ìˆ˜ì¤€ 5%ì—ì„œ ì„±ë³„ê³¼ ì„ í˜¸ë„ëŠ” ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ìœ ì˜ìˆ˜ì¤€ 5%ì—ì„œ ì„±ë³„ê³¼ ì„ í˜¸ë„ëŠ” ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤.")

crosstab_analysis_example()
```

### ğŸ“Š **ë¶„ì‚°ë¶„ì„ (ANOVA)**

```python
def anova_example():
    """ì¼ì›ë¶„ì‚°ë¶„ì„ ì˜ˆì œ"""
    
    from scipy import stats
    
    # ì˜ˆì œ: 3ê°œ ë°˜ì˜ ì‹œí—˜ ì ìˆ˜
    class_a = [85, 87, 83, 89, 86, 84, 88, 85]
    class_b = [78, 82, 80, 79, 81, 83, 77, 80]
    class_c = [92, 94, 91, 93, 95, 90, 89, 92]
    
    # ì¼ì›ë¶„ì‚°ë¶„ì„
    f_stat, p_value = stats.f_oneway(class_a, class_b, class_c)
    
    print("ğŸ“Š ì¼ì›ë¶„ì‚°ë¶„ì„ (One-way ANOVA)")
    print("=" * 40)
    print(f"Aë°˜ í‰ê· : {np.mean(class_a):.2f}")
    print(f"Bë°˜ í‰ê· : {np.mean(class_b):.2f}")
    print(f"Cë°˜ í‰ê· : {np.mean(class_c):.2f}")
    print(f"\nF-í†µê³„ëŸ‰: {f_stat:.4f}")
    print(f"P-value: {p_value:.4f}")
    
    if p_value < 0.05:
        print("âœ… ì„¸ ë°˜ ê°„ì— ìœ ì˜í•œ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ì„¸ ë°˜ ê°„ì— ìœ ì˜í•œ ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤.")

anova_example()
```

---

## ğŸ¤– **VI. AIë¶„ì„ëª¨í˜•**

### ğŸ” **ìµœê·¼ì ‘ ì´ì›ƒ (K-Nearest Neighbors)**

```python
def knn_explanation():
    """KNN ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…"""
    
    print("ğŸ¯ K-ìµœê·¼ì ‘ ì´ì›ƒ (KNN) ì•Œê³ ë¦¬ì¦˜")
    print("=" * 35)
    
    knn_concepts = {
        "í•µì‹¬ ì•„ì´ë””ì–´": "ë¹„ìŠ·í•œ ê²ƒë¼ë¦¬ ëª¨ì—¬ìˆë‹¤ëŠ” ê°€ì •",
        "ì‘ë™ ì›ë¦¬": "ìƒˆë¡œìš´ ë°ì´í„°ì™€ ê°€ì¥ ê°€ê¹Œìš´ Kê°œ ì´ì›ƒì˜ ë‹¤ìˆ˜ê²°",
        "ê±°ë¦¬ ì¸¡ì •": "ìœ í´ë¦¬ë“œ, ë§¨í•˜íƒ„, ì½”ì‚¬ì¸ ê±°ë¦¬ ë“±",
        "ì¥ì ": "ì§ê´€ì ì´ê³  êµ¬í˜„ì´ ê°„ë‹¨",
        "ë‹¨ì ": "ê³„ì‚° ë¹„ìš©ì´ ë†’ê³  ì°¨ì›ì˜ ì €ì£¼ì— ë¯¼ê°"
    }
    
    for concept, description in knn_concepts.items():
        print(f"ğŸ“Š {concept}: {description}")
    
    print(f"\nğŸ’¡ Kê°’ ì„ íƒ ê°€ì´ë“œ:")
    print(f"   â€¢ Kê°€ ì‘ìœ¼ë©´: ë…¸ì´ì¦ˆì— ë¯¼ê°, ë³µì¡í•œ ê²°ì •ê²½ê³„")
    print(f"   â€¢ Kê°€ í¬ë©´: ë¶€ë“œëŸ¬ìš´ ê²°ì •ê²½ê³„, ê³¼ì†Œì í•© ìœ„í—˜")
    print(f"   â€¢ ì¼ë°˜ì ìœ¼ë¡œ í™€ìˆ˜ ì„ íƒ (ë™ì  ë°©ì§€)")

knn_explanation()
```

### ğŸŒ³ **ì˜ì‚¬ê²°ì •ë‚˜ë¬´ (Decision Tree)**

```python
def decision_tree_explanation():
    """ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…"""
    
    print("ğŸŒ³ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ (Decision Tree)")
    print("=" * 35)
    
    dt_concepts = {
        "í•µì‹¬ ì•„ì´ë””ì–´": "if-then ê·œì¹™ì˜ ê³„ì¸µì  êµ¬ì¡°",
        "ë¶„í•  ê¸°ì¤€": "ì •ë³´ ì´ë“, ì§€ë‹ˆ ë¶ˆìˆœë„, ì—”íŠ¸ë¡œí”¼",
        "ì¥ì ": "í•´ì„ì´ ì‰½ê³  ë¹„ì„ í˜• ê´€ê³„ í•™ìŠµ ê°€ëŠ¥",
        "ë‹¨ì ": "ê³¼ì í•© ê²½í–¥, ë¶ˆì•ˆì •ì„±"
    }
    
    for concept, description in dt_concepts.items():
        print(f"ğŸ“Š {concept}: {description}")
    
    print(f"\nğŸ”§ ê³¼ì í•© ë°©ì§€ ë°©ë²•:")
    print(f"   â€¢ ê°€ì§€ì¹˜ê¸° (Pruning)")
    print(f"   â€¢ ìµœëŒ€ ê¹Šì´ ì œí•œ")
    print(f"   â€¢ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ ì„¤ì •")

decision_tree_explanation()
```

---

## ğŸ§  **VII. ë”¥ëŸ¬ë‹**

### ğŸ”— **ì¸ê³µì‹ ê²½ë§ (Artificial Neural Network)**

```python
def ann_basics():
    """ì¸ê³µì‹ ê²½ë§ ê¸°ì´ˆ ê°œë…"""
    
    print("ğŸ§  ì¸ê³µì‹ ê²½ë§ (ANN) ê¸°ì´ˆ")
    print("=" * 30)
    
    ann_concepts = {
        "ë‰´ëŸ° ëª¨ë¸": "ì…ë ¥ â†’ ê°€ì¤‘í•© â†’ í™œì„±í™”í•¨ìˆ˜ â†’ ì¶œë ¥",
        "í•™ìŠµ ê³¼ì •": "ìˆœì „íŒŒ â†’ ì˜¤ì°¨ê³„ì‚° â†’ ì—­ì „íŒŒ â†’ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸",
        "í™œì„±í™”í•¨ìˆ˜": "ReLU, Sigmoid, Tanh ë“±",
        "ì†ì‹¤í•¨ìˆ˜": "MSE(íšŒê·€), Cross-entropy(ë¶„ë¥˜)"
    }
    
    for concept, description in ann_concepts.items():
        print(f"ğŸ“Š {concept}: {description}")
    
    print(f"\nğŸ”§ ì‹ ê²½ë§ êµ¬ì¡°:")
    print(f"   â€¢ ì…ë ¥ì¸µ: ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ëŠ” ì¸µ")
    print(f"   â€¢ ì€ë‹‰ì¸µ: íŠ¹ì§•ì„ í•™ìŠµí•˜ëŠ” ì¸µ")
    print(f"   â€¢ ì¶œë ¥ì¸µ: ìµœì¢… ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ì¸µ")

ann_basics()
```

### ğŸï¸ **í•©ì„±ê³±ì‹ ê²½ë§ (CNN)**

```python
def cnn_explanation():
    """CNN í•µì‹¬ ê°œë…"""
    
    print("ğŸï¸ í•©ì„±ê³±ì‹ ê²½ë§ (CNN)")
    print("=" * 25)
    
    cnn_concepts = {
        "ì£¼ìš” ìš©ë„": "ì´ë¯¸ì§€ ì¸ì‹, ì»´í“¨í„° ë¹„ì „",
        "í•µì‹¬ ì—°ì‚°": "í•©ì„±ê³±(Convolution), í’€ë§(Pooling)",
        "íŠ¹ì§•": "ì§€ì—­ì  íŒ¨í„´ í•™ìŠµ, ë§¤ê°œë³€ìˆ˜ ê³µìœ ",
        "ëŒ€í‘œ ëª¨ë¸": "LeNet, AlexNet, VGG, ResNet"
    }
    
    for concept, description in cnn_concepts.items():
        print(f"ğŸ“Š {concept}: {description}")
    
    print(f"\nğŸ”§ CNN êµ¬ì¡°:")
    print(f"   â€¢ í•©ì„±ê³±ì¸µ: íŠ¹ì§• ì¶”ì¶œ")
    print(f"   â€¢ í’€ë§ì¸µ: ì°¨ì› ì¶•ì†Œ")
    print(f"   â€¢ ì™„ì „ì—°ê²°ì¸µ: ìµœì¢… ë¶„ë¥˜")

cnn_explanation()
```

---

## ğŸ“š **í•™ìŠµì— ë„ì›€ì´ ë˜ëŠ” ì‚¬ì´íŠ¸ ëª¨ìŒ**

### ğŸŒ **ì˜¨ë¼ì¸ í•™ìŠµ í”Œë«í¼**

```python
def learning_resources():
    """í†µê³„ì™€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ í•™ìŠµ ìë£Œ"""
    
    resources = {
        "ğŸ“Š í†µê³„ ê¸°ì´ˆ": {
            "Khan Academy": "https://ko.khanacademy.org/math/statistics-probability",
            "KMOOC": "í•œêµ­í˜• ì˜¨ë¼ì¸ ê³µê°œê°•ì¢Œ",
            "Coursera Statistics": "ë“€í¬ëŒ€í•™êµ í†µê³„ ê³¼ì •"
        },
        
        "ğŸ íŒŒì´ì¬ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤": {
            "Kaggle Learn": "https://www.kaggle.com/learn",
            "DataCamp": "ì¸í„°ë™í‹°ë¸Œ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ í•™ìŠµ",
            "Jupyter Notebook": "ì‹¤ìŠµ í™˜ê²½"
        },
        
        "ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹": {
            "Fast.ai": "https://www.fast.ai/",
            "Andrew Ng Coursera": "ë¨¸ì‹ ëŸ¬ë‹ ì…ë¬¸ ê°•ì˜",
            "Papers With Code": "ìµœì‹  ë…¼ë¬¸ê³¼ ì½”ë“œ"
        },
        
        "ğŸ“– ì±…ê³¼ ë¬¸ì„œ": {
            "Think Stats": "ë¬´ë£Œ í†µê³„í•™ êµì¬",
            "Scikit-learn Documentation": "ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê³µì‹ ë¬¸ì„œ",
            "Pandas Documentation": "ë°ì´í„° ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê³µì‹ ë¬¸ì„œ"
        }
    }
    
    print("ğŸ“š í†µê³„Â·ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ í•™ìŠµ ìë£Œ")
    print("=" * 45)
    
    for category, sites in resources.items():
        print(f"\n{category}:")
        for site, description in sites.items():
            print(f"   â€¢ {site}: {description}")

learning_resources()
```

---

## ğŸ¯ **í•™ìŠµ ë¡œë“œë§µê³¼ ë‹¤ìŒ ë‹¨ê³„**

### ğŸ“ˆ **ë‹¨ê³„ë³„ í•™ìŠµ ê³„íš**

```python
def learning_roadmap():
    """í†µê³„ì—ì„œ AIê¹Œì§€ í•™ìŠµ ë¡œë“œë§µ"""
    
    roadmap = {
        "1ë‹¨ê³„ (ê¸°ì´ˆ í†µê³„)": {
            "ê¸°ê°„": "2-4ì£¼",
            "ë‚´ìš©": ["ê¸°ìˆ í†µê³„", "ì¶”ë¡ í†µê³„", "ê°€ì„¤ê²€ì •"],
            "ëª©í‘œ": "í†µê³„ì  ì‚¬ê³  ê¸°ë°˜ êµ¬ì¶•"
        },
        
        "2ë‹¨ê³„ (ë°ì´í„° ë¶„ì„)": {
            "ê¸°ê°„": "4-6ì£¼", 
            "ë‚´ìš©": ["Python/Pandas", "ë°ì´í„° ì „ì²˜ë¦¬", "EDA"],
            "ëª©í‘œ": "ì‹¤ë¬´ ë°ì´í„° ë¶„ì„ ëŠ¥ë ¥"
        },
        
        "3ë‹¨ê³„ (í†µê³„ ëª¨ë¸ë§)": {
            "ê¸°ê°„": "6-8ì£¼",
            "ë‚´ìš©": ["íšŒê·€ë¶„ì„", "ë¶„ì‚°ë¶„ì„", "ì‹œê³„ì—´ë¶„ì„"],
            "ëª©í‘œ": "í†µê³„ ëª¨ë¸ë§ ë§ˆìŠ¤í„°"
        },
        
        "4ë‹¨ê³„ (ë¨¸ì‹ ëŸ¬ë‹)": {
            "ê¸°ê°„": "8-12ì£¼",
            "ë‚´ìš©": ["ì§€ë„í•™ìŠµ", "ë¹„ì§€ë„í•™ìŠµ", "ëª¨ë¸ í‰ê°€"],
            "ëª©í‘œ": "ë¨¸ì‹ ëŸ¬ë‹ ì‹¤ë¬´ ì ìš©"
        },
        
        "5ë‹¨ê³„ (ë”¥ëŸ¬ë‹)": {
            "ê¸°ê°„": "12-16ì£¼",
            "ë‚´ìš©": ["ì‹ ê²½ë§", "CNN", "RNN", "Transformer"],
            "ëª©í‘œ": "ë”¥ëŸ¬ë‹ ì „ë¬¸ê°€"
        }
    }
    
    print("ğŸ—ºï¸ í†µê³„ â†’ AI í•™ìŠµ ë¡œë“œë§µ")
    print("=" * 35)
    
    for stage, details in roadmap.items():
        print(f"\nğŸ“ˆ {stage}")
        print(f"   ê¸°ê°„: {details['ê¸°ê°„']}")
        print(f"   ë‚´ìš©: {', '.join(details['ë‚´ìš©'])}")
        print(f"   ëª©í‘œ: {details['ëª©í‘œ']}")

learning_roadmap()
```

### ğŸ¯ **ì‹¤ë¬´ í”„ë¡œì íŠ¸ ì œì•ˆ**

```python
def project_suggestions():
    """ë‹¨ê³„ë³„ ì‹¤ë¬´ í”„ë¡œì íŠ¸"""
    
    projects = {
        "ì´ˆê¸‰ í”„ë¡œì íŠ¸": [
            "ê³µê³µë°ì´í„°ë¥¼ í™œìš©í•œ ê¸°ìˆ í†µê³„ ë¶„ì„",
            "ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„° êµì°¨ë¶„ì„",
            "ì£¼ì‹ ë°ì´í„° ì‹œê³„ì—´ ì‹œê°í™”"
        ],
        
        "ì¤‘ê¸‰ í”„ë¡œì íŠ¸": [
            "A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„ ë° ë¶„ì„",
            "ê³ ê° ì„¸ê·¸ë©˜í…Œì´ì…˜ ë¶„ì„",
            "íŒë§¤ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•"
        ],
        
        "ê³ ê¸‰ í”„ë¡œì íŠ¸": [
            "ì¶”ì²œ ì‹œìŠ¤í…œ ê°œë°œ",
            "ìì—°ì–´ ê°ì„± ë¶„ì„",
            "ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶•"
        ]
    }
    
    print("ğŸš€ ë‹¨ê³„ë³„ ì‹¤ë¬´ í”„ë¡œì íŠ¸")
    print("=" * 30)
    
    for level, project_list in projects.items():
        print(f"\nğŸ“Š {level}:")
        for i, project in enumerate(project_list, 1):
            print(f"   {i}. {project}")

project_suggestions()
```

---

## ğŸ’¡ **ë§ˆë¬´ë¦¬: í†µê³„í•™ì˜ ê°€ì¹˜ì™€ ë¯¸ë˜**

### ğŸ¯ **ì™œ í†µê³„ë¥¼ ë°°ì›Œì•¼ í• ê¹Œ?**

í†µê³„í•™ì€ ë‹¨ìˆœí•œ ìˆ«ì ë†€ìŒì´ ì•„ë‹™ë‹ˆë‹¤. **ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •**ì˜ í•µì‹¬ì´ë©°, **ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”**í•˜ì—¬ í•©ë¦¬ì  íŒë‹¨ì„ ë‚´ë¦¬ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

```python
def statistics_value():
    """í†µê³„í•™ì˜ ê°€ì¹˜ì™€ ë¯¸ë˜ ì „ë§"""
    
    value_props = {
        "ğŸ” ê³¼í•™ì  ì‚¬ê³ ": "ê°€ì„¤ ì„¤ì • â†’ ë°ì´í„° ìˆ˜ì§‘ â†’ ê²€ì¦ â†’ ê²°ë¡ ",
        "ğŸ“Š ì˜ì‚¬ê²°ì • ì§€ì›": "ê°ì´ ì•„ë‹Œ ë°ì´í„° ê¸°ë°˜ íŒë‹¨",
        "ğŸ¯ ë¦¬ìŠ¤í¬ ê´€ë¦¬": "ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ë° ëŒ€ì‘",
        "ğŸš€ í˜ì‹  ë„êµ¬": "AI/MLì˜ ìˆ˜í•™ì  ê¸°ì´ˆ",
        "ğŸ’¼ ê²½ìŸë ¥": "ëª¨ë“  ì‚°ì—…ì—ì„œ ìš”êµ¬ë˜ëŠ” í•µì‹¬ ì—­ëŸ‰"
    }
    
    print("ğŸ’ í†µê³„í•™ì´ ì£¼ëŠ” ê°€ì¹˜")
    print("=" * 25)
    
    for value, description in value_props.items():
        print(f"{value}: {description}")
    
    print(f"\nğŸ”® ë¯¸ë˜ ì „ë§:")
    print(f"   â€¢ ë¹…ë°ì´í„° ì‹œëŒ€ì˜ í•µì‹¬ ì—­ëŸ‰")
    print(f"   â€¢ AI ìœ¤ë¦¬ì™€ í•´ì„ ê°€ëŠ¥ì„±")
    print(f"   â€¢ ìë™í™”ëœ ì˜ì‚¬ê²°ì • ì‹œìŠ¤í…œ")
    print(f"   â€¢ ê°œì¸í™”ëœ ì„œë¹„ìŠ¤ ì„¤ê³„")

statistics_value()
```

---

## ğŸ **ë‹¤ìŒ í•™ìŠµ ë‹¨ê³„**

ì´ì œ **22.01 ê¸°ìˆ í†µê³„**ì™€ **22.02 ì¶”ë¡ í†µê³„**ë¡œ ë„˜ì–´ê°€ì„œ ë” ê¹Šì´ ìˆëŠ” í†µê³„ í•™ìŠµì„ ì‹œì‘í•´ë³´ì„¸ìš”!

```python
def next_steps():
    """ë‹¤ìŒ í•™ìŠµ ë‹¨ê³„ ì•ˆë‚´"""
    
    next_learning = [
        "ğŸ“Š 22.01 ê¸°ìˆ í†µê³„ - ë°ì´í„° ìš”ì•½ê³¼ ì‹œê°í™”",
        "ğŸ” 22.02 ì¶”ë¡ í†µê³„ - ê°€ì„¤ê²€ì •ê³¼ ì‹ ë¢°êµ¬ê°„", 
        "ğŸ“ˆ í†µê³„ ë¶„ì„ ê¸°ë²• - ì‹¤ë¬´ ì ìš©",
        "ğŸ¤– AIë¶„ì„ëª¨í˜• - ë¨¸ì‹ ëŸ¬ë‹ ì…ë¬¸",
        "ğŸ§  ë”¥ëŸ¬ë‹ - ì‹ ê²½ë§ê³¼ ìµœì‹  ê¸°ë²•"
    ]
    
    print("ğŸ¯ ë‹¤ìŒ í•™ìŠµ ë‹¨ê³„")
    print("=" * 20)
    
    for i, step in enumerate(next_learning, 1):
        print(f"{i}. {step}")
    
    print(f"\nğŸ’ª í™”ì´íŒ…! í†µê³„ ë§ˆìŠ¤í„°ì˜ ì—¬ì •ì´ ì‹œì‘ë©ë‹ˆë‹¤! ğŸš€")

next_steps()
```

**í†µê³„í•™ì€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ì˜ ì–¸ì–´ì…ë‹ˆë‹¤. ì´ ì–¸ì–´ë¥¼ ë§ˆìŠ¤í„°í•˜ë©´ ë°ì´í„°ê°€ ë§í•˜ëŠ” ì´ì•¼ê¸°ë¥¼ ë“£ê³ , ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ë©°, í˜„ëª…í•œ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤!** ğŸ“âœ¨