# 15. ì›¹ í¬ë¡¤ë§ì´ë€ ë¬´ì—‡ì¼ê¹Œ? ğŸ•¸ï¸

> **pandas ë§ˆìŠ¤í„°ì—ì„œ ì›¹ ë°ì´í„° ìˆ˜ì§‘ ì „ë¬¸ê°€ë¡œ!**  
> ì›¹ì˜ ë™ì‘ ì›ë¦¬ë¶€í„° ê³ ê¸‰ í¬ë¡¤ë§ ë„êµ¬ê¹Œì§€, ì²´ê³„ì ì¸ í•™ìŠµ ê°€ì´ë“œ

---

## ğŸ¯ **ì´ ì±•í„°ì—ì„œ ë°°ìš°ëŠ” ê²ƒ**

```mermaid
graph LR
    A[ğŸŒ ì›¹ ê¸°ì´ˆ ê°œë…] --> B[ğŸ”§ HTMLê³¼ íŒŒì‹±]
    B --> C[ğŸ² BeautifulSoup ì›ë¦¬]
    C --> D[ğŸ“Š pandas ì—°ë™]
    D --> E[ğŸš€ ê³ ê¸‰ ë„êµ¬ ì†Œê°œ]
    
    style A fill:#ff9999
    style B fill:#ffcc99
    style C fill:#99ccff
    style D fill:#99ff99
    style E fill:#cc99ff
```

### ğŸ“‹ **í•™ìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸**
- [ ] ì›¹ì˜ ë™ì‘ ì›ë¦¬ì™€ HTTP í†µì‹  ì´í•´
- [ ] HTML êµ¬ì¡°ì™€ CSS ì„ íƒì ë§ˆìŠ¤í„°
- [ ] BeautifulSoup íŒŒì‹± ì›ë¦¬ ì™„ë²½ ì´í•´
- [ ] pandasì™€ í¬ë¡¤ë§ ë°ì´í„° ì—°ë™
- [ ] Selenium, Scrapy ë“± ê³ ê¸‰ ë„êµ¬ ê°œë… ì´í•´

---

## ğŸ“ **1ë¶€: ì›¹ì˜ ì„¸ê³„ ì´í•´í•˜ê¸°** ğŸŒ

### ğŸ¤” **ì›¹ í¬ë¡¤ë§ì´ ë­ê¸¸ë˜?**

ì¼ë‹¨ ê°„ë‹¨í•œ ì˜ˆì‹œë¶€í„° ë³´ê² ìŠµë‹ˆë‹¤:

```python
# ğŸ¯ ì´ëŸ° ê²ƒì´ ì›¹ í¬ë¡¤ë§ì…ë‹ˆë‹¤!
import requests
from bs4 import BeautifulSoup
import pandas as pd

# ì›¹í˜ì´ì§€ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
url = "https://quotes.toscrape.com/"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# ëª…ì–¸ê³¼ ì €ì ì¶”ì¶œ
quotes = soup.select("div.quote")
data = []

for quote in quotes:
    text = quote.select_one("span.text").get_text(strip=True)
    author = quote.select_one("small.author").get_text(strip=True)
    data.append({"ëª…ì–¸": text, "ì €ì": author})

# pandas DataFrameìœ¼ë¡œ ë³€í™˜ (14ì±•í„°ì—ì„œ ë°°ìš´ ë‚´ìš©!)
df = pd.DataFrame(data)
print(f"ìˆ˜ì§‘í•œ ëª…ì–¸ ê°œìˆ˜: {len(df)}ê°œ")
print(df.head(3))
```

**ğŸ’¡ ê²°ê³¼ ì˜ˆì‹œ:**
```
ìˆ˜ì§‘í•œ ëª…ì–¸ ê°œìˆ˜: 10ê°œ
                                               ëª…ì–¸          ì €ì
0  "The world as we have created it is a proces...  Albert Einstein
1  "It is our choices, Harry, that show what we ...    J.K. Rowling  
2  "There are only two ways to live your life. ...  Albert Einstein
```

### ğŸŒ **ì›¹ì€ ì–´ë–»ê²Œ ë™ì‘í• ê¹Œ?**

#### **1) ì›¹ì˜ ê¸°ë³¸ êµ¬ì¡°**
```
ğŸ‘¤ ì‚¬ìš©ì (Client) â†â†’ ğŸŒ ì›¹ì„œë²„ (Server)

1. ì‚¬ìš©ìê°€ ì›¹ë¸Œë¼ìš°ì €ì—ì„œ URL ì…ë ¥
2. ì›¹ë¸Œë¼ìš°ì €ê°€ ì„œë²„ì— HTTP ìš”ì²­ ì „ì†¡
3. ì„œë²„ê°€ HTML ë¬¸ì„œ ì‘ë‹µ
4. ì›¹ë¸Œë¼ìš°ì €ê°€ HTMLì„ í•´ì„í•˜ì—¬ í™”ë©´ í‘œì‹œ
```

#### **2) HTTP í†µì‹ ì˜ ê¸°ì´ˆ**
```python
import requests

# GET ìš”ì²­: ì„œë²„ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
response = requests.get("https://httpbin.org/get")
print(f"ìƒíƒœ ì½”ë“œ: {response.status_code}")  # 200 = ì„±ê³µ
print(f"ì‘ë‹µ ì‹œê°„: {response.elapsed.total_seconds()}ì´ˆ")

# POST ìš”ì²­: ì„œë²„ì— ë°ì´í„° ì „ì†¡ (ì˜ˆ: ë¡œê·¸ì¸)
# response = requests.post("https://httpbin.org/post", 
#                         data={"username": "admin"})
```

#### **3) ì •ì  vs ë™ì  ì›¹í˜ì´ì§€**

| êµ¬ë¶„ | ì •ì  í˜ì´ì§€ | ë™ì  í˜ì´ì§€ |
|------|-------------|-------------|
| **ìƒì„± ë°©ì‹** | ë¯¸ë¦¬ ë§Œë“¤ì–´ì§„ HTML | JavaScriptë¡œ ì‹¤ì‹œê°„ ìƒì„± |
| **í¬ë¡¤ë§ ë‚œì´ë„** | ì‰¬ì›€ (requests + BeautifulSoup) | ì–´ë ¤ì›€ (Selenium í•„ìš”) |
| **ì˜ˆì‹œ** | ë‰´ìŠ¤ ê¸°ì‚¬, ìœ„í‚¤ë°±ê³¼ | Facebook í”¼ë“œ, Gmail |
| **ë°ì´í„° ë¡œë”©** | í˜ì´ì§€ì™€ í•¨ê»˜ ë¡œë”© | ë‚˜ì¤‘ì— ë³„ë„ ë¡œë”© |

```python
# ğŸ” í˜ì´ì§€ ìœ í˜• íŒë³„í•˜ëŠ” ê°„ë‹¨í•œ ë°©ë²•
def check_page_type(url):
    response = requests.get(url)
    html_length = len(response.text)
    
    # JavaScript ì½”ë“œê°€ ë§ìœ¼ë©´ ë™ì  í˜ì´ì§€ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
    js_count = response.text.count('<script')
    
    if js_count > 10 or 'react' in response.text.lower():
        return "ë™ì  í˜ì´ì§€ (Selenium ê¶Œì¥)"
    else:
        return "ì •ì  í˜ì´ì§€ (BeautifulSoup ê°€ëŠ¥)"

# í…ŒìŠ¤íŠ¸
print(check_page_type("https://quotes.toscrape.com/"))
```

---

## ğŸ“ **2ë¶€: HTMLê³¼ íŒŒì‹±ì˜ ì›ë¦¬** ğŸ”§

### ğŸ—ï¸ **HTML êµ¬ì¡° ì´í•´í•˜ê¸°**

HTMLì€ **íŠ¸ë¦¬ êµ¬ì¡°**ë¡œ ë˜ì–´ìˆìŠµë‹ˆë‹¤. DOM(Document Object Model)ì´ë¼ê³  ë¶€ë¥´ì£ .

```html
<!DOCTYPE html>
<html>                    <!-- ë£¨íŠ¸ ìš”ì†Œ -->
  <head>                  <!-- ë©”íƒ€ë°ì´í„° -->
    <title>í˜ì´ì§€ ì œëª©</title>
  </head>
  <body>                  <!-- ì‹¤ì œ ì½˜í…ì¸  -->
    <div class="container">
      <h1 id="title">ë©”ì¸ ì œëª©</h1>
      <p class="content">ë‚´ìš© 1</p>
      <p class="content">ë‚´ìš© 2</p>
      <ul>
        <li>í•­ëª© 1</li>
        <li>í•­ëª© 2</li>
      </ul>
    </div>
  </body>
</html>
```

#### **ğŸ¯ CSS ì„ íƒì ì™„ì „ ì •ë³µ**

ì´ì œ ì´ HTMLì—ì„œ ì›í•˜ëŠ” ìš”ì†Œë¥¼ ì„ íƒí•´ë³´ê² ìŠµë‹ˆë‹¤:

```python
# ë‹¤ì–‘í•œ CSS ì„ íƒì ì‚¬ìš©ë²•
from bs4 import BeautifulSoup

html_sample = """
<div class="container">
    <h1 id="title">ë©”ì¸ ì œëª©</h1>
    <p class="content highlight">ì¤‘ìš”í•œ ë‚´ìš©</p>
    <p class="content">ì¼ë°˜ ë‚´ìš©</p>
    <ul>
        <li data-value="1">ì²« ë²ˆì§¸</li>
        <li data-value="2">ë‘ ë²ˆì§¸</li>
    </ul>
</div>
"""

soup = BeautifulSoup(html_sample, 'html.parser')

# ğŸ¯ ì„ íƒìë³„ ì‚¬ìš©ë²• ë¹„êµ
selectors = {
    # 1. íƒœê·¸ ì„ íƒì
    "h1": "íƒœê·¸ëª…ìœ¼ë¡œ ì„ íƒ",
    
    # 2. í´ë˜ìŠ¤ ì„ íƒì  
    ".content": "í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ì„ íƒ",
    ".content.highlight": "ì—¬ëŸ¬ í´ë˜ìŠ¤ ë™ì‹œ ë§Œì¡±",
    
    # 3. ID ì„ íƒì
    "#title": "IDë¡œ ì„ íƒ (ê³ ìœ ê°’)",
    
    # 4. ì†ì„± ì„ íƒì
    "[data-value='1']": "ì†ì„±ê°’ìœ¼ë¡œ ì„ íƒ",
    
    # 5. ê²°í•© ì„ íƒì
    "div.container p": "í›„ì† ì„ íƒì",
    "ul > li": "ì§ê³„ ìì‹ ì„ íƒì",
    
    # 6. ê°€ìƒ ì„ íƒì
    "li:first-child": "ì²« ë²ˆì§¸ ìì‹",
    "li:last-child": "ë§ˆì§€ë§‰ ìì‹",
    "li:nth-child(2)": "në²ˆì§¸ ìì‹"
}

# ì‹¤ì œ í…ŒìŠ¤íŠ¸
for selector, description in selectors.items():
    result = soup.select(selector)
    print(f"{selector:20} ({description}): {len(result)}ê°œ ë°œê²¬")
    if result:
        print(f"  â””â”€ í…ìŠ¤íŠ¸: '{result[0].get_text(strip=True)}'")
```

#### **ğŸ” find vs select ë¹„êµ**

```python
# ë‘ ê°€ì§€ ë°©ì‹ì˜ ì°¨ì´ì  ì´í•´í•˜ê¸°
print("=" * 50)
print("find ë°©ì‹ vs select ë°©ì‹ ë¹„êµ")
print("=" * 50)

# ë°©ì‹ 1: find ê³„ì—´ (BeautifulSoup ì „í†µ ë°©ì‹)
h1_find = soup.find('h1', id='title')
content_find = soup.find_all('p', class_='content')

print("ğŸ” find ë°©ì‹:")
print(f"  h1 íƒœê·¸: {h1_find.text if h1_find else 'None'}")
print(f"  p íƒœê·¸ë“¤: {len(content_find)}ê°œ")

# ë°©ì‹ 2: select ê³„ì—´ (CSS ì„ íƒì ë°©ì‹) 
h1_select = soup.select_one('h1#title')
content_select = soup.select('p.content')

print("\nğŸ¯ select ë°©ì‹:")
print(f"  h1 íƒœê·¸: {h1_select.text if h1_select else 'None'}")
print(f"  p íƒœê·¸ë“¤: {len(content_select)}ê°œ")

# ğŸ’¡ ì–¸ì œ ë¬´ì—‡ì„ ì“¸ê¹Œ?
print("\nğŸ’¡ ì‚¬ìš© ê°€ì´ë“œ:")
print("  find: ë‹¨ìˆœí•œ íƒœê·¸/ì†ì„± ê²€ìƒ‰ì— ì í•©")
print("  select: ë³µì¡í•œ CSS ì„ íƒì ì‚¬ìš©ì‹œ ì í•©")
print("  â†’ ì‹¤ë¬´ì—ì„œëŠ” select ë°©ì‹ì„ ë” ë§ì´ ì‚¬ìš©!")
```

---

## ğŸ“ **3ë¶€: BeautifulSoup íŒŒì‹± ì›ë¦¬** ğŸ²

### ğŸ§  **íŒŒì‹±ì´ ë„ëŒ€ì²´ ë­ì§€?**

**íŒŒì‹±(Parsing)**ì€ í…ìŠ¤íŠ¸ í˜•íƒœì˜ HTMLì„ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

```python
# íŒŒì‹± ê³¼ì • ì‹œê°í™”
raw_html = "<p>Hello <b>World</b>!</p>"
print("1ï¸âƒ£ ì›ë³¸ HTML (í…ìŠ¤íŠ¸):")
print(f"   '{raw_html}'")
print(f"   íƒ€ì…: {type(raw_html)}")

# íŒŒì‹± ì‹¤í–‰
soup = BeautifulSoup(raw_html, 'html.parser')
print("\n2ï¸âƒ£ íŒŒì‹± í›„ (ê°ì²´):")
print(f"   {soup}")
print(f"   íƒ€ì…: {type(soup)}")

# ì´ì œ êµ¬ì¡°ì  ì ‘ê·¼ ê°€ëŠ¥!
print("\n3ï¸âƒ£ êµ¬ì¡°ì  ì ‘ê·¼:")
print(f"   ì „ì²´ í…ìŠ¤íŠ¸: '{soup.get_text()}'")
print(f"   p íƒœê·¸: {soup.find('p')}")
print(f"   b íƒœê·¸ ë‚´ìš©: '{soup.find('b').text}'")
```

#### **ğŸ”§ íŒŒì„œë³„ íŠ¹ì§• ë¹„êµ**

íŒŒì´ì¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” HTML íŒŒì„œë“¤ì„ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤:

```python
import time
from bs4 import BeautifulSoup

# í…ŒìŠ¤íŠ¸ìš© HTML (ë³µì¡í•œ êµ¬ì¡°)
complex_html = """
<!DOCTYPE html>
<html>
<head><title>Test</title></head>
<body>
    <div class="content">
        <p>Paragraph 1</p>
        <p>Paragraph 2 with <a href="test.html">link</a></p>
        <ul>
            <li>Item 1</li>
            <li>Item 2</li>
        </ul>
    </div>
</body>
</html>
""" * 100  # 100ë°° ë°˜ë³µí•´ì„œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

print("ğŸ”§ íŒŒì„œë³„ íŠ¹ì§• ë¹„êµ")
print("=" * 60)

parsers = [
    ('html.parser', 'ê¸°ë³¸ ë‚´ì¥, ì¤‘ê°„ ì†ë„, ì•ˆì •ì '),
    ('lxml', 'ê°€ì¥ ë¹ ë¦„, ì™¸ë¶€ ì„¤ì¹˜ í•„ìš”'),
    ('html5lib', 'ê°€ì¥ ì •í™•, ê°€ì¥ ëŠë¦¼')
]

for parser_name, description in parsers:
    try:
        start_time = time.time()
        soup = BeautifulSoup(complex_html, parser_name)
        links = soup.find_all('a')
        end_time = time.time()
        
        print(f"{parser_name:12} | {description}")
        print(f"             â””â”€ ì²˜ë¦¬ì‹œê°„: {end_time - start_time:.4f}ì´ˆ")
        print(f"             â””â”€ ë§í¬ ê°œìˆ˜: {len(links)}ê°œ")
        print()
        
    except Exception as e:
        print(f"{parser_name:12} | ì˜¤ë¥˜: {e}")
        print()
```

#### **ğŸ’¡ íŒŒì„œ ì„ íƒ ê°€ì´ë“œ**

```python
def recommend_parser(html_size, accuracy_needed, speed_priority):
    """ìƒí™©ë³„ ìµœì  íŒŒì„œ ì¶”ì²œ"""
    
    if accuracy_needed == "highest":
        return "html5lib", "ë¸Œë¼ìš°ì € ìˆ˜ì¤€ì˜ ì •í™•ì„±"
    
    if speed_priority == "highest" and html_size == "large":
        return "lxml", "ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ì— ìµœì "
    
    if html_size == "small" or speed_priority == "medium":
        return "html.parser", "ì¼ë°˜ì ì¸ ìš©ë„ì— ì í•©"
    
    return "html.parser", "ë¬´ë‚œí•œ ì„ íƒ"

# ì‚¬ìš© ì˜ˆì‹œ
situations = [
    ("small", "medium", "medium", "ê°„ë‹¨í•œ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘"),
    ("large", "medium", "highest", "ëŒ€ìš©ëŸ‰ ì‡¼í•‘ëª° í¬ë¡¤ë§"),
    ("medium", "highest", "low", "ê¸ˆìœµ ë°ì´í„° ì •í™•í•œ íŒŒì‹±"),
]

print("ğŸ“Š ìƒí™©ë³„ íŒŒì„œ ì¶”ì²œ")
print("=" * 50)

for size, accuracy, speed, scenario in situations:
    parser, reason = recommend_parser(size, accuracy, speed)
    print(f"ğŸ“Œ {scenario}")
    print(f"   ì¶”ì²œ: {parser} ({reason})")
    print()
```

---

## ğŸ“ **4ë¶€: pandas ì—°ë™ì˜ ë§ˆë²•** ğŸ“Š

### ğŸ”— **í¬ë¡¤ë§ + pandas = ë°ì´í„° ë¶„ì„ì˜ ì‹œì‘**

ì´ì œ 14ì±•í„°ì—ì„œ ë°°ìš´ pandasì™€ í¬ë¡¤ë§ì„ ì—°ê²°í•´ë³´ê² ìŠµë‹ˆë‹¤!

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np

def crawl_to_dataframe_demo():
    """í¬ë¡¤ë§ â†’ pandas ì—°ë™ ì™„ë²½ ë°ëª¨"""
    
    print("ğŸ•¸ï¸ ì›¹ í¬ë¡¤ë§ â†’ ğŸ“Š pandas ì—°ë™ ë°ëª¨")
    print("=" * 50)
    
    # 1ë‹¨ê³„: ì›¹ ë°ì´í„° ìˆ˜ì§‘
    url = "https://quotes.toscrape.com/"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # 2ë‹¨ê³„: ë°ì´í„° ì¶”ì¶œ ë° ì •ì œ
    quotes_data = []
    for quote in soup.select("div.quote"):
        text = quote.select_one("span.text").get_text(strip=True)
        author = quote.select_one("small.author").get_text(strip=True)
        
        # íƒœê·¸ ì •ë³´ë„ ìˆ˜ì§‘
        tags = [tag.get_text(strip=True) 
                for tag in quote.select("div.tags a.tag")]
        
        quotes_data.append({
            "ëª…ì–¸": text,
            "ì €ì": author,
            "íƒœê·¸_ê°œìˆ˜": len(tags),
            "ê¸€ì_ìˆ˜": len(text),
            "íƒœê·¸_ëª©ë¡": ", ".join(tags)
        })
    
    # 3ë‹¨ê³„: DataFrame ìƒì„± (14ì±•í„° ë‚´ìš© í™œìš©!)
    df = pd.DataFrame(quotes_data)
    
    # 4ë‹¨ê³„: ë°ì´í„° ë¶„ì„ (pandasì˜ í˜!)
    print(f"ğŸ“ˆ ê¸°ë³¸ ì •ë³´:")
    print(f"   ìˆ˜ì§‘ëœ ëª…ì–¸: {len(df)}ê°œ")
    print(f"   í‰ê·  ê¸€ì ìˆ˜: {df['ê¸€ì_ìˆ˜'].mean():.1f}ì")
    print(f"   ê°€ì¥ ë§ì€ íƒœê·¸: {df['íƒœê·¸_ê°œìˆ˜'].max()}ê°œ")
    
    # 5ë‹¨ê³„: ê³ ê¸‰ ë¶„ì„
    print(f"\nğŸ“Š ì €ìë³„ í†µê³„:")
    author_stats = df.groupby('ì €ì').agg({
        'ëª…ì–¸': 'count',
        'ê¸€ì_ìˆ˜': 'mean',
        'íƒœê·¸_ê°œìˆ˜': 'sum'
    }).round(1)
    
    print(author_stats)
    
    # 6ë‹¨ê³„: ë°ì´í„° ì €ì¥ (CSV í˜•íƒœë¡œ)
    output_file = 'crawled_quotes.csv'
    df.to_csv(output_file, index=False, encoding='utf-8')
    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_file}")
    
    return df

# ì‹¤í–‰
df_quotes = crawl_to_dataframe_demo()
```

#### **ğŸ”„ ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸**

```python
def create_monitoring_system():
    """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì˜ˆì‹œ"""
    
    def collect_current_data():
        # ì‹¤ì œë¡œëŠ” ë‰´ìŠ¤, ì£¼ê°€, ë‚ ì”¨ ë“±ì˜ ì‹¤ì‹œê°„ ë°ì´í„°
        return pd.DataFrame({
            'ì‹œê°„': [pd.Timestamp.now()],
            'ì˜¨ë„': [np.random.randint(20, 30)],
            'ìŠµë„': [np.random.randint(40, 80)],
            'ìƒíƒœ': [np.random.choice(['ë§‘ìŒ', 'íë¦¼', 'ë¹„'])]
        })
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
    try:
        existing_df = pd.read_csv('monitoring_data.csv')
        existing_df['ì‹œê°„'] = pd.to_datetime(existing_df['ì‹œê°„'])
    except FileNotFoundError:
        existing_df = pd.DataFrame()
    
    # ìƒˆ ë°ì´í„° ì¶”ê°€
    new_data = collect_current_data()
    updated_df = pd.concat([existing_df, new_data], ignore_index=True)
    
    # ìµœê·¼ 100ê°œ ë°ì´í„°ë§Œ ìœ ì§€
    if len(updated_df) > 100:
        updated_df = updated_df.tail(100).reset_index(drop=True)
    
    # ì €ì¥
    updated_df.to_csv('monitoring_data.csv', index=False)
    
    print(f"ğŸ“¡ ë°ì´í„° ì—…ë°ì´íŠ¸: {len(updated_df)}ê°œ ë ˆì½”ë“œ")
    return updated_df

# ì‚¬ìš©ë²•
# monitoring_df = create_monitoring_system()
```

---

## ğŸ“ **5ë¶€: ê³ ê¸‰ ë„êµ¬ ì†Œê°œ** ğŸš€

ì´ì œ BeautifulSoupì„ ë„˜ì–´ì„œëŠ” ê³ ê¸‰ ë„êµ¬ë“¤ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤!

### ğŸ¤– **Selenium: ë¸Œë¼ìš°ì € ìë™í™”ì˜ ì™•**

Seleniumì€ ì‹¤ì œ ì›¹ë¸Œë¼ìš°ì €ë¥¼ ì œì–´í•´ì„œ JavaScriptê°€ ì‹¤í–‰ë˜ëŠ” ë™ì  ì›¹ì‚¬ì´íŠ¸ë„ í¬ë¡¤ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ğŸš¨ ì£¼ì˜: ì‹¤ì œ ì‹¤í–‰ì„ ìœ„í•´ì„œëŠ” selenium ì„¤ì¹˜ ë° chromedriver í•„ìš”
# pip install selenium webdriver-manager

"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

def selenium_crawling_example():
    # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì •
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    options.add_argument('--no-sandbox')
    
    # ë¸Œë¼ìš°ì € ì‹œì‘
    driver = webdriver.Chrome(
        service=webdriver.ChromeService(ChromeDriverManager().install()),
        options=options
    )
    
    try:
        # í˜ì´ì§€ ë¡œë“œ
        driver.get("https://example.com")
        
        # JavaScript ì‹¤í–‰ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
        wait = WebDriverWait(driver, 10)
        element = wait.until(
            EC.presence_of_element_located((By.CLASS_NAME, "content"))
        )
        
        # ë°ì´í„° ì¶”ì¶œ
        title = driver.title
        content = driver.find_element(By.CLASS_NAME, "content").text
        
        return {"title": title, "content": content}
        
    finally:
        driver.quit()  # ë¸Œë¼ìš°ì € ì¢…ë£Œ

# ì–¸ì œ Seleniumì„ ì‚¬ìš©í• ê¹Œ?
selenium_use_cases = {
    "Instagram í”¼ë“œ": "ë¬´í•œ ìŠ¤í¬ë¡¤ë¡œ ë°ì´í„° ë¡œë”©",
    "ì˜¨ë¼ì¸ ì‡¼í•‘ëª°": "JavaScriptë¡œ ê°€ê²© ì •ë³´ í‘œì‹œ", 
    "êµ¬ê¸€ ë§µ": "ì‚¬ìš©ì ìƒí˜¸ì‘ìš© í›„ ë°ì´í„° í‘œì‹œ",
    "Single Page App": "React, Vue ë“±ìœ¼ë¡œ ì œì‘ëœ ì‚¬ì´íŠ¸"
}

print("ğŸ¤– Selenium ì‚¬ìš© ì‚¬ë¡€:")
for use_case, description in selenium_use_cases.items():
    print(f"  â€¢ {use_case}: {description}")
"""
```

#### **ğŸ¤– vs ğŸ² ë¹„êµ: Selenium vs BeautifulSoup**

```python
# ë„êµ¬ë³„ íŠ¹ì§• ë¹„êµí‘œ
comparison_data = {
    "íŠ¹ì§•": ["ì²˜ë¦¬ ì†ë„", "ë©”ëª¨ë¦¬ ì‚¬ìš©", "JavaScript ì§€ì›", "ì„¤ì¹˜ ë³µì¡ë„", "ì•ˆì •ì„±"],
    "BeautifulSoup": ["â­â­â­â­â­", "â­â­â­â­â­", "âŒ", "â­â­â­â­â­", "â­â­â­â­"],
    "Selenium": ["â­â­", "â­â­", "â­â­â­â­â­", "â­â­â­", "â­â­â­"],
    "ì ìš© ì‚¬ì´íŠ¸": ["ì •ì  ì‚¬ì´íŠ¸", "ë™ì  ì‚¬ì´íŠ¸"]
}

comparison_df = pd.DataFrame(comparison_data)
print("ğŸ¥Š BeautifulSoup vs Selenium ë¹„êµ")
print("=" * 50)
print(comparison_df.to_string(index=False))

print("\nğŸ’¡ ì„ íƒ ê°€ì´ë“œ:")
print("  ğŸ“„ ì •ì  ì‚¬ì´íŠ¸ (ë‰´ìŠ¤, ìœ„í‚¤): BeautifulSoup")
print("  ğŸ”„ ë™ì  ì‚¬ì´íŠ¸ (SNS, SPA): Selenium")
print("  âš¡ ì†ë„ ìš°ì„ : BeautifulSoup")  
print("  ğŸ¯ ì •í™•ì„± ìš°ì„ : Selenium")
```

### ğŸ•·ï¸ **Scrapy: ëŒ€ê·œëª¨ í¬ë¡¤ë§ í”„ë ˆì„ì›Œí¬**

ScrapyëŠ” ê¸°ì—… ìˆ˜ì¤€ì˜ ëŒ€ê·œëª¨ í¬ë¡¤ë§ì„ ìœ„í•œ ì „ë¬¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

```python
"""
# ğŸš¨ Scrapy ì˜ˆì‹œ (ì‹¤ì œ ì‹¤í–‰ì„ ìœ„í•´ì„œëŠ” ë³„ë„ í”„ë¡œì íŠ¸ ìƒì„± í•„ìš”)
# pip install scrapy

import scrapy

class QuotesSpider(scrapy.Spider):
    name = 'quotes'
    start_urls = ['https://quotes.toscrape.com/']
    
    def parse(self, response):
        # ê° ëª…ì–¸ ì²˜ë¦¬
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }
        
        # ë‹¤ìŒ í˜ì´ì§€ ì²˜ë¦¬ (ìë™ í˜ì´ì§•!)
        next_page = response.css('li.next a::attr(href)').get()
        if next_page:
            yield response.follow(next_page, callback=self.parse)

# Scrapyì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ë“¤
scrapy_features = {
    "ë¹„ë™ê¸° ì²˜ë¦¬": "ë™ì‹œì— ìˆ˜ë°± ê°œ ìš”ì²­ ì²˜ë¦¬",
    "ìë™ ì¬ì‹œë„": "ì‹¤íŒ¨í•œ ìš”ì²­ ìë™ ì¬ì‹œë„",
    "ë¡œë´‡ ë°°ì œ": "robots.txt ìë™ ì¤€ìˆ˜",
    "ë¯¸ë“¤ì›¨ì–´": "ìš”ì²­/ì‘ë‹µ ìë™ ì²˜ë¦¬",
    "íŒŒì´í”„ë¼ì¸": "ë°ì´í„° ê²€ì¦, ì¤‘ë³µ ì œê±°, ì €ì¥",
    "ë¶„ì‚° ì²˜ë¦¬": "ì—¬ëŸ¬ ì„œë²„ì—ì„œ ë™ì‹œ í¬ë¡¤ë§"
}

print("ğŸ•·ï¸ Scrapy ì£¼ìš” ê¸°ëŠ¥:")
for feature, description in scrapy_features.items():
    print(f"  â€¢ {feature}: {description}")
"""
```

#### **ğŸ“Š ë„êµ¬ë³„ ì„±ëŠ¥ ë¹„êµ**

```python
# ê°€ìƒì˜ ì„±ëŠ¥ ë°ì´í„° (ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼)
performance_data = pd.DataFrame({
    "ë„êµ¬": ["requests+BS4", "Selenium", "Scrapy"],
    "ì‹œê°„ë‹¹_í˜ì´ì§€": [1000, 100, 10000],
    "ë©”ëª¨ë¦¬_MB": [50, 200, 80],
    "ë™ì‹œ_ì²˜ë¦¬": [1, 4, 100],
    "í•™ìŠµ_ë‚œì´ë„": [2, 4, 5]  # 1-5 ì²™ë„
})

print("ğŸ† í¬ë¡¤ë§ ë„êµ¬ ì„±ëŠ¥ ë¹„êµ")
print("=" * 40)
print(performance_data.to_string(index=False))

# ì‹œê°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
performance_data['íš¨ìœ¨ì„±_ì ìˆ˜'] = (
    performance_data['ì‹œê°„ë‹¹_í˜ì´ì§€'] / 1000 * 3 +
    (200 - performance_data['ë©”ëª¨ë¦¬_MB']) / 200 * 2 +
    performance_data['ë™ì‹œ_ì²˜ë¦¬'] / 100 * 5 -
    performance_data['í•™ìŠµ_ë‚œì´ë„'] * 0.5
).round(1)

print(f"\nğŸ¯ ì¢…í•© íš¨ìœ¨ì„± ì ìˆ˜:")
for i, row in performance_data.iterrows():
    print(f"  {row['ë„êµ¬']:15}: {row['íš¨ìœ¨ì„±_ì ìˆ˜']:4.1f}ì ")
```

### ğŸ”Œ **API ìš°ì„  ì ‘ê·¼ë²•**

í¬ë¡¤ë§ë³´ë‹¤ ë” ë‚˜ì€ ë°©ë²•ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ - ë°”ë¡œ API ì‚¬ìš©!

```python
def api_vs_crawling():
    """API vs í¬ë¡¤ë§ ë¹„êµ ë¶„ì„"""
    
    comparison = pd.DataFrame({
        "í•­ëª©": ["ë°ì´í„° í’ˆì§ˆ", "ìˆ˜ì§‘ ì†ë„", "ì•ˆì •ì„±", "ë²•ì  ì•ˆì „ì„±", "ìœ ì§€ë³´ìˆ˜"],
        "API": ["â­â­â­â­â­", "â­â­â­â­â­", "â­â­â­â­â­", "â­â­â­â­â­", "â­â­â­â­"],
        "í¬ë¡¤ë§": ["â­â­â­", "â­â­â­", "â­â­", "â­â­", "â­â­"]
    })
    
    print("ğŸ”Œ API vs ğŸ•¸ï¸ í¬ë¡¤ë§ ë¹„êµ")
    print("=" * 40)
    print(comparison.to_string(index=False))
    
    print(f"\nğŸ’¡ ê¶Œì¥ ìˆœì„œ:")
    print(f"  1ï¸âƒ£ API ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ìµœìš°ì„ )")
    print(f"  2ï¸âƒ£ ì •ì  í˜ì´ì§€ë©´ BeautifulSoup")
    print(f"  3ï¸âƒ£ ë™ì  í˜ì´ì§€ë©´ Selenium")
    print(f"  4ï¸âƒ£ ëŒ€ê·œëª¨ë©´ Scrapy")

# ë¬´ë£Œ API ì˜ˆì‹œë“¤
free_apis = {
    "ë‚ ì”¨": "OpenWeatherMap API",
    "ë‰´ìŠ¤": "NewsAPI",
    "ì£¼ì‹": "Alpha Vantage API", 
    "í™˜ìœ¨": "Fixer API",
    "ì˜í™”": "TMDB API",
    "ì†Œì…œ": "Twitter API v2"
}

print(f"\nğŸ†“ ìœ ìš©í•œ ë¬´ë£Œ APIë“¤:")
for category, api_name in free_apis.items():
    print(f"  â€¢ {category}: {api_name}")

api_vs_crawling()
```

---

## ğŸ“ **6ë¶€: ì‹¤ë¬´ ì¤€ë¹„í•˜ê¸°** ğŸ’¼

### âš–ï¸ **ë²•ì Â·ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­**

```python
def check_robots_txt(domain):
    """robots.txt í™•ì¸ í•¨ìˆ˜"""
    import urllib.robotparser
    
    try:
        rp = urllib.robotparser.RobotFileParser()
        rp.set_url(f"https://{domain}/robots.txt")
        rp.read()
        
        # ì¼ë°˜ì ì¸ ê²½ë¡œë“¤ ì²´í¬
        paths_to_check = ["/", "/search", "/api", "/admin"]
        
        print(f"ğŸ¤– {domain}ì˜ robots.txt ë¶„ì„:")
        for path in paths_to_check:
            allowed = rp.can_fetch("*", f"https://{domain}{path}")
            status = "âœ… í—ˆìš©" if allowed else "âŒ ê¸ˆì§€"
            print(f"  {path:10}: {status}")
            
    except Exception as e:
        print(f"âŒ robots.txt í™•ì¸ ì‹¤íŒ¨: {e}")

# ì‚¬ìš©ë²• (ì˜ˆì‹œ)
# check_robots_txt("example.com")

# í¬ë¡¤ë§ ìœ¤ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸
ethics_checklist = [
    "robots.txt íŒŒì¼ í™•ì¸í•˜ê³  ì¤€ìˆ˜í•˜ê¸°",
    "ì ì ˆí•œ ì§€ì—°ì‹œê°„(0.5ì´ˆ ì´ìƒ) ì„¤ì •í•˜ê¸°", 
    "User-Agent í—¤ë” ì •í™•íˆ ì„¤ì •í•˜ê¸°",
    "ê°œì¸ì •ë³´ ìˆ˜ì§‘ ì‹œ ë™ì˜ í™•ì¸í•˜ê¸°",
    "ì €ì‘ê¶Œì´ ìˆëŠ” ì½˜í…ì¸  ì£¼ì˜í•˜ê¸°",
    "ì„œë²„ ë¶€í•˜ ìµœì†Œí™”í•˜ê¸°",
    "ìˆ˜ì§‘ ëª©ì ê³¼ ë²”ìœ„ ëª…í™•íˆ í•˜ê¸°"
]

print("âœ… í¬ë¡¤ë§ ìœ¤ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸:")
for i, item in enumerate(ethics_checklist, 1):
    print(f"  {i}. {item}")
```

### ğŸ› ï¸ **í”„ë¡œë•ì…˜ ìˆ˜ì¤€ í¬ë¡¤ëŸ¬ êµ¬ì¡°**

```python
import time
import logging
from typing import List, Dict
import requests
from bs4 import BeautifulSoup
import pandas as pd

class ProductionCrawler:
    """ì‹¤ë¬´ ìˆ˜ì¤€ì˜ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    def __init__(self, delay: float = 1.0):
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Educational-Crawler/1.0 (+http://example.com/bot)'
        })
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def safe_request(self, url: str, retries: int = 3) -> requests.Response:
        """ì•ˆì „í•œ HTTP ìš”ì²­ (ì¬ì‹œë„ í¬í•¨)"""
        for attempt in range(retries):
            try:
                self.logger.info(f"ìš”ì²­ ì¤‘: {url} (ì‹œë„ {attempt + 1})")
                
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                # ì§€ì—° ì‹œê°„ ì ìš©
                time.sleep(self.delay)
                return response
                
            except requests.RequestException as e:
                self.logger.warning(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {e}")
                if attempt == retries - 1:
                    raise
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
    
    def extract_data(self, html: str) -> List[Dict]:
        """HTMLì—ì„œ ë°ì´í„° ì¶”ì¶œ (í•˜ìœ„ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError("extract_data ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì£¼ì„¸ìš”")
    
    def crawl_urls(self, urls: List[str]) -> pd.DataFrame:
        """ì—¬ëŸ¬ URL í¬ë¡¤ë§"""
        all_data = []
        
        for i, url in enumerate(urls, 1):
            try:
                self.logger.info(f"ì§„í–‰ë¥ : {i}/{len(urls)} ({i/len(urls)*100:.1f}%)")
                
                response = self.safe_request(url)
                data = self.extract_data(response.text)
                all_data.extend(data)
                
            except Exception as e:
                self.logger.error(f"URL ì²˜ë¦¬ ì‹¤íŒ¨ {url}: {e}")
                continue
        
        df = pd.DataFrame(all_data)
        self.logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ ìˆ˜ì§‘")
        return df

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
class NewsCrawler(ProductionCrawler):
    """ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì˜ˆì‹œ"""
    
    def extract_data(self, html: str) -> List[Dict]:
        soup = BeautifulSoup(html, 'html.parser')
        
        # ì‹¤ì œë¡œëŠ” íŠ¹ì • ë‰´ìŠ¤ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ì¶° ìˆ˜ì •
        articles = []
        for article in soup.select('article'):
            title_elem = article.select_one('h2')
            if title_elem:
                articles.append({
                    'title': title_elem.get_text(strip=True),
                    'timestamp': pd.Timestamp.now(),
                    'source': 'example_news'
                })
        
        return articles

print("ğŸ—ï¸ ì‹¤ë¬´ ìˆ˜ì¤€ í¬ë¡¤ëŸ¬ êµ¬ì¡° ì™„ì„±!")
print("  â€¢ ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§")  
print("  â€¢ ë¡œê¹… ì‹œìŠ¤í…œ")
print("  â€¢ ì†ë„ ì œí•œ")
print("  â€¢ í™•ì¥ ê°€ëŠ¥í•œ í´ë˜ìŠ¤ ì„¤ê³„")
```

---

## ğŸ¯ **ë§ˆë¬´ë¦¬: ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„**

### ğŸ“‹ **15ì¥ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸**

ì´ì œ ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í™•ì¸í•´ë³´ì„¸ìš”:

```python
# ìê°€ ì§„ë‹¨ í€´ì¦ˆ
quiz_questions = {
    "ì›¹ ë™ì‘ ì›ë¦¬": "í´ë¼ì´ì–¸íŠ¸-ì„œë²„ ëª¨ë¸ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‚˜ìš”?",
    "HTML êµ¬ì¡°": "DOM íŠ¸ë¦¬ì™€ CSS ì„ íƒìë¥¼ ì´í•´í–ˆë‚˜ìš”?", 
    "íŒŒì‹± ì›ë¦¬": "BeautifulSoupì˜ ë™ì‘ ë°©ì‹ì„ ì•„ì‹œë‚˜ìš”?",
    "pandas ì—°ë™": "í¬ë¡¤ë§ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‚˜ìš”?",
    "ë„êµ¬ ì„ íƒ": "ì •ì /ë™ì  ì‚¬ì´íŠ¸ë³„ë¡œ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë‚˜ìš”?",
    "ìœ¤ë¦¬ ì¤€ìˆ˜": "robots.txtì™€ í¬ë¡¤ë§ ìœ¤ë¦¬ë¥¼ ì´í•´í–ˆë‚˜ìš”?"
}

print("ğŸ“‹ 15ì¥ ìê°€ ì§„ë‹¨ ì²´í¬ë¦¬ìŠ¤íŠ¸")
print("=" * 50)

for topic, question in quiz_questions.items():
    print(f"â˜‘ï¸ {topic}: {question}")

print(f"\nğŸ“ ëª¨ë“  í•­ëª©ì— ìì‹  ìˆê²Œ ë‹µí•  ìˆ˜ ìˆë‹¤ë©´,")
print(f"   16ì¥ ì‹¤ì „ í¬ë¡¤ë§ ì¤€ë¹„ ì™„ë£Œì…ë‹ˆë‹¤!")
```

### ğŸš€ **16ì¥ ì—°ê²° í¬ì¸íŠ¸**

```python
# 16ì¥ì—ì„œ ë°°ìš¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
next_chapter_preview = {
    "16. ì‹¤ì „ í¬ë¡¤ë§": [
        "quotes.toscrape.com ê¸°ì´ˆ ì‹¤ìŠµ",
        "ë‹¤ì¤‘ í˜ì´ì§€ ì²˜ë¦¬ (í˜ì´ì§•)",  
        "CSV íŒŒì¼ë¡œ ë°ì´í„° ì €ì¥",
        "pandas DataFrame ì—°ë™ ì‹¤ìŠµ",
        "ì—ëŸ¬ ì²˜ë¦¬ì™€ ì˜ˆì™¸ ìƒí™© ëŒ€ì‘"
    ],
    "17. ê³ ê¸‰ í¬ë¡¤ë§": [
        "ì‹¤ì œ ì‡¼í•‘ëª°/ë‰´ìŠ¤ ì‚¬ì´íŠ¸ í¬ë¡¤ë§",
        "XML/JSON ë°ì´í„° ì²˜ë¦¬",
        "ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê¸°ë²•", 
        "Selenium ë™ì  ì‚¬ì´íŠ¸ í¬ë¡¤ë§",
        "ì‹¤ë¬´ í”„ë¡œì íŠ¸ ì™„ì„±"
    ]
}

print("ğŸ”® ë‹¤ìŒ í•™ìŠµ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°")
print("=" * 40)

for chapter, topics in next_chapter_preview.items():
    print(f"\nğŸ“š {chapter}")
    for topic in topics:
        print(f"  â€¢ {topic}")

print(f"\nğŸ’ª ì§€ê¸ˆê¹Œì§€ ë°°ìš´ pandas ê¸°ì´ˆ(14ì¥)ì™€")
print(f"   ì›¹ í¬ë¡¤ë§ ê°œë…(15ì¥)ì´ ì™„ë²½í•˜ê²Œ ê²°í•©ë˜ì–´")
print(f"   ì‹¤ì „ ë°ì´í„° ë¶„ì„ê°€ë¡œ ì„±ì¥í•˜ê²Œ ë©ë‹ˆë‹¤!")
```

---

## ğŸ’¡ **í•µì‹¬ ìš”ì•½**

### ğŸ¯ **ì´ë²ˆ ì¥ì—ì„œ ë°°ìš´ ê²ƒ**

1. **ì›¹ì˜ ê¸°ë³¸ êµ¬ì¡°**: í´ë¼ì´ì–¸íŠ¸-ì„œë²„, HTTP í†µì‹ , HTML/CSS
2. **íŒŒì‹±ì˜ ì›ë¦¬**: í…ìŠ¤íŠ¸ â†’ êµ¬ì¡°í™” ë°ì´í„° ë³€í™˜ ê³¼ì •
3. **BeautifulSoup ì™„ì „ ì •ë³µ**: find vs select, íŒŒì„œ ì„ íƒ
4. **pandas ì—°ë™**: í¬ë¡¤ë§ â†’ DataFrame â†’ ë¶„ì„ì˜ ì™„ë²½í•œ íë¦„
5. **ê³ ê¸‰ ë„êµ¬ ë¡œë“œë§µ**: Selenium, Scrapy, API í™œìš© ì „ëµ
6. **ì‹¤ë¬´ ì¤€ë¹„**: ìœ¤ë¦¬, ë²•ì  ê³ ë ¤ì‚¬í•­, í”„ë¡œë•ì…˜ ì½”ë“œ êµ¬ì¡°

### ğŸ”— **ì „ì²´ í•™ìŠµ ì—°ê²°ê³ ë¦¬**

```
NumPy(ê¸°ì´ˆ) â†’ pandas(14ì¥) â†’ ì›¹ í¬ë¡¤ë§(15ì¥) â†’ ì‹¤ì „ í”„ë¡œì íŠ¸(16-17ì¥)
     â†“              â†“                â†“                    â†“
   ë°°ì—´ ì—°ì‚°    ë°ì´í„° ì²˜ë¦¬      ë°ì´í„° ìˆ˜ì§‘         ì‹¤ë¬´ ì‘ìš©
```

**ğŸš€ ì´ì œ ì •ë§ë¡œ ì‹¤ì „ í¬ë¡¤ë§ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**

16ì¥ì—ì„œëŠ” ë°”ë¡œ ì½”ë”©ë¶€í„° ì‹œì‘í•´ì„œ ì‹¤ì œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤! ğŸ’ª
