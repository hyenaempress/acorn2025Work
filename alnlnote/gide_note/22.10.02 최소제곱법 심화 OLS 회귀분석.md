# 📊 OLS 회귀분석 완전 마스터 가이드
## 🎯 다중선형회귀 이해를 위한 필수 개념

---

## 🔥 왜 선생님이 OLS를 강조하시는가?

### 💡 **핵심 이유**
```
OLS = 모든 회귀분석의 '수학적 엔진' 🚗
- 단순회귀 → OLS로 계산
- 다중회귀 → OLS로 계산  
- 머신러닝 → OLS 기반 확장

"OLS를 이해하면 회귀분석의 90%를 이해한 것!"
```

---

## 📈 1. OLS란 무엇인가?

### 🎯 **정의**
```python
OLS (Ordinary Least Squares) = 최소제곱법
```
**"오차제곱합을 최소화하는 최적의 직선(평면)을 찾는 방법"**

### 🔢 **수학적 원리**
```
목표: Σ(실제값 - 예측값)² 를 최소화

y = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ + ε

여기서:
- y: 종속변수 (예측하고 싶은 것)
- x₁,x₂,...: 독립변수들 (설명변수)
- β₀: 절편 (intercept)
- β₁,β₂,...: 회귀계수 (기울기)
- ε: 오차항
```

---

## 🔧 2. Python에서 OLS 사용하기

### 📊 **Statsmodels OLS 기본 문법**

```python
import statsmodels.api as sm
import pandas as pd

# 데이터 준비
df = pd.DataFrame({
    '만족도': [3, 4, 2, 5, 3, 4, 1, 5, 2, 4],
    '적절성': [4, 5, 2, 4, 3, 5, 2, 4, 3, 5],
    '친밀도': [2, 3, 1, 4, 2, 3, 1, 4, 2, 3]
})

# OLS 모델 생성
model = sm.OLS(종속변수, 독립변수).fit()

# 결과 출력
print(model.summary())
```

### 🎯 **실제 예제 - 강의에서 다룬 내용**

```python
import statsmodels.api as sm
import pandas as pd
import numpy as np

# 드링킹워터 데이터 (강의 예제)
# 종속변수: 만족도, 독립변수: 적절성

# Step 1: 데이터 준비
y = df['만족도']  # 종속변수
X = df['적절성']  # 독립변수
X = sm.add_constant(X)  # 절편 추가 (중요!)

# Step 2: OLS 모델 실행
model = sm.OLS(y, X).fit()

# Step 3: 결과 확인
print(model.summary())
```

---

## 📋 3. OLS Results 완전 해석 (강의 핵심!)

### 🎯 **OLS Results 표 구조**

```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   만족도   R-squared:                       0.588
Model:                            OLS   Adj. R-squared:                  0.537
Method:                 Least Squares   F-statistic:                     11.43
Date:                Mon, 22 Nov 2024   Prob (F-statistic):            0.00896
Time:                        14:30:00   Log-Likelihood:                -26.789
No. Observations:                  10   AIC:                             57.58
Df Residuals:                       8   BIC:                             58.38
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.7393      0.XXX      X.XXX      0.XXX       X.XXX       X.XXX
적절성         0.7393      0.XXX      X.XXX      0.XXX       X.XXX       X.XXX
==============================================================================
Omnibus:                        X.XXX   Durbin-Watson:                   X.XXX
Prob(Omnibus):                  X.XXX   Jarque-Bera (JB):               X.XXX
Skew:                           X.XXX   Prob(JB):                        X.XXX
Kurtosis:                       X.XXX   Cond. No.                        X.XXX
==============================================================================
```

### 🔍 **각 수치의 의미 (강의 내용)**

#### **📊 모델 전체 성능**
```python
# R-squared (결정계수): 설명력
R² = 0.588  # 58.8%의 변동을 설명
"적절성이 만족도 변동의 58.8%를 설명한다"

# F-statistic: 모델 전체의 유의성
F = 11.43, p-value = 0.00896 < 0.05
"모델이 통계적으로 유의하다"
```

#### **📈 회귀계수 해석**
```python
# 절편 (const): 0.7393
"적절성이 0일 때 만족도의 기댓값"

# 기울기 (적절성): 0.7393  
"적절성이 1 증가하면 만족도가 0.7393 증가"
```

#### **🔬 통계적 검정**
```python
# t-statistic: 회귀계수의 유의성
t값 = 계수 / 표준오차
p-value < 0.05 이면 유의

# P>|t|: p-value
"이 계수가 0과 다른지 검정"
```

---

## 🧮 4. 다중선형회귀로의 확장

### 🎯 **단순 → 다중 회귀**

```python
# 단순회귀 (변수 1개)
y = β₀ + β₁x₁ + ε
model = sm.OLS(y, sm.add_constant(X1)).fit()

# 다중회귀 (변수 여러개)  
y = β₀ + β₁x₁ + β₂x₂ + β₃x₃ + ε

# 실제 코드
X_multi = df[['적절성', '친밀도', '중요성']]
X_multi = sm.add_constant(X_multi)
model_multi = sm.OLS(y, X_multi).fit()
```

### 📊 **다중회귀에서 추가로 봐야 할 것들**

#### **1. 수정된 R² (Adjusted R²)**
```python
# 일반 R²: 변수 추가하면 무조건 증가 (나쁜 지표)
# 수정된 R²: 의미있는 변수만 증가시킴 (좋은 지표)

"다중회귀에서는 Adj. R²를 봐야 한다!"
```

#### **2. 다중공선성 체크**
```python
# VIF (Variance Inflation Factor)
from statsmodels.stats.outliers_influence import variance_inflation_factor

# VIF > 10 이면 다중공선성 문제
for i in range(X_multi.shape[1]):
    vif = variance_inflation_factor(X_multi.values, i)
    print(f"{X_multi.columns[i]}: {vif:.2f}")
```

#### **3. 모든 계수의 해석**
```python
print("📈 회귀계수 해석:")
print("절편:", model_multi.params['const'])
print("적절성 효과:", model_multi.params['적절성']) 
print("친밀도 효과:", model_multi.params['친밀도'])
print("중요성 효과:", model_multi.params['중요성'])

# 예: "다른 변수가 동일할 때, 적절성 1증가 → 만족도 X 증가"
```

---

## 🔥 5. 실무에서 OLS 활용법

### 📊 **모델 구축 단계**

```python
# Step 1: 데이터 탐색
print("=== 1. 데이터 기초 통계 ===")
print(df.describe())
print("\n=== 2. 상관관계 ===")
print(df.corr())

# Step 2: OLS 모델
print("\n=== 3. OLS 회귀분석 ===")
y = df['만족도']
X = df[['적절성', '친밀도', '중요성']]
X = sm.add_constant(X)

model = sm.OLS(y, X).fit()
print(model.summary())

# Step 3: 모델 진단
print("\n=== 4. 모델 진단 ===")
print(f"R²: {model.rsquared:.3f}")
print(f"Adj. R²: {model.rsquared_adj:.3f}")
print(f"F-statistic p-value: {model.f_pvalue:.3f}")

# Step 4: 예측
print("\n=== 5. 예측 ===")
new_data = pd.DataFrame({
    'const': [1],
    '적절성': [4.5],
    '친밀도': [3.2], 
    '중요성': [4.0]
})
prediction = model.predict(new_data)
print(f"예측 만족도: {prediction[0]:.2f}")
```

### 🎯 **체크리스트**

```python
def ols_체크리스트():
    체크항목 = [
        "✅ 절편(const) 추가했는가? - sm.add_constant()",
        "✅ R²가 적절한가? (0.3 이상)",
        "✅ F-statistic p-value < 0.05인가?",
        "✅ 각 계수의 p-value < 0.05인가?",
        "✅ 잔차가 정규분포를 따르는가?",
        "✅ 다중공선성 문제는 없는가? (VIF < 10)",
        "✅ Durbin-Watson이 2에 가까운가?"
    ]
    
    for 항목 in 체크항목:
        print(항목)

ols_체크리스트()
```

---

## 🚀 6. OLS vs Scikit-learn 비교

### 📊 **언제 무엇을 쓸까?**

```python
# 🎯 OLS (Statsmodels) 사용 시기
- 통계적 해석이 중요할 때
- p-value, 신뢰구간 필요할 때  
- 회귀 가정 검정할 때
- 논문, 보고서 작성할 때

# 🚀 Scikit-learn 사용 시기  
- 예측 성능이 중요할 때
- 머신러닝 파이프라인 구축할 때
- 교차검증, 하이퍼파라미터 튜닝할 때
- 대용량 데이터 처리할 때
```

### 🔄 **코드 비교**

```python
# === Statsmodels OLS ===
import statsmodels.api as sm

X = sm.add_constant(df[['적절성', '친밀도']])
y = df['만족도']
model_ols = sm.OLS(y, X).fit()
print(model_ols.summary())  # 풍부한 통계 정보

# === Scikit-learn ===
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

X = df[['적절성', '친밀도']]  # 자동으로 절편 추가
y = df['만족도']
model_sklearn = LinearRegression().fit(X, y)
print(f"R²: {model_sklearn.score(X, y):.3f}")  # 간단한 정보
```

---

## 💡 7. 자주 나오는 실수와 해결법

### ⚠️ **흔한 실수들**

```python
# 🚫 실수 1: 절편 안 넣기
X = df[['적절성']]  # 잘못됨
model = sm.OLS(y, X).fit()

# ✅ 올바른 방법
X = sm.add_constant(df[['적절성']])
model = sm.OLS(y, X).fit()

# 🚫 실수 2: 다중공선성 무시
# 상관관계 높은 변수들 동시 투입

# ✅ 올바른 방법  
print(df.corr())  # 먼저 상관관계 확인
# 상관관계 > 0.8인 변수들 중 하나 제거

# 🚫 실수 3: 결과 해석 오류
"R² = 0.588은 58.8% 정확도" # 잘못된 해석

# ✅ 올바른 해석
"R² = 0.588은 독립변수가 종속변수 변동의 58.8%를 설명"
```

---

## 🎯 8. 실습: 완전한 OLS 분석

### 📊 **종합 실습 코드**

```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# === 데이터 준비 ===
np.random.seed(42)
n = 100

df = pd.DataFrame({
    'TV광고': np.random.normal(50, 15, n),
    '라디오광고': np.random.normal(30, 10, n), 
    '신문광고': np.random.normal(20, 8, n)
})

# 매출 = 실제 관계 + 노이즈
df['매출'] = (0.05 * df['TV광고'] + 
              0.12 * df['라디오광고'] + 
              0.02 * df['신문광고'] + 
              np.random.normal(0, 2, n))

print("=== 📊 1. 기초 통계 ===")
print(df.describe())

print("\n=== 📈 2. 상관관계 ===")
correlation = df.corr()
print(correlation)

# === OLS 회귀분석 ===
print("\n=== 🔥 3. OLS 회귀분석 ===")
y = df['매출']
X = df[['TV광고', '라디오광고', '신문광고']]
X = sm.add_constant(X)

model = sm.OLS(y, X).fit()
print(model.summary())

# === 핵심 결과 추출 ===
print("\n=== 🎯 4. 핵심 결과 해석 ===")
print(f"R²: {model.rsquared:.3f} ({model.rsquared*100:.1f}% 설명)")
print(f"Adj. R²: {model.rsquared_adj:.3f}")
print(f"전체 모델 유의성: p = {model.f_pvalue:.3f}")

print("\n📈 회귀계수:")
for var, coef, pval in zip(X.columns, model.params, model.pvalues):
    sig = "***" if pval < 0.001 else "**" if pval < 0.01 else "*" if pval < 0.05 else ""
    print(f"  {var}: {coef:.4f} (p={pval:.3f}) {sig}")

# === 예측 ===
print("\n=== 🔮 5. 예측 예시 ===")
new_campaign = pd.DataFrame({
    'const': [1],
    'TV광고': [60],
    '라디오광고': [35],
    '신문광고': [25]
})

prediction = model.predict(new_campaign)
print(f"예상 매출: {prediction[0]:.2f}")

# 시각화
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(df['TV광고'], df['매출'])
plt.xlabel('TV광고')
plt.ylabel('매출')
plt.title('TV광고 vs 매출')

plt.subplot(1, 3, 2)
plt.scatter(df['라디오광고'], df['매출'])
plt.xlabel('라디오광고') 
plt.ylabel('매출')
plt.title('라디오광고 vs 매출')

plt.subplot(1, 3, 3)
plt.scatter(model.fittedvalues, model.resid)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('예측값')
plt.ylabel('잔차')
plt.title('잔차 플롯')

plt.tight_layout()
plt.show()

print("\n✅ OLS 분석 완료!")
```

---

## 🏆 정리: OLS 마스터 포인트

### 🎯 **꼭 기억해야 할 것들**

1. **🔧 OLS = 최소제곱법의 Python 구현체**
2. **📊 statsmodels가 OLS의 표준 라이브러리**  
3. **📈 `sm.add_constant()` 반드시 필요**
4. **🎯 R², F-statistic, p-value 3대 지표**
5. **🔍 다중회귀에서는 Adj. R² 사용**
6. **⚠️ 다중공선성 체크 필수**

### 🚀 **다중선형회귀 준비 완료!**

```python
print("🎓 축하합니다!")
print("OLS 개념을 완벽 마스터했습니다!")
print("이제 다중선형회귀의 모든 것을 이해할 준비가 되었습니다! 🚀")
```

---

## 📚 다음 학습 단계

1. **다중선형회귀 심화** - 변수 선택, 정규화
2. **회귀 가정 검정** - 잔차 분석, 이상치 처리  
3. **고급 회귀기법** - Ridge, Lasso, Elastic Net
4. **머신러닝 연결** - scikit-learn 완전 마스터

**"OLS를 정복했으니, 이제 진짜 회귀분석의 세계로 떠날 시간입니다! 🌟"**