# ğŸ“Š OLS íšŒê·€ë¶„ì„ ì™„ì „ ë§ˆìŠ¤í„° ê°€ì´ë“œ
## ğŸ¯ ë‹¤ì¤‘ì„ í˜•íšŒê·€ ì´í•´ë¥¼ ìœ„í•œ í•„ìˆ˜ ê°œë…

---

## ğŸ”¥ ì™œ ì„ ìƒë‹˜ì´ OLSë¥¼ ê°•ì¡°í•˜ì‹œëŠ”ê°€?

### ğŸ’¡ **í•µì‹¬ ì´ìœ **
```
OLS = ëª¨ë“  íšŒê·€ë¶„ì„ì˜ 'ìˆ˜í•™ì  ì—”ì§„' ğŸš—
- ë‹¨ìˆœíšŒê·€ â†’ OLSë¡œ ê³„ì‚°
- ë‹¤ì¤‘íšŒê·€ â†’ OLSë¡œ ê³„ì‚°  
- ë¨¸ì‹ ëŸ¬ë‹ â†’ OLS ê¸°ë°˜ í™•ì¥

"OLSë¥¼ ì´í•´í•˜ë©´ íšŒê·€ë¶„ì„ì˜ 90%ë¥¼ ì´í•´í•œ ê²ƒ!"
```

---

## ğŸ“ˆ 1. OLSë€ ë¬´ì—‡ì¸ê°€?

### ğŸ¯ **ì •ì˜**
```python
OLS (Ordinary Least Squares) = ìµœì†Œì œê³±ë²•
```
**"ì˜¤ì°¨ì œê³±í•©ì„ ìµœì†Œí™”í•˜ëŠ” ìµœì ì˜ ì§ì„ (í‰ë©´)ì„ ì°¾ëŠ” ë°©ë²•"**

### ğŸ”¢ **ìˆ˜í•™ì  ì›ë¦¬**
```
ëª©í‘œ: Î£(ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’)Â² ë¥¼ ìµœì†Œí™”

y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚šxâ‚š + Îµ

ì—¬ê¸°ì„œ:
- y: ì¢…ì†ë³€ìˆ˜ (ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ ê²ƒ)
- xâ‚,xâ‚‚,...: ë…ë¦½ë³€ìˆ˜ë“¤ (ì„¤ëª…ë³€ìˆ˜)
- Î²â‚€: ì ˆí¸ (intercept)
- Î²â‚,Î²â‚‚,...: íšŒê·€ê³„ìˆ˜ (ê¸°ìš¸ê¸°)
- Îµ: ì˜¤ì°¨í•­
```

---

## ğŸ”§ 2. Pythonì—ì„œ OLS ì‚¬ìš©í•˜ê¸°

### ğŸ“Š **Statsmodels OLS ê¸°ë³¸ ë¬¸ë²•**

```python
import statsmodels.api as sm
import pandas as pd

# ë°ì´í„° ì¤€ë¹„
df = pd.DataFrame({
    'ë§Œì¡±ë„': [3, 4, 2, 5, 3, 4, 1, 5, 2, 4],
    'ì ì ˆì„±': [4, 5, 2, 4, 3, 5, 2, 4, 3, 5],
    'ì¹œë°€ë„': [2, 3, 1, 4, 2, 3, 1, 4, 2, 3]
})

# OLS ëª¨ë¸ ìƒì„±
model = sm.OLS(ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜).fit()

# ê²°ê³¼ ì¶œë ¥
print(model.summary())
```

### ğŸ¯ **ì‹¤ì œ ì˜ˆì œ - ê°•ì˜ì—ì„œ ë‹¤ë£¬ ë‚´ìš©**

```python
import statsmodels.api as sm
import pandas as pd
import numpy as np

# ë“œë§í‚¹ì›Œí„° ë°ì´í„° (ê°•ì˜ ì˜ˆì œ)
# ì¢…ì†ë³€ìˆ˜: ë§Œì¡±ë„, ë…ë¦½ë³€ìˆ˜: ì ì ˆì„±

# Step 1: ë°ì´í„° ì¤€ë¹„
y = df['ë§Œì¡±ë„']  # ì¢…ì†ë³€ìˆ˜
X = df['ì ì ˆì„±']  # ë…ë¦½ë³€ìˆ˜
X = sm.add_constant(X)  # ì ˆí¸ ì¶”ê°€ (ì¤‘ìš”!)

# Step 2: OLS ëª¨ë¸ ì‹¤í–‰
model = sm.OLS(y, X).fit()

# Step 3: ê²°ê³¼ í™•ì¸
print(model.summary())
```

---

## ğŸ“‹ 3. OLS Results ì™„ì „ í•´ì„ (ê°•ì˜ í•µì‹¬!)

### ğŸ¯ **OLS Results í‘œ êµ¬ì¡°**

```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   ë§Œì¡±ë„   R-squared:                       0.588
Model:                            OLS   Adj. R-squared:                  0.537
Method:                 Least Squares   F-statistic:                     11.43
Date:                Mon, 22 Nov 2024   Prob (F-statistic):            0.00896
Time:                        14:30:00   Log-Likelihood:                -26.789
No. Observations:                  10   AIC:                             57.58
Df Residuals:                       8   BIC:                             58.38
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.7393      0.XXX      X.XXX      0.XXX       X.XXX       X.XXX
ì ì ˆì„±         0.7393      0.XXX      X.XXX      0.XXX       X.XXX       X.XXX
==============================================================================
Omnibus:                        X.XXX   Durbin-Watson:                   X.XXX
Prob(Omnibus):                  X.XXX   Jarque-Bera (JB):               X.XXX
Skew:                           X.XXX   Prob(JB):                        X.XXX
Kurtosis:                       X.XXX   Cond. No.                        X.XXX
==============================================================================
```

### ğŸ” **ê° ìˆ˜ì¹˜ì˜ ì˜ë¯¸ (ê°•ì˜ ë‚´ìš©)**

#### **ğŸ“Š ëª¨ë¸ ì „ì²´ ì„±ëŠ¥**
```python
# R-squared (ê²°ì •ê³„ìˆ˜): ì„¤ëª…ë ¥
RÂ² = 0.588  # 58.8%ì˜ ë³€ë™ì„ ì„¤ëª…
"ì ì ˆì„±ì´ ë§Œì¡±ë„ ë³€ë™ì˜ 58.8%ë¥¼ ì„¤ëª…í•œë‹¤"

# F-statistic: ëª¨ë¸ ì „ì²´ì˜ ìœ ì˜ì„±
F = 11.43, p-value = 0.00896 < 0.05
"ëª¨ë¸ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ë‹¤"
```

#### **ğŸ“ˆ íšŒê·€ê³„ìˆ˜ í•´ì„**
```python
# ì ˆí¸ (const): 0.7393
"ì ì ˆì„±ì´ 0ì¼ ë•Œ ë§Œì¡±ë„ì˜ ê¸°ëŒ“ê°’"

# ê¸°ìš¸ê¸° (ì ì ˆì„±): 0.7393  
"ì ì ˆì„±ì´ 1 ì¦ê°€í•˜ë©´ ë§Œì¡±ë„ê°€ 0.7393 ì¦ê°€"
```

#### **ğŸ”¬ í†µê³„ì  ê²€ì •**
```python
# t-statistic: íšŒê·€ê³„ìˆ˜ì˜ ìœ ì˜ì„±
tê°’ = ê³„ìˆ˜ / í‘œì¤€ì˜¤ì°¨
p-value < 0.05 ì´ë©´ ìœ ì˜

# P>|t|: p-value
"ì´ ê³„ìˆ˜ê°€ 0ê³¼ ë‹¤ë¥¸ì§€ ê²€ì •"
```

---

## ğŸ§® 4. ë‹¤ì¤‘ì„ í˜•íšŒê·€ë¡œì˜ í™•ì¥

### ğŸ¯ **ë‹¨ìˆœ â†’ ë‹¤ì¤‘ íšŒê·€**

```python
# ë‹¨ìˆœíšŒê·€ (ë³€ìˆ˜ 1ê°œ)
y = Î²â‚€ + Î²â‚xâ‚ + Îµ
model = sm.OLS(y, sm.add_constant(X1)).fit()

# ë‹¤ì¤‘íšŒê·€ (ë³€ìˆ˜ ì—¬ëŸ¬ê°œ)  
y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + Î²â‚ƒxâ‚ƒ + Îµ

# ì‹¤ì œ ì½”ë“œ
X_multi = df[['ì ì ˆì„±', 'ì¹œë°€ë„', 'ì¤‘ìš”ì„±']]
X_multi = sm.add_constant(X_multi)
model_multi = sm.OLS(y, X_multi).fit()
```

### ğŸ“Š **ë‹¤ì¤‘íšŒê·€ì—ì„œ ì¶”ê°€ë¡œ ë´ì•¼ í•  ê²ƒë“¤**

#### **1. ìˆ˜ì •ëœ RÂ² (Adjusted RÂ²)**
```python
# ì¼ë°˜ RÂ²: ë³€ìˆ˜ ì¶”ê°€í•˜ë©´ ë¬´ì¡°ê±´ ì¦ê°€ (ë‚˜ìœ ì§€í‘œ)
# ìˆ˜ì •ëœ RÂ²: ì˜ë¯¸ìˆëŠ” ë³€ìˆ˜ë§Œ ì¦ê°€ì‹œí‚´ (ì¢‹ì€ ì§€í‘œ)

"ë‹¤ì¤‘íšŒê·€ì—ì„œëŠ” Adj. RÂ²ë¥¼ ë´ì•¼ í•œë‹¤!"
```

#### **2. ë‹¤ì¤‘ê³µì„ ì„± ì²´í¬**
```python
# VIF (Variance Inflation Factor)
from statsmodels.stats.outliers_influence import variance_inflation_factor

# VIF > 10 ì´ë©´ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ
for i in range(X_multi.shape[1]):
    vif = variance_inflation_factor(X_multi.values, i)
    print(f"{X_multi.columns[i]}: {vif:.2f}")
```

#### **3. ëª¨ë“  ê³„ìˆ˜ì˜ í•´ì„**
```python
print("ğŸ“ˆ íšŒê·€ê³„ìˆ˜ í•´ì„:")
print("ì ˆí¸:", model_multi.params['const'])
print("ì ì ˆì„± íš¨ê³¼:", model_multi.params['ì ì ˆì„±']) 
print("ì¹œë°€ë„ íš¨ê³¼:", model_multi.params['ì¹œë°€ë„'])
print("ì¤‘ìš”ì„± íš¨ê³¼:", model_multi.params['ì¤‘ìš”ì„±'])

# ì˜ˆ: "ë‹¤ë¥¸ ë³€ìˆ˜ê°€ ë™ì¼í•  ë•Œ, ì ì ˆì„± 1ì¦ê°€ â†’ ë§Œì¡±ë„ X ì¦ê°€"
```

---

## ğŸ”¥ 5. ì‹¤ë¬´ì—ì„œ OLS í™œìš©ë²•

### ğŸ“Š **ëª¨ë¸ êµ¬ì¶• ë‹¨ê³„**

```python
# Step 1: ë°ì´í„° íƒìƒ‰
print("=== 1. ë°ì´í„° ê¸°ì´ˆ í†µê³„ ===")
print(df.describe())
print("\n=== 2. ìƒê´€ê´€ê³„ ===")
print(df.corr())

# Step 2: OLS ëª¨ë¸
print("\n=== 3. OLS íšŒê·€ë¶„ì„ ===")
y = df['ë§Œì¡±ë„']
X = df[['ì ì ˆì„±', 'ì¹œë°€ë„', 'ì¤‘ìš”ì„±']]
X = sm.add_constant(X)

model = sm.OLS(y, X).fit()
print(model.summary())

# Step 3: ëª¨ë¸ ì§„ë‹¨
print("\n=== 4. ëª¨ë¸ ì§„ë‹¨ ===")
print(f"RÂ²: {model.rsquared:.3f}")
print(f"Adj. RÂ²: {model.rsquared_adj:.3f}")
print(f"F-statistic p-value: {model.f_pvalue:.3f}")

# Step 4: ì˜ˆì¸¡
print("\n=== 5. ì˜ˆì¸¡ ===")
new_data = pd.DataFrame({
    'const': [1],
    'ì ì ˆì„±': [4.5],
    'ì¹œë°€ë„': [3.2], 
    'ì¤‘ìš”ì„±': [4.0]
})
prediction = model.predict(new_data)
print(f"ì˜ˆì¸¡ ë§Œì¡±ë„: {prediction[0]:.2f}")
```

### ğŸ¯ **ì²´í¬ë¦¬ìŠ¤íŠ¸**

```python
def ols_ì²´í¬ë¦¬ìŠ¤íŠ¸():
    ì²´í¬í•­ëª© = [
        "âœ… ì ˆí¸(const) ì¶”ê°€í–ˆëŠ”ê°€? - sm.add_constant()",
        "âœ… RÂ²ê°€ ì ì ˆí•œê°€? (0.3 ì´ìƒ)",
        "âœ… F-statistic p-value < 0.05ì¸ê°€?",
        "âœ… ê° ê³„ìˆ˜ì˜ p-value < 0.05ì¸ê°€?",
        "âœ… ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ”ê°€?",
        "âœ… ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œëŠ” ì—†ëŠ”ê°€? (VIF < 10)",
        "âœ… Durbin-Watsonì´ 2ì— ê°€ê¹Œìš´ê°€?"
    ]
    
    for í•­ëª© in ì²´í¬í•­ëª©:
        print(í•­ëª©)

ols_ì²´í¬ë¦¬ìŠ¤íŠ¸()
```

---

## ğŸš€ 6. OLS vs Scikit-learn ë¹„êµ

### ğŸ“Š **ì–¸ì œ ë¬´ì—‡ì„ ì“¸ê¹Œ?**

```python
# ğŸ¯ OLS (Statsmodels) ì‚¬ìš© ì‹œê¸°
- í†µê³„ì  í•´ì„ì´ ì¤‘ìš”í•  ë•Œ
- p-value, ì‹ ë¢°êµ¬ê°„ í•„ìš”í•  ë•Œ  
- íšŒê·€ ê°€ì • ê²€ì •í•  ë•Œ
- ë…¼ë¬¸, ë³´ê³ ì„œ ì‘ì„±í•  ë•Œ

# ğŸš€ Scikit-learn ì‚¬ìš© ì‹œê¸°  
- ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì¤‘ìš”í•  ë•Œ
- ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•í•  ë•Œ
- êµì°¨ê²€ì¦, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹í•  ë•Œ
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬í•  ë•Œ
```

### ğŸ”„ **ì½”ë“œ ë¹„êµ**

```python
# === Statsmodels OLS ===
import statsmodels.api as sm

X = sm.add_constant(df[['ì ì ˆì„±', 'ì¹œë°€ë„']])
y = df['ë§Œì¡±ë„']
model_ols = sm.OLS(y, X).fit()
print(model_ols.summary())  # í’ë¶€í•œ í†µê³„ ì •ë³´

# === Scikit-learn ===
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

X = df[['ì ì ˆì„±', 'ì¹œë°€ë„']]  # ìë™ìœ¼ë¡œ ì ˆí¸ ì¶”ê°€
y = df['ë§Œì¡±ë„']
model_sklearn = LinearRegression().fit(X, y)
print(f"RÂ²: {model_sklearn.score(X, y):.3f}")  # ê°„ë‹¨í•œ ì •ë³´
```

---

## ğŸ’¡ 7. ìì£¼ ë‚˜ì˜¤ëŠ” ì‹¤ìˆ˜ì™€ í•´ê²°ë²•

### âš ï¸ **í”í•œ ì‹¤ìˆ˜ë“¤**

```python
# ğŸš« ì‹¤ìˆ˜ 1: ì ˆí¸ ì•ˆ ë„£ê¸°
X = df[['ì ì ˆì„±']]  # ì˜ëª»ë¨
model = sm.OLS(y, X).fit()

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•
X = sm.add_constant(df[['ì ì ˆì„±']])
model = sm.OLS(y, X).fit()

# ğŸš« ì‹¤ìˆ˜ 2: ë‹¤ì¤‘ê³µì„ ì„± ë¬´ì‹œ
# ìƒê´€ê´€ê³„ ë†’ì€ ë³€ìˆ˜ë“¤ ë™ì‹œ íˆ¬ì…

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•  
print(df.corr())  # ë¨¼ì € ìƒê´€ê´€ê³„ í™•ì¸
# ìƒê´€ê´€ê³„ > 0.8ì¸ ë³€ìˆ˜ë“¤ ì¤‘ í•˜ë‚˜ ì œê±°

# ğŸš« ì‹¤ìˆ˜ 3: ê²°ê³¼ í•´ì„ ì˜¤ë¥˜
"RÂ² = 0.588ì€ 58.8% ì •í™•ë„" # ì˜ëª»ëœ í•´ì„

# âœ… ì˜¬ë°”ë¥¸ í•´ì„
"RÂ² = 0.588ì€ ë…ë¦½ë³€ìˆ˜ê°€ ì¢…ì†ë³€ìˆ˜ ë³€ë™ì˜ 58.8%ë¥¼ ì„¤ëª…"
```

---

## ğŸ¯ 8. ì‹¤ìŠµ: ì™„ì „í•œ OLS ë¶„ì„

### ğŸ“Š **ì¢…í•© ì‹¤ìŠµ ì½”ë“œ**

```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

# === ë°ì´í„° ì¤€ë¹„ ===
np.random.seed(42)
n = 100

df = pd.DataFrame({
    'TVê´‘ê³ ': np.random.normal(50, 15, n),
    'ë¼ë””ì˜¤ê´‘ê³ ': np.random.normal(30, 10, n), 
    'ì‹ ë¬¸ê´‘ê³ ': np.random.normal(20, 8, n)
})

# ë§¤ì¶œ = ì‹¤ì œ ê´€ê³„ + ë…¸ì´ì¦ˆ
df['ë§¤ì¶œ'] = (0.05 * df['TVê´‘ê³ '] + 
              0.12 * df['ë¼ë””ì˜¤ê´‘ê³ '] + 
              0.02 * df['ì‹ ë¬¸ê´‘ê³ '] + 
              np.random.normal(0, 2, n))

print("=== ğŸ“Š 1. ê¸°ì´ˆ í†µê³„ ===")
print(df.describe())

print("\n=== ğŸ“ˆ 2. ìƒê´€ê´€ê³„ ===")
correlation = df.corr()
print(correlation)

# === OLS íšŒê·€ë¶„ì„ ===
print("\n=== ğŸ”¥ 3. OLS íšŒê·€ë¶„ì„ ===")
y = df['ë§¤ì¶œ']
X = df[['TVê´‘ê³ ', 'ë¼ë””ì˜¤ê´‘ê³ ', 'ì‹ ë¬¸ê´‘ê³ ']]
X = sm.add_constant(X)

model = sm.OLS(y, X).fit()
print(model.summary())

# === í•µì‹¬ ê²°ê³¼ ì¶”ì¶œ ===
print("\n=== ğŸ¯ 4. í•µì‹¬ ê²°ê³¼ í•´ì„ ===")
print(f"RÂ²: {model.rsquared:.3f} ({model.rsquared*100:.1f}% ì„¤ëª…)")
print(f"Adj. RÂ²: {model.rsquared_adj:.3f}")
print(f"ì „ì²´ ëª¨ë¸ ìœ ì˜ì„±: p = {model.f_pvalue:.3f}")

print("\nğŸ“ˆ íšŒê·€ê³„ìˆ˜:")
for var, coef, pval in zip(X.columns, model.params, model.pvalues):
    sig = "***" if pval < 0.001 else "**" if pval < 0.01 else "*" if pval < 0.05 else ""
    print(f"  {var}: {coef:.4f} (p={pval:.3f}) {sig}")

# === ì˜ˆì¸¡ ===
print("\n=== ğŸ”® 5. ì˜ˆì¸¡ ì˜ˆì‹œ ===")
new_campaign = pd.DataFrame({
    'const': [1],
    'TVê´‘ê³ ': [60],
    'ë¼ë””ì˜¤ê´‘ê³ ': [35],
    'ì‹ ë¬¸ê´‘ê³ ': [25]
})

prediction = model.predict(new_campaign)
print(f"ì˜ˆìƒ ë§¤ì¶œ: {prediction[0]:.2f}")

# ì‹œê°í™”
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.scatter(df['TVê´‘ê³ '], df['ë§¤ì¶œ'])
plt.xlabel('TVê´‘ê³ ')
plt.ylabel('ë§¤ì¶œ')
plt.title('TVê´‘ê³  vs ë§¤ì¶œ')

plt.subplot(1, 3, 2)
plt.scatter(df['ë¼ë””ì˜¤ê´‘ê³ '], df['ë§¤ì¶œ'])
plt.xlabel('ë¼ë””ì˜¤ê´‘ê³ ') 
plt.ylabel('ë§¤ì¶œ')
plt.title('ë¼ë””ì˜¤ê´‘ê³  vs ë§¤ì¶œ')

plt.subplot(1, 3, 3)
plt.scatter(model.fittedvalues, model.resid)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('ì˜ˆì¸¡ê°’')
plt.ylabel('ì”ì°¨')
plt.title('ì”ì°¨ í”Œë¡¯')

plt.tight_layout()
plt.show()

print("\nâœ… OLS ë¶„ì„ ì™„ë£Œ!")
```

---

## ğŸ† ì •ë¦¬: OLS ë§ˆìŠ¤í„° í¬ì¸íŠ¸

### ğŸ¯ **ê¼­ ê¸°ì–µí•´ì•¼ í•  ê²ƒë“¤**

1. **ğŸ”§ OLS = ìµœì†Œì œê³±ë²•ì˜ Python êµ¬í˜„ì²´**
2. **ğŸ“Š statsmodelsê°€ OLSì˜ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬**  
3. **ğŸ“ˆ `sm.add_constant()` ë°˜ë“œì‹œ í•„ìš”**
4. **ğŸ¯ RÂ², F-statistic, p-value 3ëŒ€ ì§€í‘œ**
5. **ğŸ” ë‹¤ì¤‘íšŒê·€ì—ì„œëŠ” Adj. RÂ² ì‚¬ìš©**
6. **âš ï¸ ë‹¤ì¤‘ê³µì„ ì„± ì²´í¬ í•„ìˆ˜**

### ğŸš€ **ë‹¤ì¤‘ì„ í˜•íšŒê·€ ì¤€ë¹„ ì™„ë£Œ!**

```python
print("ğŸ“ ì¶•í•˜í•©ë‹ˆë‹¤!")
print("OLS ê°œë…ì„ ì™„ë²½ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤!")
print("ì´ì œ ë‹¤ì¤‘ì„ í˜•íšŒê·€ì˜ ëª¨ë“  ê²ƒì„ ì´í•´í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸš€")
```

---

## ğŸ“š ë‹¤ìŒ í•™ìŠµ ë‹¨ê³„

1. **ë‹¤ì¤‘ì„ í˜•íšŒê·€ ì‹¬í™”** - ë³€ìˆ˜ ì„ íƒ, ì •ê·œí™”
2. **íšŒê·€ ê°€ì • ê²€ì •** - ì”ì°¨ ë¶„ì„, ì´ìƒì¹˜ ì²˜ë¦¬  
3. **ê³ ê¸‰ íšŒê·€ê¸°ë²•** - Ridge, Lasso, Elastic Net
4. **ë¨¸ì‹ ëŸ¬ë‹ ì—°ê²°** - scikit-learn ì™„ì „ ë§ˆìŠ¤í„°

**"OLSë¥¼ ì •ë³µí–ˆìœ¼ë‹ˆ, ì´ì œ ì§„ì§œ íšŒê·€ë¶„ì„ì˜ ì„¸ê³„ë¡œ ë– ë‚  ì‹œê°„ì…ë‹ˆë‹¤! ğŸŒŸ"**