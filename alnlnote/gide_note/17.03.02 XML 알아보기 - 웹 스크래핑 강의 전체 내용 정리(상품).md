# 17.03.01 XML 알아보기 - 웹 스크래핑 강의 전체 내용 정리

> **📹 강의 음성 전사 내용 기반**  
> **주제**: 웹 스크래핑 기초부터 실전 교촌치킨 데이터 수집까지  
> **목표**: BeautifulSoup와 requests를 활용한 실무 웹 스크래핑 기술 습득

---

## 🔧 **고급 웹 스크래핑 도구 소개**

### **전문적인 웹 스크래핑 도구들**

```python
# 전문적인 웹 스크래핑 도구
from selenium import webdriver          # 브라우저 제어 (동적 콘텐츠)
import scrapy                           # 대규모 웹 스크래핑 프레임워크
```

**도구별 특징:**
- **Selenium**: 브라우저를 직접 제어하여 JavaScript가 실행되는 동적 웹사이트 처리
- **Scrapy**: 대규모 웹 크롤링 프로젝트에 적합한 전문 프레임워크
- **노가다**: 복잡한 사이트일수록 많은 수작업과 분석이 필요

### **브라우저 제어의 필요성**
```python
# Selenium 사용 예시 (참고용)
from selenium import webdriver

# 브라우저 제어를 통한 동적 콘텐츠 접근
driver = webdriver.Chrome()
driver.get("https://example.com")
# JavaScript 실행 후 데이터 수집 가능
```

---

## 🎯 **교촌치킨 웹 스크래핑 실전 프로젝트**

### **1. 프로젝트 목표**
- 교촌치킨 메뉴명과 가격 데이터 수집
- 가격 통계 분석 (평균, 표준편차, 사분위수)
- 상품별 다양한 분석 작업 수행

### **2. 대상 사이트 분석**
```python
# 교촌치킨 메뉴 페이지
url = "https://www.kyochon.com/menu/chicken.asp"

# ASP 기반 웹사이트 (Microsoft .NET Framework)
# - C# 또는 Visual Basic.NET 사용
# - 빌 게이츠가 선호하는 Visual Basic 기반
```

### **3. 개발자 도구를 통한 HTML 구조 분석**

**메뉴명 구조:**
```html
<dl class="txt">
    <dt>메뉴명</dt>  <!-- 여기서 메뉴명 추출 -->
</dl>
```

**가격 구조:**
```html
<p class="money">
    <strong>22,000</strong>  <!-- 여기서 가격 추출 -->
</p>
```

---

## 💻 **실제 코드 구현**

### **1. 라이브러리 import 및 기본 설정**

```python
# 웹 스크래핑용 라이브러리
from bs4 import BeautifulSoup
import urllib.request         # 연습용 (코드가 장황함)
import requests              # 실전용 (코드가 간결함, 권장!)
import pandas as pd
import numpy as np
```

### **2. 웹 페이지 요청 및 응답 처리**

```python
# 교촌치킨 메뉴 페이지 URL
url = "https://www.kyochon.com/menu/chicken.asp"

# GET 요청 보내고 응답 받기
response = requests.get(url)

# 응답 코드가 200이 아니면 예외 발생 (매우 중요!)
response.raise_for_status()

# ⚠️ 중요: 웹사이트 구조는 언제든 변경될 수 있음
# 실행 전 항상 사이트가 정상인지 확인 필요
```

### **3. HTML 파싱 및 데이터 추출**

```python
# BeautifulSoup으로 HTML 파싱
soup = BeautifulSoup(response.text, 'html.parser')

# 전체 HTML 확인 (너무 길어서 생략)
# print(soup)

# 메뉴명 추출
names = [tag.text.strip() for tag in soup.select('dl.txt > dt')]
print("메뉴명:", names)

# 가격 추출 및 숫자 변환
prices = [int(tag.text.strip().replace(',', '')) 
          for tag in soup.select('p.money strong')]
print("가격:", prices)
```

### **4. CSS 선택자 상세 설명**

```python
# CSS 선택자 문법 정리
selectors = {
    'dl.txt > dt': 'dl 태그 중 class가 txt인 요소의 직계 자식 dt',
    'p.money strong': 'p 태그 중 class가 money인 요소의 후손 strong'
}

# find_all() 방식도 가능하지만 select()가 더 직관적
# names = soup.find_all('dt')  # 이것도 가능
```

---

## ⚠️ **웹 스크래핑 주의사항 및 베스트 프랙티스**

### **1. 속도 제한 (Rate Limiting)**

```python
import time

# 서버 부하 방지를 위한 지연 시간 설정
for page in range(1, 10):
    response = requests.get(f"https://example.com/page/{page}")
    
    # 0.5초 이상 대기 (필수!)
    time.sleep(0.5)  # 너무 빠르게 요청하면 IP 차단됨
    
    # 0.01초는 너무 빠름 → 의심받아서 차단됨
```

### **2. 예외 처리 (Exception Handling)**

```python
# 네트워크 작업은 반드시 try-except 사용!
try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    
    soup = BeautifulSoup(response.text, 'html.parser')
    # 데이터 처리 로직
    
except requests.exceptions.RequestException as e:
    print(f"네트워크 오류 발생: {e}")
except Exception as e:
    print(f"예상치 못한 오류: {e}")
```

### **3. 대용량 데이터 처리 경험담**

**NASA 위성 데이터 수집 프로젝트:**
- 구름 패턴 데이터로 날씨 예측 모델 구축
- 데이터 수집에 **3-4일** 소요 (24시간 연속 실행)
- 컴퓨터를 계속 켜두고 천천히 데이터 수집
- 결과: 구름 데이터 기반 날씨 예측 시스템 완성

```python
# 장시간 데이터 수집 시 고려사항
import time
from datetime import datetime

def long_term_scraping():
    start_time = datetime.now()
    
    for i in range(10000):  # 대량 데이터 처리
        try:
            # 데이터 수집 로직
            response = requests.get(f"https://api.nasa.gov/data/{i}")
            
            # 충분한 지연 시간 (서버 부하 방지)
            time.sleep(2.0)  # 2초 대기
            
            if i % 100 == 0:
                print(f"진행률: {i/10000*100:.1f}%")
                
        except Exception as e:
            print(f"오류 발생 at index {i}: {e}")
            continue
    
    end_time = datetime.now()
    print(f"총 소요 시간: {end_time - start_time}")
```

---

## 📊 **데이터 정제 및 분석**

### **1. pandas DataFrame 생성**

```python
# 수집한 데이터를 DataFrame으로 변환
df = pd.DataFrame({
    '상품명': names,
    '가격': prices
})

print(df.head(3))
```

### **2. 기본 통계 분석**

```python
# 기본 통계 계산
print('가격 평균:', round(df['가격'].mean(), 2))
print('가격 표준편차:', round(df['가격'].std(), 2))

# f-string 사용 (더 깔끔한 출력)
print(f"가격평균: {df['가격'].mean():.2f}, 표준편차: {df['가격'].std():.2f}")

# 추가 분석 가능한 항목들
analysis_options = [
    "가장 비싼 상품",
    "가장 저렴한 상품", 
    "사분위수 분석",
    "가격 분포 히스토그램",
    "상품별 가성비 분석"
]
```

### **3. 데이터 활용 방안**

```python
# 비즈니스 인사이트 도출
insights = {
    "최저가_상품": df.loc[df['가격'].idxmin()],
    "최고가_상품": df.loc[df['가격'].idxmax()],
    "가격_중위값": df['가격'].median(),
    "사분위수": df['가격'].quantile([0.25, 0.5, 0.75])
}

# 시계열 분석 (데이터가 정기적으로 수집된다면)
# - 계절별 메뉴 가격 변동 추이
# - 신제품 출시 패턴
# - 가격 인상/인하 주기 분석
```

---

## 🛠 **HTML 구조 이해하기**

### **HTML 태그 설명**

```html
<!-- 설명 목록 (Description List) -->
<dl class="txt">          <!-- dl: Description List -->
    <dt>메뉴명</dt>        <!-- dt: Description Term -->
    <dd>설명내용</dd>      <!-- dd: Description Definition -->
</dl>

<!-- 다른 HTML 목록 태그들 -->
<ul>                       <!-- ul: Unordered List -->
    <li>항목1</li>         <!-- li: List Item -->
    <li>항목2</li>
</ul>

<ol>                       <!-- ol: Ordered List -->
    <li>첫번째</li>
    <li>두번째</li>
</ol>
```

### **CSS 선택자 심화**

```python
# 다양한 CSS 선택자 패턴
css_selectors = {
    'dl.txt > dt': '직계 자식 선택 (>)',
    'p.money strong': '후손 선택 (공백)',
    '.txt': '클래스 선택',
    '#menu': 'ID 선택',
    'p:first-child': '의사 클래스',
    'td:nth-child(2)': 'n번째 자식'
}

# BeautifulSoup에서 활용
menu_items = soup.select('dl.txt > dt')      # 직계 자식
price_items = soup.select('p.money strong')  # 후손 요소
```

---

## 🎯 **프로젝트 확장 아이디어**

### **1. 다중 페이지 스크래핑**

```python
# 여러 카테고리 메뉴 수집
categories = ['chicken', 'side', 'beverage']
all_data = []

for category in categories:
    url = f"https://www.kyochon.com/menu/{category}.asp"
    # 각 카테고리별 데이터 수집
    category_data = scrape_menu(url)
    category_data['카테고리'] = category
    all_data.append(category_data)

final_df = pd.concat(all_data, ignore_index=True)
```

### **2. 경쟁사 비교 분석**

```python
# 치킨 브랜드별 가격 비교
competitors = {
    '교촌치킨': 'https://www.kyochon.com/menu/chicken.asp',
    'BBQ': 'https://www.bbq.co.kr/menu/',
    '굽네치킨': 'https://www.goobne.co.kr/menu'
}

# 브랜드별 평균 가격 비교 차트 생성
```

### **3. 자동화 및 모니터링**

```python
import schedule

def daily_menu_check():
    """매일 메뉴 및 가격 변동 체크"""
    current_data = scrape_kyochon_menu()
    
    # 이전 데이터와 비교하여 변경사항 알림
    if price_changed(current_data):
        send_notification("가격 변동 감지!")

# 매일 오전 9시에 실행
schedule.every().day.at("09:00").do(daily_menu_check)
```

---

## 💡 **실무 팁 및 주의사항**

### **1. 법적/윤리적 고려사항**
```python
# robots.txt 확인
robots_url = "https://www.kyochon.com/robots.txt"
# 웹사이트의 스크래핑 정책 확인 필수

# 이용약관 준수
# - 개인적 용도로만 사용
# - 상업적 이용 금지
# - 서버 부하 최소화
```

### **2. 코드 유지보수성**
```python
class MenuScraper:
    def __init__(self, base_url):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
        })
    
    def scrape_menu(self, category):
        """메뉴 스크래핑 로직"""
        pass
    
    def validate_data(self, data):
        """데이터 검증 로직"""
        pass
```

### **3. 성능 최적화**
```python
# 동일한 요청 중복 방지
from functools import lru_cache

@lru_cache(maxsize=128)
def fetch_page(url):
    """페이지 캐싱으로 중복 요청 방지"""
    return requests.get(url)
```

---

## 🚀 **다음 단계: XML 데이터 처리**

강의에서 언급된 대로, 다음에는 **XML 데이터 처리**를 다룰 예정입니다.

```python
# 다음 강의 미리보기
import xml.etree.ElementTree as ET

# XML 파일 또는 API 응답 처리
# GitHub 예제: https://github.com/pykwon/python
xml_data = """
<menu>
    <item>
        <name>후라이드 치킨</name>
        <price>18000</price>
    </item>
</menu>
"""

root = ET.fromstring(xml_data)
# XML 파싱 및 pandas DataFrame 변환
```

---

## 📝 **요약**

이번 강의에서는 웹 스크래핑의 기초부터 실전 프로젝트까지 다음과 같은 내용을 학습했습니다:

1. **고급 도구 소개**: Selenium, Scrapy 등 전문적인 웹 스크래핑 도구
2. **실전 프로젝트**: 교촌치킨 메뉴 데이터 수집 및 분석
3. **주의사항**: 속도 제한, 예외 처리, 윤리적 고려사항
4. **데이터 분석**: pandas를 활용한 기본 통계 분석
5. **확장 가능성**: 다중 페이지, 경쟁사 비교, 자동화 등

**핵심 메시지**: 웹에서 데이터를 수집하여 pandas DataFrame으로 변환하면, pandas의 모든 강력한 기능을 활용할 수 있습니다! 🎯