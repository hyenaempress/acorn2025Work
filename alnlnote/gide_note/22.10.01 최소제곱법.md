# 22.10 최소제곱법 완전 가이드
*데이터에 가장 잘 맞는 직선을 찾는 수학적 마법*

---

## 🎯 최소제곱법(Least Squares Method)이란?

**최소제곱법**은 데이터와 모델(직선, 곡선 등) 사이의 **오차의 제곱합을 최소화**하여 가장 잘 맞는 모델을 찾는 통계적 방법입니다. **회귀분석과 머신러닝의 수학적 심장**입니다!

### 🔍 핵심 아이디어

데이터 포인트들 중에서 **"가장 가운데를 지나가는 직선"**을 찾는 것이 목표입니다:

```
실제 데이터: (0, -1), (1, 0.2), (2, 0.5), (3, 2.1)
목표: 이 점들에 가장 잘 맞는 y = wx + b 직선 찾기!
```

### 🎯 왜 "제곱"을 사용할까?

| 방법 | 문제점 | 해결책 |
|------|--------|--------|
| **절댓값 사용** | 미분 불가능 | ❌ |
| **단순 합** | 양수와 음수가 상쇄됨 | ❌ |
| **제곱 사용** | 항상 양수, 미분 가능 | ✅ |

> 💡 **핵심**: 제곱을 사용하면 큰 오차는 더욱 크게 패널티를 받아서, 극단적인 오차를 피할 수 있습니다!

---

## 📐 최소제곱법의 수학적 원리

### 🎯 목표 함수: 오차제곱합(SSE) 최소화

$$\text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - (wx_i + b))^2$$

여기서:
- $y_i$: 실제 값 (관측값)
- $\hat{y}_i = wx_i + b$: 예측 값 (모델 값)
- $w$: 기울기 (가중치, Weight)
- $b$: 절편 (편향, Bias)

### 🔍 최적화 과정

**1단계**: SSE를 w와 b에 대해 편미분
**2단계**: 편미분 값을 0으로 설정 (최솟값 조건)
**3단계**: 연립방정식을 풀어서 w, b 구하기

### 📊 최소제곱법 해법 (정규방정식)

행렬 형태로 표현하면:
$$\mathbf{A}\boldsymbol{\theta} = \mathbf{y}$$

여기서:
- $\mathbf{A} = \begin{bmatrix} x_1 & 1 \\ x_2 & 1 \\ \vdots & \vdots \\ x_n & 1 \end{bmatrix}$ (설계 행렬)
- $\boldsymbol{\theta} = \begin{bmatrix} w \\ b \end{bmatrix}$ (파라미터 벡터)
- $\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$ (관측값 벡터)

**최소제곱해**:
$$\boldsymbol{\theta} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{y}$$

---

## 💻 실습 1: 기본 최소제곱법 구현

### 🎯 문제 상황

4개의 데이터 포인트가 주어졌을 때, 가장 잘 맞는 직선을 찾아보겠습니다.

```python
import numpy as np
import matplotlib.pyplot as plt
import numpy.linalg as lin

plt.rc('font', family='Malgun Gothic')

print("📐 최소제곱법으로 최적의 직선 찾기")
print("=" * 60)

# 데이터 준비
x = np.array([0, 1, 2, 3])           # x 데이터
y = np.array([-1, 0.2, 0.5, 2.1])   # y 데이터

print(f"📊 주어진 데이터:")
for i in range(len(x)):
    print(f"   점 {i+1}: ({x[i]}, {y[i]})")

# 설계 행렬 A 구성: [x, 1] 형태
A = np.vstack([x, np.ones(len(x))]).T
print(f"\n🔧 설계 행렬 A:")
print(A)
print(f"   → 각 행: [x_i, 1] (기울기와 절편을 위한 구조)")

# 최소제곱해 계산
theta = lin.lstsq(A, y, rcond=None)[0]
w, b = theta

print(f"\n⚡ 최소제곱법 결과:")
print(f"   기울기 (w): {w:.5f}")
print(f"   절편 (b): {b:.5f}")
print(f"   최적 직선: y = {w:.5f}x + {b:.5f}")

# 예측값 계산
y_pred = w * x + b
print(f"\n📈 예측값과 실제값 비교:")
for i in range(len(x)):
    residual = y[i] - y_pred[i]
    print(f"   x={x[i]}: 실제={y[i]:6.2f}, 예측={y_pred[i]:6.2f}, 잔차={residual:6.3f}")

# SSE 계산
sse = np.sum((y - y_pred) ** 2)
print(f"\n🎯 오차제곱합 (SSE): {sse:.6f}")
print(f"   → 이 값이 최소가 되도록 w, b를 구한 것!")

# 시각화
plt.figure(figsize=(12, 8))

# 1. 메인 플롯
plt.subplot(2, 2, 1)
plt.scatter(x, y, color='red', s=100, zorder=5, label='실제 데이터')
plt.plot(x, y_pred, 'b-', linewidth=2, label=f'최적 직선: y={w:.3f}x+{b:.3f}')

# 잔차 선 그리기
for i in range(len(x)):
    plt.plot([x[i], x[i]], [y[i], y_pred[i]], 'g--', alpha=0.7, linewidth=1)

plt.xlabel('x')
plt.ylabel('y')
plt.title('최소제곱법으로 구한 최적 직선')
plt.legend()
plt.grid(True, alpha=0.3)

# 2. 잔차 플롯
plt.subplot(2, 2, 2)
residuals = y - y_pred
plt.bar(range(len(x)), residuals, alpha=0.7, color=['red' if r < 0 else 'blue' for r in residuals])
plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)
plt.xlabel('데이터 포인트')
plt.ylabel('잔차 (실제값 - 예측값)')
plt.title('잔차 분포')
plt.grid(True, alpha=0.3)

# 3. 제곱 오차 시각화
plt.subplot(2, 2, 3)
squared_errors = (y - y_pred) ** 2
plt.bar(range(len(x)), squared_errors, alpha=0.7, color='orange')
plt.xlabel('데이터 포인트')
plt.ylabel('제곱 오차')
plt.title(f'제곱 오차 (총합: {sse:.3f})')
plt.grid(True, alpha=0.3)

# 4. 다양한 직선과 SSE 비교
plt.subplot(2, 2, 4)
w_range = np.linspace(0.5, 1.5, 50)
sse_values = []

for w_test in w_range:
    y_test = w_test * x + b
    sse_test = np.sum((y - y_test) ** 2)
    sse_values.append(sse_test)

plt.plot(w_range, sse_values, 'purple', linewidth=2)
plt.axvline(x=w, color='red', linestyle='--', label=f'최적 w={w:.3f}')
plt.axhline(y=sse, color='red', linestyle='--', alpha=0.5)
plt.xlabel('기울기 (w)')
plt.ylabel('SSE')
plt.title('기울기에 따른 SSE 변화')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 🧠 실습 2: 최소제곱법의 기하학적 의미

### 🎯 다양한 직선과 오차 비교

```python
print("🧠 여러 직선의 오차 비교: 왜 최소제곱법이 최적인가?")
print("=" * 70)

# 동일한 데이터 사용
x = np.array([0, 1, 2, 3])
y = np.array([-1, 0.2, 0.5, 2.1])

# 최소제곱해
A = np.vstack([x, np.ones(len(x))]).T
w_optimal, b_optimal = lin.lstsq(A, y, rcond=None)[0]

# 다양한 직선들 테스트
test_lines = [
    {"name": "임의 직선 1", "w": 0.5, "b": -0.5, "color": "orange"},
    {"name": "임의 직선 2", "w": 1.2, "b": -1.5, "color": "green"},
    {"name": "최소제곱해", "w": w_optimal, "b": b_optimal, "color": "red"},
    {"name": "임의 직선 3", "w": 0.8, "b": -0.2, "color": "purple"}
]

print("📊 각 직선의 오차 비교:")
print(f"{'직선':<12} {'기울기':<8} {'절편':<8} {'SSE':<10} {'RMSE':<8}")
print("-" * 50)

plt.figure(figsize=(15, 10))

# 1. 모든 직선 비교
plt.subplot(2, 3, 1)
plt.scatter(x, y, color='black', s=100, zorder=5, label='실제 데이터')

for line in test_lines:
    y_pred = line["w"] * x + line["b"]
    sse = np.sum((y - y_pred) ** 2)
    rmse = np.sqrt(sse / len(y))
    
    plt.plot(x, y_pred, color=line["color"], linewidth=2, 
             label=f'{line["name"]}: SSE={sse:.3f}')
    
    print(f'{line["name"]:<12} {line["w"]:<8.3f} {line["b"]:<8.3f} {sse:<10.3f} {rmse:<8.3f}')

plt.xlabel('x')
plt.ylabel('y')
plt.title('여러 직선의 데이터 적합도 비교')
plt.legend()
plt.grid(True, alpha=0.3)

# 2-5. 각 직선별 상세 분석
for i, line in enumerate(test_lines):
    plt.subplot(2, 3, i+2)
    
    y_pred = line["w"] * x + line["b"]
    residuals = y - y_pred
    sse = np.sum(residuals ** 2)
    
    # 데이터와 직선
    plt.scatter(x, y, color='black', s=100, zorder=5)
    plt.plot(x, y_pred, color=line["color"], linewidth=2)
    
    # 잔차 선
    for j in range(len(x)):
        plt.plot([x[j], x[j]], [y[j], y_pred[j]], 'gray', linestyle='--', alpha=0.7)
        # 제곱 영역 표시 (작은 사각형으로)
        if residuals[j] != 0:
            square_size = abs(residuals[j]) * 0.1
            rect = plt.Rectangle((x[j] - square_size/2, min(y[j], y_pred[j])), 
                               square_size, abs(residuals[j]), 
                               alpha=0.3, color=line["color"])
            plt.gca().add_patch(rect)
    
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title(f'{line["name"]}\nSSE = {sse:.3f}')
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 최적성 확인
print(f"\n🎯 결론:")
print(f"   최소제곱해가 가장 작은 SSE를 가짐을 확인!")
print(f"   최소제곱법 = 수학적으로 증명된 최적해")
```

---

## 📊 실습 3: 실제 데이터에 최소제곱법 적용

### 🎯 키와 몸무게 관계 분석

```python
print("📊 실제 데이터 분석: 키와 몸무게의 관계")
print("=" * 60)

# 실제 데이터 생성 (키 → 몸무게 관계)
np.random.seed(42)
n_people = 20

# 키 데이터 (cm)
height = np.random.normal(170, 10, n_people)  # 평균 170cm, 표준편차 10cm
height = np.round(height, 1)

# 몸무게 데이터 (실제 관계 + 노이즈)
# 실제 관계: 몸무게 = 키 * 0.7 - 50 + 노이즈
weight_true = height * 0.7 - 50
noise = np.random.normal(0, 5, n_people)  # 노이즈
weight = weight_true + noise
weight = np.round(weight, 1)

print(f"📊 데이터 샘플 (총 {n_people}명):")
print(f"{'번호':<4} {'키(cm)':<8} {'몸무게(kg)':<10}")
print("-" * 25)
for i in range(min(10, n_people)):
    print(f"{i+1:<4} {height[i]:<8.1f} {weight[i]:<10.1f}")
if n_people > 10:
    print("...")

# 기초 통계량
print(f"\n📈 기초 통계량:")
print(f"   키 평균: {np.mean(height):.1f}cm, 표준편차: {np.std(height, ddof=1):.1f}cm")
print(f"   몸무게 평균: {np.mean(weight):.1f}kg, 표준편차: {np.std(weight, ddof=1):.1f}kg")
print(f"   상관계수: {np.corrcoef(height, weight)[0,1]:.4f}")

# 최소제곱법 적용
A = np.vstack([height, np.ones(len(height))]).T
w, b = lin.lstsq(A, weight, rcond=None)[0]

print(f"\n⚡ 최소제곱법 결과:")
print(f"   회귀식: 몸무게 = {w:.4f} × 키 + {b:.4f}")
print(f"   해석: 키가 1cm 증가할 때마다 몸무게는 {w:.3f}kg 증가")

# 모델 성능 평가
weight_pred = w * height + b
residuals = weight - weight_pred
sse = np.sum(residuals ** 2)
mse = sse / len(weight)
rmse = np.sqrt(mse)
r_squared = 1 - (sse / np.sum((weight - np.mean(weight)) ** 2))

print(f"\n📊 모델 성능 지표:")
print(f"   SSE (오차제곱합): {sse:.3f}")
print(f"   MSE (평균제곱오차): {mse:.3f}")
print(f"   RMSE (평균제곱근오차): {rmse:.3f}kg")
print(f"   R² (결정계수): {r_squared:.4f} ({r_squared*100:.1f}% 설명력)")

# 예측 기능
print(f"\n🔮 예측 기능:")
test_heights = [160, 165, 170, 175, 180, 185]
print(f"{'키(cm)':<8} {'예측 몸무게(kg)':<15}")
print("-" * 25)
for h in test_heights:
    pred_w = w * h + b
    print(f"{h:<8} {pred_w:<15.1f}")

# 시각화
plt.figure(figsize=(15, 10))

# 1. 산점도와 회귀선
plt.subplot(2, 3, 1)
plt.scatter(height, weight, alpha=0.7, s=80, color='blue', edgecolors='black')
plt.plot(height, weight_pred, 'r-', linewidth=2, 
         label=f'회귀선: y={w:.3f}x+{b:.1f}')
plt.xlabel('키 (cm)')
plt.ylabel('몸무게 (kg)')
plt.title('키와 몸무게의 관계')
plt.legend()
plt.grid(True, alpha=0.3)

# 2. 잔차 플롯
plt.subplot(2, 3, 2)
plt.scatter(weight_pred, residuals, alpha=0.7, s=80, color='green')
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('예측값')
plt.ylabel('잔차')
plt.title('잔차 플롯 (잔차 = 실제 - 예측)')
plt.grid(True, alpha=0.3)

# 3. 잔차 히스토그램
plt.subplot(2, 3, 3)
plt.hist(residuals, bins=10, alpha=0.7, color='orange', edgecolor='black')
plt.axvline(x=0, color='red', linestyle='--')
plt.xlabel('잔차')
plt.ylabel('빈도')
plt.title(f'잔차 분포 (평균: {np.mean(residuals):.3f})')
plt.grid(True, alpha=0.3)

# 4. Q-Q 플롯 (정규성 확인)
plt.subplot(2, 3, 4)
from scipy import stats
stats.probplot(residuals, dist="norm", plot=plt)
plt.title('Q-Q 플롯 (정규성 확인)')
plt.grid(True, alpha=0.3)

# 5. 실제 vs 예측 플롯
plt.subplot(2, 3, 5)
plt.scatter(weight, weight_pred, alpha=0.7, s=80, color='purple')
# 완벽한 예측선 (y=x)
min_val = min(min(weight), min(weight_pred))
max_val = max(max(weight), max(weight_pred))
plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='완벽한 예측선')
plt.xlabel('실제값')
plt.ylabel('예측값')
plt.title('실제값 vs 예측값')
plt.legend()
plt.grid(True, alpha=0.3)

# 6. 모델 성능 요약
plt.subplot(2, 3, 6)
metrics = ['RMSE', 'R²']
values = [rmse, r_squared]
colors = ['lightcoral', 'lightgreen']

bars = plt.bar(metrics, values, color=colors, alpha=0.7, edgecolor='black')
plt.ylabel('값')
plt.title('모델 성능 지표')

# 값을 막대 위에 표시
for bar, value in zip(bars, values):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + max(values)*0.01,
             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 🔬 실습 4: 최소제곱법의 한계와 대안

### ⚠️ 이상치에 대한 민감성

```python
print("⚠️ 최소제곱법의 한계: 이상치 민감성")
print("=" * 60)

# 기본 데이터
x_clean = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
y_clean = 2 * x_clean + 3 + np.random.normal(0, 1, 10)  # y = 2x + 3 + 노이즈

# 이상치 추가
x_outlier = np.append(x_clean, [5.5])
y_outlier = np.append(y_clean, [30])  # 심각한 이상치

print("📊 이상치 영향 분석:")

# 깨끗한 데이터에 대한 회귀
A_clean = np.vstack([x_clean, np.ones(len(x_clean))]).T
w_clean, b_clean = lin.lstsq(A_clean, y_clean, rcond=None)[0]

# 이상치 포함 데이터에 대한 회귀
A_outlier = np.vstack([x_outlier, np.ones(len(x_outlier))]).T
w_outlier, b_outlier = lin.lstsq(A_outlier, y_outlier, rcond=None)[0]

print(f"   깨끗한 데이터: y = {w_clean:.3f}x + {b_clean:.3f}")
print(f"   이상치 포함: y = {w_outlier:.3f}x + {b_outlier:.3f}")
print(f"   기울기 변화: {((w_outlier - w_clean) / w_clean * 100):+.1f}%")
print(f"   절편 변화: {((b_outlier - b_clean) / abs(b_clean) * 100):+.1f}%")

# 시각화
plt.figure(figsize=(15, 5))

# 1. 깨끗한 데이터
plt.subplot(1, 3, 1)
plt.scatter(x_clean, y_clean, color='blue', s=80, alpha=0.7)
y_pred_clean = w_clean * x_clean + b_clean
plt.plot(x_clean, y_pred_clean, 'r-', linewidth=2, 
         label=f'y={w_clean:.2f}x+{b_clean:.2f}')
plt.xlabel('x')
plt.ylabel('y')
plt.title('깨끗한 데이터')
plt.legend()
plt.grid(True, alpha=0.3)

# 2. 이상치 포함
plt.subplot(1, 3, 2)
plt.scatter(x_clean, y_clean, color='blue', s=80, alpha=0.7, label='정상 데이터')
plt.scatter([5.5], [30], color='red', s=150, marker='x', linewidth=3, label='이상치')
y_pred_outlier = w_outlier * x_outlier + b_outlier
plt.plot(x_outlier, y_pred_outlier, 'g-', linewidth=2, 
         label=f'y={w_outlier:.2f}x+{b_outlier:.2f}')
plt.xlabel('x')
plt.ylabel('y')
plt.title('이상치 포함 데이터')
plt.legend()
plt.grid(True, alpha=0.3)

# 3. 비교
plt.subplot(1, 3, 3)
plt.scatter(x_clean, y_clean, color='blue', s=80, alpha=0.7, label='정상 데이터')
plt.scatter([5.5], [30], color='red', s=150, marker='x', linewidth=3, label='이상치')

x_range = np.linspace(1, 10, 100)
y_clean_line = w_clean * x_range + b_clean
y_outlier_line = w_outlier * x_range + b_outlier

plt.plot(x_range, y_clean_line, 'r-', linewidth=2, label='깨끗한 데이터 회귀선')
plt.plot(x_range, y_outlier_line, 'g--', linewidth=2, label='이상치 포함 회귀선')
plt.xlabel('x')
plt.ylabel('y')
plt.title('회귀선 비교')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\n💡 해결책들:")
print(f"   1. 이상치 탐지 및 제거")
print(f"   2. 로버스트 회귀 (Robust Regression)")
print(f"   3. RANSAC (Random Sample Consensus)")
print(f"   4. 휴버 회귀 (Huber Regression)")
```

### 🔄 다양한 회귀 방법 비교

```python
from sklearn.linear_model import LinearRegression, HuberRegressor, RANSACRegressor
from sklearn.metrics import mean_squared_error, r2_score

print("\n🔄 다양한 회귀 방법 비교")
print("=" * 50)

# 이상치가 있는 데이터 사용
X = x_outlier.reshape(-1, 1)
y = y_outlier

# 다양한 회귀 모델
models = {
    "최소제곱법": LinearRegression(),
    "Huber 회귀": HuberRegressor(),
    "RANSAC": RANSACRegressor(random_state=42)
}

results = {}
plt.figure(figsize=(15, 5))

for i, (name, model) in enumerate(models.items()):
    # 모델 학습
    model.fit(X, y)
    y_pred = model.predict(X)
    
    # 성능 평가
    mse = mean_squared_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    
    results[name] = {"MSE": mse, "R²": r2}
    
    # 시각화
    plt.subplot(1, 3, i+1)
    plt.scatter(x_clean, y_clean, color='blue', s=80, alpha=0.7, label='정상 데이터')
    plt.scatter([5.5], [30], color='red', s=150, marker='x', linewidth=3, label='이상치')
    
    x_range = np.linspace(1, 10, 100).reshape(-1, 1)
    y_range_pred = model.predict(x_range)
    plt.plot(x_range, y_range_pred, 'g-', linewidth=2, label=f'{name} 회귀선')
    
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title(f'{name}\nMSE={mse:.2f}, R²={r2:.3f}')
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 결과 비교
print(f"📊 모델 성능 비교:")
print(f"{'모델':<12} {'MSE':<8} {'R²':<8}")
print("-" * 30)
for name, metrics in results.items():
    print(f"{name:<12} {metrics['MSE']:<8.2f} {metrics['R²']:<8.3f}")
```

---

## 🎯 실무 활용: 예측 시스템 구축

### 🏠 부동산 가격 예측 모델

```python
print("🏠 실무 프로젝트: 부동산 가격 예측 시스템")
print("=" * 60)

# 실제 부동산 데이터 시뮬레이션
np.random.seed(42)
n_houses = 100

# 집 크기 (평)
size = np.random.uniform(20, 80, n_houses)

# 실제 관계: 가격 = 크기 * 1000만원 + 기본가격 + 노이즈
# 기본가격: 2억원, 평당 1000만원
base_price = 2  # 억원
price_per_pyeong = 1000  # 만원
price = (size * price_per_pyeong + base_price * 10000) / 10000  # 억원 단위
price += np.random.normal(0, 0.5, n_houses)  # 노이즈 (0.5억원 표준편차)

print(f"📊 부동산 데이터 (총 {n_houses}건):")
print(f"{'번호':<4} {'크기(평)':<10} {'가격(억원)':<12}")
print("-" * 30)
for i in range(10):
    print(f"{i+1:<4} {size[i]:<10.1f} {price[i]:<12.2f}")
print("...")

# 최소제곱법으로 예측 모델 구축
A = np.vstack([size, np.ones(len(size))]).T
w, b = lin.lstsq(A, price, rcond=None)[0]

print(f"\n⚡ 예측 모델:")
print(f"   가격(억원) = {w:.4f} × 크기(평) + {b:.4f}")
print(f"   해석: 1평 증가당 {w*10000:.0f}만원 가격 상승")
print(f"   기본 가격: {b:.2f}억원")

# 모델 검증
price_pred = w * size + b
residuals = price - price_pred
rmse = np.sqrt(np.mean(residuals ** 2))
r_squared = 1 - (np.sum(residuals ** 2) / np.sum((price - np.mean(price)) ** 2))

print(f"\n📊 모델 성능:")
print(f"   RMSE: {rmse:.3f}억원 (={rmse*10000:.0f}만원)")
print(f"   R²: {r_squared:.4f} ({r_squared*100:.1f}% 설명력)")

# 실무 예측 기능
print(f"\n🔮 실무 예측 시나리오:")
test_sizes = [25, 35, 45, 55, 65]
print(f"{'크기(평)':<10} {'예측가격(억원)':<15} {'예측범위(억원)':<20}")
print("-" * 50)

for test_size in test_sizes:
    pred_price = w * test_size + b
    # 95% 예측구간 (대략적으로 ±2*RMSE)
    lower_bound = pred_price - 2 * rmse
    upper_bound = pred_price + 2 * rmse
    
    print(f"{test_size:<10.0f} {pred_price:<15.2f} {lower_bound:.2f} ~ {upper_bound:.2f}")

# 시각화
plt.figure(figsize=(15, 10))

# 1. 기본 산점도와 회귀선
plt.subplot(2, 3, 1)
plt.scatter(size, price, alpha=0.6, s=60, color='blue')
plt.plot(size, price_pred, 'r-', linewidth=2, 
         label=f'예측 모델: y={w:.3f}x+{b:.2f}')

# 예측구간 표시
size_sorted = np.sort(size)
pred_sorted = w * size_sorted + b
upper_bound = pred_sorted + 2 * rmse
lower_bound = pred_sorted - 2 * rmse

plt.fill_between(size_sorted, lower_bound, upper_bound, alpha=0.2, color='red', 
                label='95% 예측구간')
plt.xlabel('크기 (평)')
plt.ylabel('가격 (억원)')
plt.title('부동산 가격 예측 모델')
plt.legend()
plt.grid(True, alpha=0.3)

# 2. 잔차 분석
plt.subplot(2, 3, 2)
plt.scatter(price_pred, residuals, alpha=0.6, s=60, color='green')
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('예측 가격 (억원)')
plt.ylabel('잔차 (실제 - 예측)')
plt.title('잔차 플롯')
plt.grid(True, alpha=0.3)

# 3. 가격대별 오차 분석
plt.subplot(2, 3, 3)
price_bins = np.linspace(price.min(), price.max(), 5)
bin_centers = []
bin_rmses = []

for i in range(len(price_bins)-1):
    mask = (price >= price_bins[i]) & (price < price_bins[i+1])
    if np.sum(mask) > 0:
        bin_rmse = np.sqrt(np.mean(residuals[mask] ** 2))
        bin_centers.append((price_bins[i] + price_bins[i+1]) / 2)
        bin_rmses.append(bin_rmse)

plt.bar(bin_centers, bin_rmses, width=(price_bins[1] - price_bins[0]) * 0.8, 
        alpha=0.7, color='orange')
plt.xlabel('가격대 (억원)')
plt.ylabel('RMSE (억원)')
plt.title('가격대별 예측 오차')
plt.grid(True, alpha=0.3)

# 4. 크기대별 데이터 분포
plt.subplot(2, 3, 4)
plt.hist(size, bins=20, alpha=0.7, color='lightblue', edgecolor='black')
plt.xlabel('크기 (평)')
plt.ylabel('빈도')
plt.title('크기 분포')
plt.grid(True, alpha=0.3)

# 5. 가격 분포
plt.subplot(2, 3, 5)
plt.hist(price, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
plt.xlabel('가격 (억원)')
plt.ylabel('빈도')
plt.title('가격 분포')
plt.grid(True, alpha=0.3)

# 6. 모델 성능 요약
plt.subplot(2, 3, 6)
metrics = ['RMSE\n(억원)', 'R²']
values = [rmse, r_squared]
colors = ['salmon', 'lightgreen']

bars = plt.bar(metrics, values, color=colors, alpha=0.7, edgecolor='black')
plt.ylabel('값')
plt.title('모델 성능 요약')

for bar, value in zip(bars, values):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + max(values)*0.01,
             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

---

## 🎓 핵심 정리 및 다음 단계

### ✅ **최소제곱법 마스터 체크리스트**

```python
def 최소제곱법_체크리스트():
    """최소제곱법 완전 이해 체크리스트"""
    
    체크리스트 = {
        "✅ 이론적 이해": [
            "오차제곱합 최소화 원리",
            "정규방정식 유도 과정",
            "기하학적 의미 이해",
            "최적성의 수학적 증명"
        ],
        "✅ 실무 구현": [
            "Python으로 최소제곱해 계산",
            "잔차 분석 및 모델 검증",
            "예측구간과 신뢰구간 계산",
            "다양한 회귀 방법 비교"
        ],
        "✅ 한계와 대안": [
            "이상치에 대한 민감성 이해",
            "로버스트 회귀 방법 활용",
            "가정 위반 시 대처방안",
            "비선형 관계 처리 방법"
        ],
        "✅ 실무 활용": [
            "예측 모델 구축 프로세스",
            "모델 성능 평가 지표",
            "비즈니스 인사이트 도출",
            "회귀분석으로의 확장 준비"
        ]
    }
    
    return 체크리스트

# 체크리스트 출력
완료항목 = 최소제곱법_체크리스트()
for 영역, 항목들 in 완료항목.items():
    print(f"\n{영역}:")
    for 항목 in 항목들:
        print(f"   • {항목}")
```

### 🚀 **다음 학습 단계: 회귀분석**

```python
def 회귀분석_preview():
    """22.11 회귀분석 미리보기"""
    
    preview = {
        "22.11 학습목표": "완전한 예측 모델 구축",
        "핵심질문": "여러 변수로 더 정확한 예측이 가능한가?",
        "발전과정": "최소제곱법 → 단순회귀 → 다중회귀",
        "학습내용": [
            "단순 선형 회귀의 모든 것",
            "다중 선형 회귀 확장",
            "회귀 가정과 진단",
            "변수 선택과 모델 개선",
            "실무 회귀분석 프로젝트"
        ],
        "최종목표": "머신러닝과의 완벽한 연결!"
    }
    
    return preview

다음단계 = 회귀분석_preview()
print("🔮 다음 학습 미리보기:")
for 항목, 내용 in 다음단계.items():
    if isinstance(내용, list):
        print(f"{항목}:")
        for 세부 in 내용:
            print(f"   • {세부}")
    else:
        print(f"{항목}: {내용}")
```

### 🔗 **전체 학습 여정**

```python
def 전체학습여정_요약():
    """지금까지의 완성된 학습 여정"""
    
    여정 = {
        "22.07 ANOVA": "평균 차이 검정 ✅",
        "22.08 비율검정": "비율 차이 검정 ✅", 
        "22.09 상관분석": "변수 간 관계 분석 ✅",
        "22.10 최소제곱법": "최적 직선 찾기 ✅",
        "22.11 회귀분석": "예측 모델 구축 🎯",
        "머신러닝": "AI 모델링 🚀"
    }
    
    print("🗺️ 전체 학습 여정:")
    for 단계, 내용 in 여정.items():
        print(f"   {단계}: {내용}")
    
    print(f"\n🎯 현재 위치: 수학적 기초 완성!")
    print(f"🚀 다음 목표: 실무 예측 모델 마스터!")

전체학습여정_요약()
```

---

## 💡 최종 메시지

### 🏆 **축하합니다!**

**최소제곱법의 모든 것**을 완전히 마스터하셨습니다! 🎉

#### **🎯 핵심 성과**
- ✅ **수학적 원리**: 오차제곱합 최소화 완벽 이해
- ✅ **실무 구현**: Python으로 예측 모델 구축 완성
- ✅ **한계 인식**: 이상치 문제와 로버스트 대안 마스터
- ✅ **프로젝트**: 부동산 가격 예측 시스템까지 완성

#### **🔗 지금까지의 놀라운 여정**

```
ANOVA → 비율검정 → 상관분석 → 최소제곱법 → 다음: 회귀분석
(검정)   (검정)     (관계분석)   (수학기초)    (예측모델)
```

#### **🚀 다음 목표: 회귀분석**

이제 **수학적 도구**(최소제곱법)를 **통계적 모델링**(회귀분석)으로 발전시킬 때입니다:

**지금**: `y = 0.96x - 0.99` (단순한 직선)
**다음**: `R², p-value, 신뢰구간을 갖춘 완전한 통계 모델`

#### **💫 머신러닝으로의 다리**

최소제곱법은 **모든 머신러닝 알고리즘의 DNA**입니다:
- 선형회귀 → 로지스틱회귀 → 신경망 → 딥러닝

**"데이터에서 패턴을 찾는 모든 여정의 수학적 심장을 마스터했습니다. 이제 진짜 예측의 세계로 들어갑시다!"** 🔮✨