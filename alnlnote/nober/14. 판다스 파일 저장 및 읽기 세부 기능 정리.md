# 14. 판다스 파일 저장 및 읽기 세부 기능 정리

## 📋 개요

판다스(Pandas)는 다양한 파일 형식의 데이터를 읽고 저장할 수 있는 강력한 I/O 기능을 제공합니다. 이 가이드는 실무에서 자주 사용되는 파일 입출력 기능들을 체계적으로 정리하고, 실제 코드 예제와 함께 설명합니다.

## 🔧 주요 파일 형식 지원

| 형식 | 읽기 함수 | 저장 메서드 | 특징 |
|------|----------|-------------|------|
| CSV | `pd.read_csv()` | `df.to_csv()` | 범용적, 경량 |
| Excel | `pd.read_excel()` | `df.to_excel()` | 다중 시트 지원 |
| JSON | `pd.read_json()` | `df.to_json()` | 웹 API 호환 |
| HTML | `pd.read_html()` | `df.to_html()` | 웹 스크래핑 |
| 텍스트 | `pd.read_table()` | - | 구분자 지정 |
| FWF | `pd.read_fwf()` | - | 고정 폭 파일 |

---

## 📂 CSV 파일 처리

### 기본 CSV 읽기 및 저장

```python
import pandas as pd
import numpy as np

# 기본 CSV 파일 읽기
df = pd.read_csv('data.csv')
print(df.head())

# 구분자 지정하여 읽기
df = pd.read_csv('data.csv', sep=',')  # 기본값
df = pd.read_csv('data.txt', sep='\s+')  # 공백으로 구분

# 기본 CSV 저장
df.to_csv('output.csv', index=False)  # 인덱스 제외하고 저장
```

### 고급 CSV 옵션

```python
# 헤더 처리
df = pd.read_csv('data.csv', header=None)  # 첫 줄을 헤더로 사용하지 않음
df = pd.read_csv('data.csv', names=['A', 'B', 'C'])  # 사용자 정의 컬럼명

# 특정 행 건너뛰기
df = pd.read_csv('data.csv', skiprows=1)  # 첫 번째 행 건너뛰기
df = pd.read_csv('data.csv', skiprows=[0, 2])  # 0번째, 2번째 행 건너뛰기

# 인코딩 처리
df = pd.read_csv('korean_data.csv', encoding='utf-8-sig')  # 한글 파일
df = pd.read_csv('data.csv', encoding='cp949')  # 윈도우 한글

# 결측값 처리
df = pd.read_csv('data.csv', na_values=['N/A', 'NULL', '-'])

# 데이터 타입 지정
dtype_dict = {'id': str, 'score': int}
df = pd.read_csv('data.csv', dtype=dtype_dict)
```

### 정규표현식을 이용한 구분자 처리

```python
# 공백으로 구분된 파일 읽기 (정규표현식 사용)
df = pd.read_table('data.txt', sep=r'\s+')

# 여러 종류의 구분자 처리
df = pd.read_csv('data.csv', sep=r'[,;|\t]', engine='python')
```

**💡 정규표현식 패턴**
- `\s+`: 하나 이상의 공백 문자
- `[,;|\t]`: 쉼표, 세미콜론, 파이프, 탭 중 하나
- 복잡한 패턴 처리 시 `engine='python'` 필요

---

## 📝 정규표현식(Regular Expression) 완전 가이드

### 기본 메타문자

| 패턴 | 설명 | 예시 | 매칭 결과 |
|------|------|------|-----------|
| `.` | 임의의 한 문자 (줄바꿈 제외) | `a.c` | abc, aac, a1c |
| `^` | 문자열의 시작 | `^abc` | abc로 시작하는 문자열 |
| `# 14. 판다스 파일 저장 및 읽기 세부 기능 정리

## 📋 개요

판다스(Pandas)는 다양한 파일 형식의 데이터를 읽고 저장할 수 있는 강력한 I/O 기능을 제공합니다. 이 가이드는 실무에서 자주 사용되는 파일 입출력 기능들을 체계적으로 정리하고, 실제 코드 예제와 함께 설명합니다.

## 🔧 주요 파일 형식 지원

| 형식 | 읽기 함수 | 저장 메서드 | 특징 |
|------|----------|-------------|------|
| CSV | `pd.read_csv()` | `df.to_csv()` | 범용적, 경량 |
| Excel | `pd.read_excel()` | `df.to_excel()` | 다중 시트 지원 |
| JSON | `pd.read_json()` | `df.to_json()` | 웹 API 호환 |
| HTML | `pd.read_html()` | `df.to_html()` | 웹 스크래핑 |
| 텍스트 | `pd.read_table()` | - | 구분자 지정 |
| FWF | `pd.read_fwf()` | - | 고정 폭 파일 |

---

## 📂 CSV 파일 처리

### 기본 CSV 읽기 및 저장

```python
import pandas as pd
import numpy as np

# 기본 CSV 파일 읽기
df = pd.read_csv('data.csv')
print(df.head())

# 구분자 지정하여 읽기
df = pd.read_csv('data.csv', sep=',')  # 기본값
df = pd.read_csv('data.txt', sep='\s+')  # 공백으로 구분

# 기본 CSV 저장
df.to_csv('output.csv', index=False)  # 인덱스 제외하고 저장
```

### 고급 CSV 옵션

```python
# 헤더 처리
df = pd.read_csv('data.csv', header=None)  # 첫 줄을 헤더로 사용하지 않음
df = pd.read_csv('data.csv', names=['A', 'B', 'C'])  # 사용자 정의 컬럼명

# 특정 행 건너뛰기
df = pd.read_csv('data.csv', skiprows=1)  # 첫 번째 행 건너뛰기
df = pd.read_csv('data.csv', skiprows=[0, 2])  # 0번째, 2번째 행 건너뛰기

# 인코딩 처리
df = pd.read_csv('korean_data.csv', encoding='utf-8-sig')  # 한글 파일
df = pd.read_csv('data.csv', encoding='cp949')  # 윈도우 한글

# 결측값 처리
df = pd.read_csv('data.csv', na_values=['N/A', 'NULL', '-'])

# 데이터 타입 지정
dtype_dict = {'id': str, 'score': int}
df = pd.read_csv('data.csv', dtype=dtype_dict)
```

 | 문자열의 끝 | `abc# 14. 판다스 파일 저장 및 읽기 세부 기능 정리

## 📋 개요

판다스(Pandas)는 다양한 파일 형식의 데이터를 읽고 저장할 수 있는 강력한 I/O 기능을 제공합니다. 이 가이드는 실무에서 자주 사용되는 파일 입출력 기능들을 체계적으로 정리하고, 실제 코드 예제와 함께 설명합니다.

## 🔧 주요 파일 형식 지원

| 형식 | 읽기 함수 | 저장 메서드 | 특징 |
|------|----------|-------------|------|
| CSV | `pd.read_csv()` | `df.to_csv()` | 범용적, 경량 |
| Excel | `pd.read_excel()` | `df.to_excel()` | 다중 시트 지원 |
| JSON | `pd.read_json()` | `df.to_json()` | 웹 API 호환 |
| HTML | `pd.read_html()` | `df.to_html()` | 웹 스크래핑 |
| 텍스트 | `pd.read_table()` | - | 구분자 지정 |
| FWF | `pd.read_fwf()` | - | 고정 폭 파일 |

---

## 📂 CSV 파일 처리

### 기본 CSV 읽기 및 저장

```python
import pandas as pd
import numpy as np

# 기본 CSV 파일 읽기
df = pd.read_csv('data.csv')
print(df.head())

# 구분자 지정하여 읽기
df = pd.read_csv('data.csv', sep=',')  # 기본값
df = pd.read_csv('data.txt', sep='\s+')  # 공백으로 구분

# 기본 CSV 저장
df.to_csv('output.csv', index=False)  # 인덱스 제외하고 저장
```

### 고급 CSV 옵션

```python
# 헤더 처리
df = pd.read_csv('data.csv', header=None)  # 첫 줄을 헤더로 사용하지 않음
df = pd.read_csv('data.csv', names=['A', 'B', 'C'])  # 사용자 정의 컬럼명

# 특정 행 건너뛰기
df = pd.read_csv('data.csv', skiprows=1)  # 첫 번째 행 건너뛰기
df = pd.read_csv('data.csv', skiprows=[0, 2])  # 0번째, 2번째 행 건너뛰기

# 인코딩 처리
df = pd.read_csv('korean_data.csv', encoding='utf-8-sig')  # 한글 파일
df = pd.read_csv('data.csv', encoding='cp949')  # 윈도우 한글

# 결측값 처리
df = pd.read_csv('data.csv', na_values=['N/A', 'NULL', '-'])

# 데이터 타입 지정
dtype_dict = {'id': str, 'score': int}
df = pd.read_csv('data.csv', dtype=dtype_dict)
```

 | abc로 끝나는 문자열 |
| `*` | 0회 이상 반복 | `ab*c` | ac, abc, abbc, abbbc |
| `+` | 1회 이상 반복 | `ab+c` | abc, abbc, abbbc |
| `?` | 0회 또는 1회 | `ab?c` | ac, abc |
| `\` | 이스케이프 문자 | `a\.c` | a.c (점 문자 그대로) |

### 문자 클래스

```python
import re
import pandas as pd

# 문자 클래스 패턴 예시
patterns = {
    r'\d+': '하나 이상의 숫자',           # 123, 4567
    r'\D+': '숫자가 아닌 문자',            # abc, def
    r'\w+': '단어 문자 (영문, 숫자, _)',    # hello, test_123
    r'\W+': '단어 문자가 아닌 것',          # !@#, 공백
    r'\s+': '하나 이상의 공백문자',         # 스페이스, 탭, 줄바꿈
    r'\S+': '공백이 아닌 문자',            # hello, 123
}

for pattern, description in patterns.items():
    print(f"{pattern:8} : {description}")
```

### 실무에서 자주 사용하는 패턴

#### 1. 구분자 패턴
```python
# 다양한 구분자 처리
separators = {
    r',': '쉼표',
    r'\t': '탭',
    r'\s+': '하나 이상의 공백',
    r'[,;\t]': '쉼표, 세미콜론, 탭 중 하나',
    r'[,\s]+': '쉼표 또는 공백',
    r'\s*,\s*': '쉼표 앞뒤 공백 포함',
    r'\|': '파이프 문자',
    r'::': '더블 콜론'
}

# 사용 예시
df = pd.read_csv('data.txt', sep=r'\s*,\s*', engine='python')  # 쉼표 앞뒤 공백 무시
df = pd.read_csv('data.txt', sep=r'[,;\t]', engine='python')   # 다중 구분자
```

#### 2. 숫자 패턴
```python
number_patterns = {
    r'\d+': '정수',                    # 123, 456
    r'\d+\.\d+': '소수',              # 123.45, 67.89
    r'-?\d+': '음수 포함 정수',        # -123, 456, -78
    r'[+-]?\d+\.?\d*': '실수 전반',    # +123.45, -67, 89.0
    r'\d{1,3}(,\d{3})*': '천단위 구분', # 1,234, 567,890
}

# 숫자 데이터 처리 예시
text_with_numbers = "가격: 1,234,567원, 할인율: -15.5%"
price = re.findall(r'\d{1,3}(,\d{3})*', text_with_numbers)
discount = re.findall(r'-?\d+\.?\d*', text_with_numbers)
```

#### 3. 날짜/시간 패턴
```python
date_patterns = {
    r'\d{4}-\d{2}-\d{2}': 'YYYY-MM-DD',        # 2024-01-15
    r'\d{2}/\d{2}/\d{4}': 'MM/DD/YYYY',        # 01/15/2024
    r'\d{4}\.\d{2}\.\d{2}': 'YYYY.MM.DD',      # 2024.01.15
    r'\d{2}:\d{2}:\d{2}': 'HH:MM:SS',          # 14:30:25
    r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}': 'DATETIME', # 2024-01-15 14:30:25
}

# 날짜 데이터 처리
date_text = "회의 일정: 2024-01-15 14:30:00"
datetime_match = re.search(r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}', date_text)
if datetime_match:
    print(f"추출된 날짜시간: {datetime_match.group()}")
```

#### 4. 이메일/URL 패턴
```python
contact_patterns = {
    r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}': '이메일',
    r'https?://[^\s]+': 'URL',
    r'\d{3}-\d{4}-\d{4}': '전화번호 (010-1234-5678)',
    r'\([0-9]{3}\)\s?[0-9]{3}-[0-9]{4}': '미국식 전화번호',
}

# 연락처 정보 추출
contact_info = """
이메일: user@example.com
웹사이트: https://www.example.com
전화: 010-1234-5678
"""

emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', contact_info)
urls = re.findall(r'https?://[^\s]+', contact_info)
phones = re.findall(r'\d{3}-\d{4}-\d{4}', contact_info)
```

### 수량자(Quantifiers)

```python
quantifier_examples = {
    r'a{3}': 'a가 정확히 3번',          # aaa
    r'a{2,5}': 'a가 2~5번',            # aa, aaa, aaaa, aaaaa
    r'a{3,}': 'a가 3번 이상',          # aaa, aaaa, aaaaa...
    r'a*': 'a가 0번 이상',             # '', a, aa, aaa...
    r'a+': 'a가 1번 이상',             # a, aa, aaa...
    r'a?': 'a가 0번 또는 1번',         # '', a
}

# 실제 데이터에서 활용
data_validation = {
    r'^\d{4}-\d{2}-\d{2}

---

## 📊 Excel 파일 처리

### 기본 Excel 읽기 및 저장

```python
# Excel 파일 읽기
df = pd.read_excel('data.xlsx')  # 첫 번째 시트
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')  # 특정 시트

# Excel 파일 저장
df.to_excel('output.xlsx', index=False)
```

### 다중 시트 처리

```python
# 여러 시트 동시 읽기
excel_dict = pd.read_excel('data.xlsx', sheet_name=None)  # 모든 시트
print(excel_dict.keys())  # 시트 이름들

# 특정 시트들만 읽기
sheets_dict = pd.read_excel('data.xlsx', sheet_name=['Sheet1', 'Sheet2'])

# 여러 시트로 저장
with pd.ExcelWriter('multi_sheet.xlsx') as writer:
    df1.to_excel(writer, sheet_name='데이터1', index=False)
    df2.to_excel(writer, sheet_name='데이터2', index=False)
```

### 실무 예제: 제품 데이터 Excel 처리

```python
# 제품 데이터 생성
product_data = {
    'name': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(product_data)

# Excel로 저장 (다양한 옵션)
df.to_excel('products.xlsx', 
           index=False,           # 인덱스 제외
           sheet_name='제품목록',  # 시트명 지정
           encoding='utf-8')      # 인코딩 지정

# Excel에서 읽기
df_read = pd.read_excel('products.xlsx', sheet_name='제품목록')
print(df_read)
```

---

## 🌐 JSON 파일 처리

### JSON 기본 처리

```python
# JSON 파일로 저장
df.to_json('data.json')
df.to_json('data.json', orient='records')  # 레코드 형태
df.to_json('data.json', orient='index')    # 인덱스 기반

# JSON 파일 읽기
df = pd.read_json('data.json')

# 문자열 JSON 처리
json_string = df.to_json(orient='records')
df_from_json = pd.read_json(json_string)
```

### JSON 형태별 저장 방식

```python
import pandas as pd

# 샘플 데이터
data = {'apple': {'count': 10, 'price': 1500},
        'orange': {'count': 4, 'price': 700}}
df = pd.DataFrame(data)

print("1. 기본 형태:")
print(df.to_json())

print("\n2. 레코드 형태 (웹 API에 적합):")
print(df.to_json(orient='records'))

print("\n3. 인덱스 형태:")
print(df.to_json(orient='index'))

print("\n4. 값만 저장:")
print(df.to_json(orient='values'))
```

---

## 🌍 HTML 및 웹 데이터 처리

### 웹 테이블 읽기

```python
# 웹 페이지에서 테이블 읽기
url = "https://ko.wikipedia.org/wiki/리눅스"
tables = pd.read_html(url, encoding='utf-8')
print(f"총 {len(tables)}개의 테이블 발견")

# 첫 번째 테이블 확인
if tables:
    first_table = tables[0]
    print(first_table.head())
```

### HTML로 저장

```python
# HTML 형태로 저장
html_string = df.to_html()
print(html_string)

# 파일로 저장
df.to_html('table.html', index=False)
```

---

## 📋 클립보드 활용

### 클립보드와 데이터 교환

```python
# 클립보드로 복사 (Excel에서 바로 붙여넣기 가능)
df.to_clipboard(index=False)
print("데이터가 클립보드에 복사되었습니다.")

# 클립보드에서 읽기 (Excel에서 복사한 데이터)
# df_from_clipboard = pd.read_clipboard()
```

---

## 🔄 대용량 데이터: 청크(Chunk) 처리

### 청크 처리 기본

```python
# 대용량 CSV 파일을 청크 단위로 처리
chunk_size = 1000
chunks = []

# 청크 단위로 읽어서 처리
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # 각 청크에 대한 처리
    processed_chunk = chunk[chunk['score'] > 80]  # 예시 필터링
    chunks.append(processed_chunk)

# 모든 청크 결합
final_df = pd.concat(chunks, ignore_index=True)
```

### 메모리 효율적인 처리

```python
import time

# 일반적인 방법 vs 청크 처리 성능 비교
def process_normal(filepath):
    start = time.time()
    df = pd.read_csv(filepath)
    result = df.groupby('category').mean()
    end = time.time()
    return result, end - start

def process_chunks(filepath, chunk_size=1000):
    start = time.time()
    results = []
    
    for chunk in pd.read_csv(filepath, chunksize=chunk_size):
        chunk_result = chunk.groupby('category').mean()
        results.append(chunk_result)
    
    final_result = pd.concat(results).groupby(level=0).mean()
    end = time.time()
    return final_result, end - start
```

**💡 청크 처리의 장단점**

**장점:**
- 메모리 사용량 절약
- 대용량 파일 처리 가능
- 스트리밍 방식 처리

**단점:**
- 처리 속도 약간 저하
- 복잡한 코드 구조
- 전체 데이터 기반 연산 어려움

---

## 🔧 고정폭 파일(FWF) 처리

### 고정폭 데이터 읽기

```python
# 고정폭 파일 읽기 예시
# 데이터 형태: 이름(10자) + 나이(3자) + 점수(5자)
df = pd.read_fwf('fixed_width.txt',
                 widths=[10, 3, 5],  # 각 컬럼의 폭
                 names=['name', 'age', 'score'],
                 encoding='utf-8')
print(df)
```

---

## 📊 다양한 저장 형식 비교

### 형식별 저장 예제

```python
# 샘플 데이터 생성
sample_data = {
    'product': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(sample_data)

# 1. CSV 저장 (구분자 옵션)
df.to_csv('data.csv', index=False, sep=',')
df.to_csv('data_tab.txt', index=False, sep='\t')  # 탭 구분

# 2. Excel 저장
df.to_excel('data.xlsx', index=False)

# 3. JSON 저장
df.to_json('data.json', orient='records', force_ascii=False)

# 4. 전치(Transpose) 저장
df_transposed = df.T  # 행과 열 바꾸기
df_transposed.to_csv('data_transposed.csv')
```

### 트랜스포즈(Transpose) 활용

```python
# 데이터 구조 변경
items = {
    'apple': {'count': 10, 'price': 1500},
    'orange': {'count': 4, 'price': 700}
}
df = pd.DataFrame(items)
print("원본 데이터:")
print(df)

# 행과 열 바꾸기
df_t = df.T
print("\n전치된 데이터:")
print(df_t)

# 전치된 데이터 저장
df_t.to_csv('transposed_data.csv')
```

---

## ⚙️ 실무 팁 및 모범 사례

### 1. 인코딩 문제 해결

```python
# 한글 파일 처리 시 권장 설정
encodings_to_try = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']

for encoding in encodings_to_try:
    try:
        df = pd.read_csv('korean_data.csv', encoding=encoding)
        print(f"성공: {encoding}")
        break
    except UnicodeDecodeError:
        print(f"실패: {encoding}")
        continue
```

### 2. 파일 존재 여부 확인

```python
import os

def safe_read_csv(filepath):
    if os.path.exists(filepath):
        return pd.read_csv(filepath)
    else:
        print(f"파일을 찾을 수 없습니다: {filepath}")
        return None

# 사용 예시
df = safe_read_csv('data.csv')
if df is not None:
    print(df.head())
```

### 3. 메모리 최적화 데이터 타입

```python
# 메모리 효율적인 데이터 타입 지정
efficient_dtypes = {
    'id': 'int32',      # int64 → int32
    'category': 'category',  # object → category
    'score': 'float32'  # float64 → float32
}

df = pd.read_csv('data.csv', dtype=efficient_dtypes)
print(f"메모리 사용량: {df.memory_usage(deep=True).sum()} bytes")
```

### 4. 배치 파일 처리

```python
import glob

# 여러 CSV 파일을 하나로 합치기
csv_files = glob.glob("data_*.csv")
dataframes = []

for file in csv_files:
    df_temp = pd.read_csv(file)
    df_temp['source_file'] = file  # 출처 파일명 추가
    dataframes.append(df_temp)

# 모든 파일 결합
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df.to_csv('combined_data.csv', index=False)
```

---

## 🎯 성능 비교 및 선택 가이드

### 파일 형식별 성능 특성

| 형식 | 읽기 속도 | 파일 크기 | 호환성 | 사용 사례 |
|------|-----------|-----------|--------|-----------|
| CSV | 빠름 | 보통 | 높음 | 범용 데이터 교환 |
| Excel | 보통 | 큼 | 보통 | 비즈니스 문서 |
| JSON | 보통 | 큼 | 높음 | 웹 API, 설정 파일 |
| Parquet | 매우 빠름 | 작음 | 낮음 | 빅데이터 분석 |

### 상황별 권장 형식

```python
# 상황별 권장 파일 형식

# 1. 범용 데이터 교환 → CSV
df.to_csv('data.csv', index=False)

# 2. 비즈니스 보고서 → Excel
df.to_excel('report.xlsx', index=False)

# 3. 웹 API 데이터 → JSON
df.to_json('api_data.json', orient='records')

# 4. 대용량 분석 데이터 → Parquet (별도 라이브러리 필요)
# df.to_parquet('big_data.parquet')

# 5. 웹 표시용 → HTML
df.to_html('table.html', index=False, table_id='data-table')
```

---

## 📋 종합 실습 예제

```python
def comprehensive_file_io_example():
    """
    판다스 파일 입출력 종합 예제
    """
    
    # 1. 데이터 생성
    print("1. 샘플 데이터 생성")
    data = {
        'product_id': range(1, 101),
        'product_name': [f'Product_{i}' for i in range(1, 101)],
        'price': np.random.randint(1000, 100000, 100),
        'category': np.random.choice(['A', 'B', 'C'], 100),
        'stock': np.random.randint(0, 50, 100)
    }
    df = pd.DataFrame(data)
    print(f"데이터 형태: {df.shape}")
    
    # 2. 다양한 형식으로 저장
    print("\n2. 다양한 형식으로 저장")
    
    # CSV (기본)
    df.to_csv('products.csv', index=False)
    
    # Excel (다중 시트)
    with pd.ExcelWriter('products_multi.xlsx') as writer:
        df.to_excel(writer, sheet_name='전체', index=False)
        df[df['category'] == 'A'].to_excel(writer, sheet_name='카테고리A', index=False)
        df[df['stock'] < 10].to_excel(writer, sheet_name='재고부족', index=False)
    
    # JSON (레코드 형태)
    df.to_json('products.json', orient='records', force_ascii=False)
    
    # 3. 파일 읽기 및 검증
    print("\n3. 파일 읽기 및 검증")
    
    df_csv = pd.read_csv('products.csv')
    df_excel = pd.read_excel('products_multi.xlsx', sheet_name='전체')
    df_json = pd.read_json('products.json')
    
    # 데이터 일치성 확인
    print(f"CSV 읽기: {df_csv.shape}")
    print(f"Excel 읽기: {df_excel.shape}")
    print(f"JSON 읽기: {df_json.shape}")
    
    # 4. 통계 정보 출력
    print("\n4. 통계 정보")
    print(df.groupby('category')['price'].agg(['mean', 'count', 'sum']))
    
    print("\n파일 입출력 예제 완료!")

# 실행
comprehensive_file_io_example()
```

---

## 🔍 문제 해결 가이드

### 자주 발생하는 문제와 해결책

#### 1. 인코딩 오류
```python
# 문제: UnicodeDecodeError
# 해결: 적절한 인코딩 지정
df = pd.read_csv('data.csv', encoding='utf-8-sig')
```

#### 2. 메모리 부족
```python
# 문제: MemoryError
# 해결: 청크 처리 또는 데이터 타입 최적화
for chunk in pd.read_csv('big_file.csv', chunksize=1000):
    process(chunk)
```

#### 3. 날짜 형식 문제
```python
# 문제: 날짜 컬럼이 문자열로 읽힘
# 해결: parse_dates 옵션 사용
df = pd.read_csv('data.csv', parse_dates=['date_column'])
```

#### 4. 구분자 문제
```python
# 문제: 데이터가 제대로 분리되지 않음
# 해결: 정확한 구분자 지정
df = pd.read_csv('data.txt', sep='\t')  # 탭 구분
df = pd.read_csv('data.txt', sep=r'\s+')  # 공백 구분
```

---

## 📚 추천 학습 자료

### 공식 문서
- [Pandas I/O Tools](https://pandas.pydata.org/docs/user_guide/io.html)
- [CSV 파일 처리 가이드](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)

### 실무 활용
- 대용량 데이터 처리 시 청크 단위 처리 활용
- 한글 데이터 처리 시 인코딩 주의
- 웹 스크래핑 시 HTML 테이블 읽기 기능 활용
- API 데이터 처리 시 JSON 형식 활용

---

## 🎯 핵심 요약

### 반드시 기억할 포인트

1. **파일 형식별 특징 이해**
   - CSV: 범용성, 가벼움
   - Excel: 비즈니스 친화적, 다중 시트
   - JSON: 웹 친화적, 구조적

2. **인코딩 처리**
   - 한글 파일: `encoding='utf-8-sig'` 또는 `encoding='cp949'`
   - 웹 데이터: `encoding='utf-8'`

3. **메모리 관리**
   - 대용량 파일: 청크 처리 활용
   - 데이터 타입 최적화로 메모리 절약

4. **실무 활용 패턴**
   - 파일 존재 여부 확인
   - 예외 처리 구현
   - 배치 처리 자동화

판다스의 파일 I/O 기능을 마스터하면 다양한 데이터 소스와의 연동이 가능해지며, 실무에서의 데이터 처리 효율성이 크게 향상됩니다! 💪: '날짜 형식 검증',
    r'^[A-Z]{2,3}\d{3,4}

---

## 📊 Excel 파일 처리

### 기본 Excel 읽기 및 저장

```python
# Excel 파일 읽기
df = pd.read_excel('data.xlsx')  # 첫 번째 시트
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')  # 특정 시트

# Excel 파일 저장
df.to_excel('output.xlsx', index=False)
```

### 다중 시트 처리

```python
# 여러 시트 동시 읽기
excel_dict = pd.read_excel('data.xlsx', sheet_name=None)  # 모든 시트
print(excel_dict.keys())  # 시트 이름들

# 특정 시트들만 읽기
sheets_dict = pd.read_excel('data.xlsx', sheet_name=['Sheet1', 'Sheet2'])

# 여러 시트로 저장
with pd.ExcelWriter('multi_sheet.xlsx') as writer:
    df1.to_excel(writer, sheet_name='데이터1', index=False)
    df2.to_excel(writer, sheet_name='데이터2', index=False)
```

### 실무 예제: 제품 데이터 Excel 처리

```python
# 제품 데이터 생성
product_data = {
    'name': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(product_data)

# Excel로 저장 (다양한 옵션)
df.to_excel('products.xlsx', 
           index=False,           # 인덱스 제외
           sheet_name='제품목록',  # 시트명 지정
           encoding='utf-8')      # 인코딩 지정

# Excel에서 읽기
df_read = pd.read_excel('products.xlsx', sheet_name='제품목록')
print(df_read)
```

---

## 🌐 JSON 파일 처리

### JSON 기본 처리

```python
# JSON 파일로 저장
df.to_json('data.json')
df.to_json('data.json', orient='records')  # 레코드 형태
df.to_json('data.json', orient='index')    # 인덱스 기반

# JSON 파일 읽기
df = pd.read_json('data.json')

# 문자열 JSON 처리
json_string = df.to_json(orient='records')
df_from_json = pd.read_json(json_string)
```

### JSON 형태별 저장 방식

```python
import pandas as pd

# 샘플 데이터
data = {'apple': {'count': 10, 'price': 1500},
        'orange': {'count': 4, 'price': 700}}
df = pd.DataFrame(data)

print("1. 기본 형태:")
print(df.to_json())

print("\n2. 레코드 형태 (웹 API에 적합):")
print(df.to_json(orient='records'))

print("\n3. 인덱스 형태:")
print(df.to_json(orient='index'))

print("\n4. 값만 저장:")
print(df.to_json(orient='values'))
```

---

## 🌍 HTML 및 웹 데이터 처리

### 웹 테이블 읽기

```python
# 웹 페이지에서 테이블 읽기
url = "https://ko.wikipedia.org/wiki/리눅스"
tables = pd.read_html(url, encoding='utf-8')
print(f"총 {len(tables)}개의 테이블 발견")

# 첫 번째 테이블 확인
if tables:
    first_table = tables[0]
    print(first_table.head())
```

### HTML로 저장

```python
# HTML 형태로 저장
html_string = df.to_html()
print(html_string)

# 파일로 저장
df.to_html('table.html', index=False)
```

---

## 📋 클립보드 활용

### 클립보드와 데이터 교환

```python
# 클립보드로 복사 (Excel에서 바로 붙여넣기 가능)
df.to_clipboard(index=False)
print("데이터가 클립보드에 복사되었습니다.")

# 클립보드에서 읽기 (Excel에서 복사한 데이터)
# df_from_clipboard = pd.read_clipboard()
```

---

## 🔄 대용량 데이터: 청크(Chunk) 처리

### 청크 처리 기본

```python
# 대용량 CSV 파일을 청크 단위로 처리
chunk_size = 1000
chunks = []

# 청크 단위로 읽어서 처리
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # 각 청크에 대한 처리
    processed_chunk = chunk[chunk['score'] > 80]  # 예시 필터링
    chunks.append(processed_chunk)

# 모든 청크 결합
final_df = pd.concat(chunks, ignore_index=True)
```

### 메모리 효율적인 처리

```python
import time

# 일반적인 방법 vs 청크 처리 성능 비교
def process_normal(filepath):
    start = time.time()
    df = pd.read_csv(filepath)
    result = df.groupby('category').mean()
    end = time.time()
    return result, end - start

def process_chunks(filepath, chunk_size=1000):
    start = time.time()
    results = []
    
    for chunk in pd.read_csv(filepath, chunksize=chunk_size):
        chunk_result = chunk.groupby('category').mean()
        results.append(chunk_result)
    
    final_result = pd.concat(results).groupby(level=0).mean()
    end = time.time()
    return final_result, end - start
```

**💡 청크 처리의 장단점**

**장점:**
- 메모리 사용량 절약
- 대용량 파일 처리 가능
- 스트리밍 방식 처리

**단점:**
- 처리 속도 약간 저하
- 복잡한 코드 구조
- 전체 데이터 기반 연산 어려움

---

## 🔧 고정폭 파일(FWF) 처리

### 고정폭 데이터 읽기

```python
# 고정폭 파일 읽기 예시
# 데이터 형태: 이름(10자) + 나이(3자) + 점수(5자)
df = pd.read_fwf('fixed_width.txt',
                 widths=[10, 3, 5],  # 각 컬럼의 폭
                 names=['name', 'age', 'score'],
                 encoding='utf-8')
print(df)
```

---

## 📊 다양한 저장 형식 비교

### 형식별 저장 예제

```python
# 샘플 데이터 생성
sample_data = {
    'product': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(sample_data)

# 1. CSV 저장 (구분자 옵션)
df.to_csv('data.csv', index=False, sep=',')
df.to_csv('data_tab.txt', index=False, sep='\t')  # 탭 구분

# 2. Excel 저장
df.to_excel('data.xlsx', index=False)

# 3. JSON 저장
df.to_json('data.json', orient='records', force_ascii=False)

# 4. 전치(Transpose) 저장
df_transposed = df.T  # 행과 열 바꾸기
df_transposed.to_csv('data_transposed.csv')
```

### 트랜스포즈(Transpose) 활용

```python
# 데이터 구조 변경
items = {
    'apple': {'count': 10, 'price': 1500},
    'orange': {'count': 4, 'price': 700}
}
df = pd.DataFrame(items)
print("원본 데이터:")
print(df)

# 행과 열 바꾸기
df_t = df.T
print("\n전치된 데이터:")
print(df_t)

# 전치된 데이터 저장
df_t.to_csv('transposed_data.csv')
```

---

## ⚙️ 실무 팁 및 모범 사례

### 1. 인코딩 문제 해결

```python
# 한글 파일 처리 시 권장 설정
encodings_to_try = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']

for encoding in encodings_to_try:
    try:
        df = pd.read_csv('korean_data.csv', encoding=encoding)
        print(f"성공: {encoding}")
        break
    except UnicodeDecodeError:
        print(f"실패: {encoding}")
        continue
```

### 2. 파일 존재 여부 확인

```python
import os

def safe_read_csv(filepath):
    if os.path.exists(filepath):
        return pd.read_csv(filepath)
    else:
        print(f"파일을 찾을 수 없습니다: {filepath}")
        return None

# 사용 예시
df = safe_read_csv('data.csv')
if df is not None:
    print(df.head())
```

### 3. 메모리 최적화 데이터 타입

```python
# 메모리 효율적인 데이터 타입 지정
efficient_dtypes = {
    'id': 'int32',      # int64 → int32
    'category': 'category',  # object → category
    'score': 'float32'  # float64 → float32
}

df = pd.read_csv('data.csv', dtype=efficient_dtypes)
print(f"메모리 사용량: {df.memory_usage(deep=True).sum()} bytes")
```

### 4. 배치 파일 처리

```python
import glob

# 여러 CSV 파일을 하나로 합치기
csv_files = glob.glob("data_*.csv")
dataframes = []

for file in csv_files:
    df_temp = pd.read_csv(file)
    df_temp['source_file'] = file  # 출처 파일명 추가
    dataframes.append(df_temp)

# 모든 파일 결합
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df.to_csv('combined_data.csv', index=False)
```

---

## 🎯 성능 비교 및 선택 가이드

### 파일 형식별 성능 특성

| 형식 | 읽기 속도 | 파일 크기 | 호환성 | 사용 사례 |
|------|-----------|-----------|--------|-----------|
| CSV | 빠름 | 보통 | 높음 | 범용 데이터 교환 |
| Excel | 보통 | 큼 | 보통 | 비즈니스 문서 |
| JSON | 보통 | 큼 | 높음 | 웹 API, 설정 파일 |
| Parquet | 매우 빠름 | 작음 | 낮음 | 빅데이터 분석 |

### 상황별 권장 형식

```python
# 상황별 권장 파일 형식

# 1. 범용 데이터 교환 → CSV
df.to_csv('data.csv', index=False)

# 2. 비즈니스 보고서 → Excel
df.to_excel('report.xlsx', index=False)

# 3. 웹 API 데이터 → JSON
df.to_json('api_data.json', orient='records')

# 4. 대용량 분석 데이터 → Parquet (별도 라이브러리 필요)
# df.to_parquet('big_data.parquet')

# 5. 웹 표시용 → HTML
df.to_html('table.html', index=False, table_id='data-table')
```

---

## 📋 종합 실습 예제

```python
def comprehensive_file_io_example():
    """
    판다스 파일 입출력 종합 예제
    """
    
    # 1. 데이터 생성
    print("1. 샘플 데이터 생성")
    data = {
        'product_id': range(1, 101),
        'product_name': [f'Product_{i}' for i in range(1, 101)],
        'price': np.random.randint(1000, 100000, 100),
        'category': np.random.choice(['A', 'B', 'C'], 100),
        'stock': np.random.randint(0, 50, 100)
    }
    df = pd.DataFrame(data)
    print(f"데이터 형태: {df.shape}")
    
    # 2. 다양한 형식으로 저장
    print("\n2. 다양한 형식으로 저장")
    
    # CSV (기본)
    df.to_csv('products.csv', index=False)
    
    # Excel (다중 시트)
    with pd.ExcelWriter('products_multi.xlsx') as writer:
        df.to_excel(writer, sheet_name='전체', index=False)
        df[df['category'] == 'A'].to_excel(writer, sheet_name='카테고리A', index=False)
        df[df['stock'] < 10].to_excel(writer, sheet_name='재고부족', index=False)
    
    # JSON (레코드 형태)
    df.to_json('products.json', orient='records', force_ascii=False)
    
    # 3. 파일 읽기 및 검증
    print("\n3. 파일 읽기 및 검증")
    
    df_csv = pd.read_csv('products.csv')
    df_excel = pd.read_excel('products_multi.xlsx', sheet_name='전체')
    df_json = pd.read_json('products.json')
    
    # 데이터 일치성 확인
    print(f"CSV 읽기: {df_csv.shape}")
    print(f"Excel 읽기: {df_excel.shape}")
    print(f"JSON 읽기: {df_json.shape}")
    
    # 4. 통계 정보 출력
    print("\n4. 통계 정보")
    print(df.groupby('category')['price'].agg(['mean', 'count', 'sum']))
    
    print("\n파일 입출력 예제 완료!")

# 실행
comprehensive_file_io_example()
```

---

## 🔍 문제 해결 가이드

### 자주 발생하는 문제와 해결책

#### 1. 인코딩 오류
```python
# 문제: UnicodeDecodeError
# 해결: 적절한 인코딩 지정
df = pd.read_csv('data.csv', encoding='utf-8-sig')
```

#### 2. 메모리 부족
```python
# 문제: MemoryError
# 해결: 청크 처리 또는 데이터 타입 최적화
for chunk in pd.read_csv('big_file.csv', chunksize=1000):
    process(chunk)
```

#### 3. 날짜 형식 문제
```python
# 문제: 날짜 컬럼이 문자열로 읽힘
# 해결: parse_dates 옵션 사용
df = pd.read_csv('data.csv', parse_dates=['date_column'])
```

#### 4. 구분자 문제
```python
# 문제: 데이터가 제대로 분리되지 않음
# 해결: 정확한 구분자 지정
df = pd.read_csv('data.txt', sep='\t')  # 탭 구분
df = pd.read_csv('data.txt', sep=r'\s+')  # 공백 구분
```

---

## 📚 추천 학습 자료

### 공식 문서
- [Pandas I/O Tools](https://pandas.pydata.org/docs/user_guide/io.html)
- [CSV 파일 처리 가이드](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)

### 실무 활용
- 대용량 데이터 처리 시 청크 단위 처리 활용
- 한글 데이터 처리 시 인코딩 주의
- 웹 스크래핑 시 HTML 테이블 읽기 기능 활용
- API 데이터 처리 시 JSON 형식 활용

---

## 🎯 핵심 요약

### 반드시 기억할 포인트

1. **파일 형식별 특징 이해**
   - CSV: 범용성, 가벼움
   - Excel: 비즈니스 친화적, 다중 시트
   - JSON: 웹 친화적, 구조적

2. **인코딩 처리**
   - 한글 파일: `encoding='utf-8-sig'` 또는 `encoding='cp949'`
   - 웹 데이터: `encoding='utf-8'`

3. **메모리 관리**
   - 대용량 파일: 청크 처리 활용
   - 데이터 타입 최적화로 메모리 절약

4. **실무 활용 패턴**
   - 파일 존재 여부 확인
   - 예외 처리 구현
   - 배치 처리 자동화

판다스의 파일 I/O 기능을 마스터하면 다양한 데이터 소스와의 연동이 가능해지며, 실무에서의 데이터 처리 효율성이 크게 향상됩니다! 💪: '상품코드 검증 (AB123, ABC1234)',
    r'^\w{8,20}

---

## 📊 Excel 파일 처리

### 기본 Excel 읽기 및 저장

```python
# Excel 파일 읽기
df = pd.read_excel('data.xlsx')  # 첫 번째 시트
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')  # 특정 시트

# Excel 파일 저장
df.to_excel('output.xlsx', index=False)
```

### 다중 시트 처리

```python
# 여러 시트 동시 읽기
excel_dict = pd.read_excel('data.xlsx', sheet_name=None)  # 모든 시트
print(excel_dict.keys())  # 시트 이름들

# 특정 시트들만 읽기
sheets_dict = pd.read_excel('data.xlsx', sheet_name=['Sheet1', 'Sheet2'])

# 여러 시트로 저장
with pd.ExcelWriter('multi_sheet.xlsx') as writer:
    df1.to_excel(writer, sheet_name='데이터1', index=False)
    df2.to_excel(writer, sheet_name='데이터2', index=False)
```

### 실무 예제: 제품 데이터 Excel 처리

```python
# 제품 데이터 생성
product_data = {
    'name': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(product_data)

# Excel로 저장 (다양한 옵션)
df.to_excel('products.xlsx', 
           index=False,           # 인덱스 제외
           sheet_name='제품목록',  # 시트명 지정
           encoding='utf-8')      # 인코딩 지정

# Excel에서 읽기
df_read = pd.read_excel('products.xlsx', sheet_name='제품목록')
print(df_read)
```

---

## 🌐 JSON 파일 처리

### JSON 기본 처리

```python
# JSON 파일로 저장
df.to_json('data.json')
df.to_json('data.json', orient='records')  # 레코드 형태
df.to_json('data.json', orient='index')    # 인덱스 기반

# JSON 파일 읽기
df = pd.read_json('data.json')

# 문자열 JSON 처리
json_string = df.to_json(orient='records')
df_from_json = pd.read_json(json_string)
```

### JSON 형태별 저장 방식

```python
import pandas as pd

# 샘플 데이터
data = {'apple': {'count': 10, 'price': 1500},
        'orange': {'count': 4, 'price': 700}}
df = pd.DataFrame(data)

print("1. 기본 형태:")
print(df.to_json())

print("\n2. 레코드 형태 (웹 API에 적합):")
print(df.to_json(orient='records'))

print("\n3. 인덱스 형태:")
print(df.to_json(orient='index'))

print("\n4. 값만 저장:")
print(df.to_json(orient='values'))
```

---

## 🌍 HTML 및 웹 데이터 처리

### 웹 테이블 읽기

```python
# 웹 페이지에서 테이블 읽기
url = "https://ko.wikipedia.org/wiki/리눅스"
tables = pd.read_html(url, encoding='utf-8')
print(f"총 {len(tables)}개의 테이블 발견")

# 첫 번째 테이블 확인
if tables:
    first_table = tables[0]
    print(first_table.head())
```

### HTML로 저장

```python
# HTML 형태로 저장
html_string = df.to_html()
print(html_string)

# 파일로 저장
df.to_html('table.html', index=False)
```

---

## 📋 클립보드 활용

### 클립보드와 데이터 교환

```python
# 클립보드로 복사 (Excel에서 바로 붙여넣기 가능)
df.to_clipboard(index=False)
print("데이터가 클립보드에 복사되었습니다.")

# 클립보드에서 읽기 (Excel에서 복사한 데이터)
# df_from_clipboard = pd.read_clipboard()
```

---

## 🔄 대용량 데이터: 청크(Chunk) 처리

### 청크 처리 기본

```python
# 대용량 CSV 파일을 청크 단위로 처리
chunk_size = 1000
chunks = []

# 청크 단위로 읽어서 처리
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # 각 청크에 대한 처리
    processed_chunk = chunk[chunk['score'] > 80]  # 예시 필터링
    chunks.append(processed_chunk)

# 모든 청크 결합
final_df = pd.concat(chunks, ignore_index=True)
```

### 메모리 효율적인 처리

```python
import time

# 일반적인 방법 vs 청크 처리 성능 비교
def process_normal(filepath):
    start = time.time()
    df = pd.read_csv(filepath)
    result = df.groupby('category').mean()
    end = time.time()
    return result, end - start

def process_chunks(filepath, chunk_size=1000):
    start = time.time()
    results = []
    
    for chunk in pd.read_csv(filepath, chunksize=chunk_size):
        chunk_result = chunk.groupby('category').mean()
        results.append(chunk_result)
    
    final_result = pd.concat(results).groupby(level=0).mean()
    end = time.time()
    return final_result, end - start
```

**💡 청크 처리의 장단점**

**장점:**
- 메모리 사용량 절약
- 대용량 파일 처리 가능
- 스트리밍 방식 처리

**단점:**
- 처리 속도 약간 저하
- 복잡한 코드 구조
- 전체 데이터 기반 연산 어려움

---

## 🔧 고정폭 파일(FWF) 처리

### 고정폭 데이터 읽기

```python
# 고정폭 파일 읽기 예시
# 데이터 형태: 이름(10자) + 나이(3자) + 점수(5자)
df = pd.read_fwf('fixed_width.txt',
                 widths=[10, 3, 5],  # 각 컬럼의 폭
                 names=['name', 'age', 'score'],
                 encoding='utf-8')
print(df)
```

---

## 📊 다양한 저장 형식 비교

### 형식별 저장 예제

```python
# 샘플 데이터 생성
sample_data = {
    'product': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(sample_data)

# 1. CSV 저장 (구분자 옵션)
df.to_csv('data.csv', index=False, sep=',')
df.to_csv('data_tab.txt', index=False, sep='\t')  # 탭 구분

# 2. Excel 저장
df.to_excel('data.xlsx', index=False)

# 3. JSON 저장
df.to_json('data.json', orient='records', force_ascii=False)

# 4. 전치(Transpose) 저장
df_transposed = df.T  # 행과 열 바꾸기
df_transposed.to_csv('data_transposed.csv')
```

### 트랜스포즈(Transpose) 활용

```python
# 데이터 구조 변경
items = {
    'apple': {'count': 10, 'price': 1500},
    'orange': {'count': 4, 'price': 700}
}
df = pd.DataFrame(items)
print("원본 데이터:")
print(df)

# 행과 열 바꾸기
df_t = df.T
print("\n전치된 데이터:")
print(df_t)

# 전치된 데이터 저장
df_t.to_csv('transposed_data.csv')
```

---

## ⚙️ 실무 팁 및 모범 사례

### 1. 인코딩 문제 해결

```python
# 한글 파일 처리 시 권장 설정
encodings_to_try = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']

for encoding in encodings_to_try:
    try:
        df = pd.read_csv('korean_data.csv', encoding=encoding)
        print(f"성공: {encoding}")
        break
    except UnicodeDecodeError:
        print(f"실패: {encoding}")
        continue
```

### 2. 파일 존재 여부 확인

```python
import os

def safe_read_csv(filepath):
    if os.path.exists(filepath):
        return pd.read_csv(filepath)
    else:
        print(f"파일을 찾을 수 없습니다: {filepath}")
        return None

# 사용 예시
df = safe_read_csv('data.csv')
if df is not None:
    print(df.head())
```

### 3. 메모리 최적화 데이터 타입

```python
# 메모리 효율적인 데이터 타입 지정
efficient_dtypes = {
    'id': 'int32',      # int64 → int32
    'category': 'category',  # object → category
    'score': 'float32'  # float64 → float32
}

df = pd.read_csv('data.csv', dtype=efficient_dtypes)
print(f"메모리 사용량: {df.memory_usage(deep=True).sum()} bytes")
```

### 4. 배치 파일 처리

```python
import glob

# 여러 CSV 파일을 하나로 합치기
csv_files = glob.glob("data_*.csv")
dataframes = []

for file in csv_files:
    df_temp = pd.read_csv(file)
    df_temp['source_file'] = file  # 출처 파일명 추가
    dataframes.append(df_temp)

# 모든 파일 결합
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df.to_csv('combined_data.csv', index=False)
```

---

## 🎯 성능 비교 및 선택 가이드

### 파일 형식별 성능 특성

| 형식 | 읽기 속도 | 파일 크기 | 호환성 | 사용 사례 |
|------|-----------|-----------|--------|-----------|
| CSV | 빠름 | 보통 | 높음 | 범용 데이터 교환 |
| Excel | 보통 | 큼 | 보통 | 비즈니스 문서 |
| JSON | 보통 | 큼 | 높음 | 웹 API, 설정 파일 |
| Parquet | 매우 빠름 | 작음 | 낮음 | 빅데이터 분석 |

### 상황별 권장 형식

```python
# 상황별 권장 파일 형식

# 1. 범용 데이터 교환 → CSV
df.to_csv('data.csv', index=False)

# 2. 비즈니스 보고서 → Excel
df.to_excel('report.xlsx', index=False)

# 3. 웹 API 데이터 → JSON
df.to_json('api_data.json', orient='records')

# 4. 대용량 분석 데이터 → Parquet (별도 라이브러리 필요)
# df.to_parquet('big_data.parquet')

# 5. 웹 표시용 → HTML
df.to_html('table.html', index=False, table_id='data-table')
```

---

## 📋 종합 실습 예제

```python
def comprehensive_file_io_example():
    """
    판다스 파일 입출력 종합 예제
    """
    
    # 1. 데이터 생성
    print("1. 샘플 데이터 생성")
    data = {
        'product_id': range(1, 101),
        'product_name': [f'Product_{i}' for i in range(1, 101)],
        'price': np.random.randint(1000, 100000, 100),
        'category': np.random.choice(['A', 'B', 'C'], 100),
        'stock': np.random.randint(0, 50, 100)
    }
    df = pd.DataFrame(data)
    print(f"데이터 형태: {df.shape}")
    
    # 2. 다양한 형식으로 저장
    print("\n2. 다양한 형식으로 저장")
    
    # CSV (기본)
    df.to_csv('products.csv', index=False)
    
    # Excel (다중 시트)
    with pd.ExcelWriter('products_multi.xlsx') as writer:
        df.to_excel(writer, sheet_name='전체', index=False)
        df[df['category'] == 'A'].to_excel(writer, sheet_name='카테고리A', index=False)
        df[df['stock'] < 10].to_excel(writer, sheet_name='재고부족', index=False)
    
    # JSON (레코드 형태)
    df.to_json('products.json', orient='records', force_ascii=False)
    
    # 3. 파일 읽기 및 검증
    print("\n3. 파일 읽기 및 검증")
    
    df_csv = pd.read_csv('products.csv')
    df_excel = pd.read_excel('products_multi.xlsx', sheet_name='전체')
    df_json = pd.read_json('products.json')
    
    # 데이터 일치성 확인
    print(f"CSV 읽기: {df_csv.shape}")
    print(f"Excel 읽기: {df_excel.shape}")
    print(f"JSON 읽기: {df_json.shape}")
    
    # 4. 통계 정보 출력
    print("\n4. 통계 정보")
    print(df.groupby('category')['price'].agg(['mean', 'count', 'sum']))
    
    print("\n파일 입출력 예제 완료!")

# 실행
comprehensive_file_io_example()
```

---

## 🔍 문제 해결 가이드

### 자주 발생하는 문제와 해결책

#### 1. 인코딩 오류
```python
# 문제: UnicodeDecodeError
# 해결: 적절한 인코딩 지정
df = pd.read_csv('data.csv', encoding='utf-8-sig')
```

#### 2. 메모리 부족
```python
# 문제: MemoryError
# 해결: 청크 처리 또는 데이터 타입 최적화
for chunk in pd.read_csv('big_file.csv', chunksize=1000):
    process(chunk)
```

#### 3. 날짜 형식 문제
```python
# 문제: 날짜 컬럼이 문자열로 읽힘
# 해결: parse_dates 옵션 사용
df = pd.read_csv('data.csv', parse_dates=['date_column'])
```

#### 4. 구분자 문제
```python
# 문제: 데이터가 제대로 분리되지 않음
# 해결: 정확한 구분자 지정
df = pd.read_csv('data.txt', sep='\t')  # 탭 구분
df = pd.read_csv('data.txt', sep=r'\s+')  # 공백 구분
```

---

## 📚 추천 학습 자료

### 공식 문서
- [Pandas I/O Tools](https://pandas.pydata.org/docs/user_guide/io.html)
- [CSV 파일 처리 가이드](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)

### 실무 활용
- 대용량 데이터 처리 시 청크 단위 처리 활용
- 한글 데이터 처리 시 인코딩 주의
- 웹 스크래핑 시 HTML 테이블 읽기 기능 활용
- API 데이터 처리 시 JSON 형식 활용

---

## 🎯 핵심 요약

### 반드시 기억할 포인트

1. **파일 형식별 특징 이해**
   - CSV: 범용성, 가벼움
   - Excel: 비즈니스 친화적, 다중 시트
   - JSON: 웹 친화적, 구조적

2. **인코딩 처리**
   - 한글 파일: `encoding='utf-8-sig'` 또는 `encoding='cp949'`
   - 웹 데이터: `encoding='utf-8'`

3. **메모리 관리**
   - 대용량 파일: 청크 처리 활용
   - 데이터 타입 최적화로 메모리 절약

4. **실무 활용 패턴**
   - 파일 존재 여부 확인
   - 예외 처리 구현
   - 배치 처리 자동화

판다스의 파일 I/O 기능을 마스터하면 다양한 데이터 소스와의 연동이 가능해지며, 실무에서의 데이터 처리 효율성이 크게 향상됩니다! 💪: '사용자명 검증 (8-20자)',
    r'^\d{2,3}-\d{3,4}-\d{4}

---

## 📊 Excel 파일 처리

### 기본 Excel 읽기 및 저장

```python
# Excel 파일 읽기
df = pd.read_excel('data.xlsx')  # 첫 번째 시트
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')  # 특정 시트

# Excel 파일 저장
df.to_excel('output.xlsx', index=False)
```

### 다중 시트 처리

```python
# 여러 시트 동시 읽기
excel_dict = pd.read_excel('data.xlsx', sheet_name=None)  # 모든 시트
print(excel_dict.keys())  # 시트 이름들

# 특정 시트들만 읽기
sheets_dict = pd.read_excel('data.xlsx', sheet_name=['Sheet1', 'Sheet2'])

# 여러 시트로 저장
with pd.ExcelWriter('multi_sheet.xlsx') as writer:
    df1.to_excel(writer, sheet_name='데이터1', index=False)
    df2.to_excel(writer, sheet_name='데이터2', index=False)
```

### 실무 예제: 제품 데이터 Excel 처리

```python
# 제품 데이터 생성
product_data = {
    'name': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(product_data)

# Excel로 저장 (다양한 옵션)
df.to_excel('products.xlsx', 
           index=False,           # 인덱스 제외
           sheet_name='제품목록',  # 시트명 지정
           encoding='utf-8')      # 인코딩 지정

# Excel에서 읽기
df_read = pd.read_excel('products.xlsx', sheet_name='제품목록')
print(df_read)
```

---

## 🌐 JSON 파일 처리

### JSON 기본 처리

```python
# JSON 파일로 저장
df.to_json('data.json')
df.to_json('data.json', orient='records')  # 레코드 형태
df.to_json('data.json', orient='index')    # 인덱스 기반

# JSON 파일 읽기
df = pd.read_json('data.json')

# 문자열 JSON 처리
json_string = df.to_json(orient='records')
df_from_json = pd.read_json(json_string)
```

### JSON 형태별 저장 방식

```python
import pandas as pd

# 샘플 데이터
data = {'apple': {'count': 10, 'price': 1500},
        'orange': {'count': 4, 'price': 700}}
df = pd.DataFrame(data)

print("1. 기본 형태:")
print(df.to_json())

print("\n2. 레코드 형태 (웹 API에 적합):")
print(df.to_json(orient='records'))

print("\n3. 인덱스 형태:")
print(df.to_json(orient='index'))

print("\n4. 값만 저장:")
print(df.to_json(orient='values'))
```

---

## 🌍 HTML 및 웹 데이터 처리

### 웹 테이블 읽기

```python
# 웹 페이지에서 테이블 읽기
url = "https://ko.wikipedia.org/wiki/리눅스"
tables = pd.read_html(url, encoding='utf-8')
print(f"총 {len(tables)}개의 테이블 발견")

# 첫 번째 테이블 확인
if tables:
    first_table = tables[0]
    print(first_table.head())
```

### HTML로 저장

```python
# HTML 형태로 저장
html_string = df.to_html()
print(html_string)

# 파일로 저장
df.to_html('table.html', index=False)
```

---

## 📋 클립보드 활용

### 클립보드와 데이터 교환

```python
# 클립보드로 복사 (Excel에서 바로 붙여넣기 가능)
df.to_clipboard(index=False)
print("데이터가 클립보드에 복사되었습니다.")

# 클립보드에서 읽기 (Excel에서 복사한 데이터)
# df_from_clipboard = pd.read_clipboard()
```

---

## 🔄 대용량 데이터: 청크(Chunk) 처리

### 청크 처리 기본

```python
# 대용량 CSV 파일을 청크 단위로 처리
chunk_size = 1000
chunks = []

# 청크 단위로 읽어서 처리
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # 각 청크에 대한 처리
    processed_chunk = chunk[chunk['score'] > 80]  # 예시 필터링
    chunks.append(processed_chunk)

# 모든 청크 결합
final_df = pd.concat(chunks, ignore_index=True)
```

### 메모리 효율적인 처리

```python
import time

# 일반적인 방법 vs 청크 처리 성능 비교
def process_normal(filepath):
    start = time.time()
    df = pd.read_csv(filepath)
    result = df.groupby('category').mean()
    end = time.time()
    return result, end - start

def process_chunks(filepath, chunk_size=1000):
    start = time.time()
    results = []
    
    for chunk in pd.read_csv(filepath, chunksize=chunk_size):
        chunk_result = chunk.groupby('category').mean()
        results.append(chunk_result)
    
    final_result = pd.concat(results).groupby(level=0).mean()
    end = time.time()
    return final_result, end - start
```

**💡 청크 처리의 장단점**

**장점:**
- 메모리 사용량 절약
- 대용량 파일 처리 가능
- 스트리밍 방식 처리

**단점:**
- 처리 속도 약간 저하
- 복잡한 코드 구조
- 전체 데이터 기반 연산 어려움

---

## 🔧 고정폭 파일(FWF) 처리

### 고정폭 데이터 읽기

```python
# 고정폭 파일 읽기 예시
# 데이터 형태: 이름(10자) + 나이(3자) + 점수(5자)
df = pd.read_fwf('fixed_width.txt',
                 widths=[10, 3, 5],  # 각 컬럼의 폭
                 names=['name', 'age', 'score'],
                 encoding='utf-8')
print(df)
```

---

## 📊 다양한 저장 형식 비교

### 형식별 저장 예제

```python
# 샘플 데이터 생성
sample_data = {
    'product': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(sample_data)

# 1. CSV 저장 (구분자 옵션)
df.to_csv('data.csv', index=False, sep=',')
df.to_csv('data_tab.txt', index=False, sep='\t')  # 탭 구분

# 2. Excel 저장
df.to_excel('data.xlsx', index=False)

# 3. JSON 저장
df.to_json('data.json', orient='records', force_ascii=False)

# 4. 전치(Transpose) 저장
df_transposed = df.T  # 행과 열 바꾸기
df_transposed.to_csv('data_transposed.csv')
```

### 트랜스포즈(Transpose) 활용

```python
# 데이터 구조 변경
items = {
    'apple': {'count': 10, 'price': 1500},
    'orange': {'count': 4, 'price': 700}
}
df = pd.DataFrame(items)
print("원본 데이터:")
print(df)

# 행과 열 바꾸기
df_t = df.T
print("\n전치된 데이터:")
print(df_t)

# 전치된 데이터 저장
df_t.to_csv('transposed_data.csv')
```

---

## ⚙️ 실무 팁 및 모범 사례

### 1. 인코딩 문제 해결

```python
# 한글 파일 처리 시 권장 설정
encodings_to_try = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']

for encoding in encodings_to_try:
    try:
        df = pd.read_csv('korean_data.csv', encoding=encoding)
        print(f"성공: {encoding}")
        break
    except UnicodeDecodeError:
        print(f"실패: {encoding}")
        continue
```

### 2. 파일 존재 여부 확인

```python
import os

def safe_read_csv(filepath):
    if os.path.exists(filepath):
        return pd.read_csv(filepath)
    else:
        print(f"파일을 찾을 수 없습니다: {filepath}")
        return None

# 사용 예시
df = safe_read_csv('data.csv')
if df is not None:
    print(df.head())
```

### 3. 메모리 최적화 데이터 타입

```python
# 메모리 효율적인 데이터 타입 지정
efficient_dtypes = {
    'id': 'int32',      # int64 → int32
    'category': 'category',  # object → category
    'score': 'float32'  # float64 → float32
}

df = pd.read_csv('data.csv', dtype=efficient_dtypes)
print(f"메모리 사용량: {df.memory_usage(deep=True).sum()} bytes")
```

### 4. 배치 파일 처리

```python
import glob

# 여러 CSV 파일을 하나로 합치기
csv_files = glob.glob("data_*.csv")
dataframes = []

for file in csv_files:
    df_temp = pd.read_csv(file)
    df_temp['source_file'] = file  # 출처 파일명 추가
    dataframes.append(df_temp)

# 모든 파일 결합
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df.to_csv('combined_data.csv', index=False)
```

---

## 🎯 성능 비교 및 선택 가이드

### 파일 형식별 성능 특성

| 형식 | 읽기 속도 | 파일 크기 | 호환성 | 사용 사례 |
|------|-----------|-----------|--------|-----------|
| CSV | 빠름 | 보통 | 높음 | 범용 데이터 교환 |
| Excel | 보통 | 큼 | 보통 | 비즈니스 문서 |
| JSON | 보통 | 큼 | 높음 | 웹 API, 설정 파일 |
| Parquet | 매우 빠름 | 작음 | 낮음 | 빅데이터 분석 |

### 상황별 권장 형식

```python
# 상황별 권장 파일 형식

# 1. 범용 데이터 교환 → CSV
df.to_csv('data.csv', index=False)

# 2. 비즈니스 보고서 → Excel
df.to_excel('report.xlsx', index=False)

# 3. 웹 API 데이터 → JSON
df.to_json('api_data.json', orient='records')

# 4. 대용량 분석 데이터 → Parquet (별도 라이브러리 필요)
# df.to_parquet('big_data.parquet')

# 5. 웹 표시용 → HTML
df.to_html('table.html', index=False, table_id='data-table')
```

---

## 📋 종합 실습 예제

```python
def comprehensive_file_io_example():
    """
    판다스 파일 입출력 종합 예제
    """
    
    # 1. 데이터 생성
    print("1. 샘플 데이터 생성")
    data = {
        'product_id': range(1, 101),
        'product_name': [f'Product_{i}' for i in range(1, 101)],
        'price': np.random.randint(1000, 100000, 100),
        'category': np.random.choice(['A', 'B', 'C'], 100),
        'stock': np.random.randint(0, 50, 100)
    }
    df = pd.DataFrame(data)
    print(f"데이터 형태: {df.shape}")
    
    # 2. 다양한 형식으로 저장
    print("\n2. 다양한 형식으로 저장")
    
    # CSV (기본)
    df.to_csv('products.csv', index=False)
    
    # Excel (다중 시트)
    with pd.ExcelWriter('products_multi.xlsx') as writer:
        df.to_excel(writer, sheet_name='전체', index=False)
        df[df['category'] == 'A'].to_excel(writer, sheet_name='카테고리A', index=False)
        df[df['stock'] < 10].to_excel(writer, sheet_name='재고부족', index=False)
    
    # JSON (레코드 형태)
    df.to_json('products.json', orient='records', force_ascii=False)
    
    # 3. 파일 읽기 및 검증
    print("\n3. 파일 읽기 및 검증")
    
    df_csv = pd.read_csv('products.csv')
    df_excel = pd.read_excel('products_multi.xlsx', sheet_name='전체')
    df_json = pd.read_json('products.json')
    
    # 데이터 일치성 확인
    print(f"CSV 읽기: {df_csv.shape}")
    print(f"Excel 읽기: {df_excel.shape}")
    print(f"JSON 읽기: {df_json.shape}")
    
    # 4. 통계 정보 출력
    print("\n4. 통계 정보")
    print(df.groupby('category')['price'].agg(['mean', 'count', 'sum']))
    
    print("\n파일 입출력 예제 완료!")

# 실행
comprehensive_file_io_example()
```

---

## 🔍 문제 해결 가이드

### 자주 발생하는 문제와 해결책

#### 1. 인코딩 오류
```python
# 문제: UnicodeDecodeError
# 해결: 적절한 인코딩 지정
df = pd.read_csv('data.csv', encoding='utf-8-sig')
```

#### 2. 메모리 부족
```python
# 문제: MemoryError
# 해결: 청크 처리 또는 데이터 타입 최적화
for chunk in pd.read_csv('big_file.csv', chunksize=1000):
    process(chunk)
```

#### 3. 날짜 형식 문제
```python
# 문제: 날짜 컬럼이 문자열로 읽힘
# 해결: parse_dates 옵션 사용
df = pd.read_csv('data.csv', parse_dates=['date_column'])
```

#### 4. 구분자 문제
```python
# 문제: 데이터가 제대로 분리되지 않음
# 해결: 정확한 구분자 지정
df = pd.read_csv('data.txt', sep='\t')  # 탭 구분
df = pd.read_csv('data.txt', sep=r'\s+')  # 공백 구분
```

---

## 📚 추천 학습 자료

### 공식 문서
- [Pandas I/O Tools](https://pandas.pydata.org/docs/user_guide/io.html)
- [CSV 파일 처리 가이드](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)

### 실무 활용
- 대용량 데이터 처리 시 청크 단위 처리 활용
- 한글 데이터 처리 시 인코딩 주의
- 웹 스크래핑 시 HTML 테이블 읽기 기능 활용
- API 데이터 처리 시 JSON 형식 활용

---

## 🎯 핵심 요약

### 반드시 기억할 포인트

1. **파일 형식별 특징 이해**
   - CSV: 범용성, 가벼움
   - Excel: 비즈니스 친화적, 다중 시트
   - JSON: 웹 친화적, 구조적

2. **인코딩 처리**
   - 한글 파일: `encoding='utf-8-sig'` 또는 `encoding='cp949'`
   - 웹 데이터: `encoding='utf-8'`

3. **메모리 관리**
   - 대용량 파일: 청크 처리 활용
   - 데이터 타입 최적화로 메모리 절약

4. **실무 활용 패턴**
   - 파일 존재 여부 확인
   - 예외 처리 구현
   - 배치 처리 자동화

판다스의 파일 I/O 기능을 마스터하면 다양한 데이터 소스와의 연동이 가능해지며, 실무에서의 데이터 처리 효율성이 크게 향상됩니다! 💪: '전화번호 형식',
}
```

### 그룹화와 캡처

```python
# 그룹화 패턴
group_patterns = {
    r'(\d{4})-(\d{2})-(\d{2})': '날짜 그룹화',
    r'([a-zA-Z]+)@([a-zA-Z0-9.-]+)': '이메일 그룹화',
    r'(\d+)\.(\d+)\.(\d+)\.(\d+)': 'IP 주소 그룹화',
    r'(?P<year>\d{4})-(?P<month>\d{2})-(?P<day>\d{2})': '명명된 그룹',
}

# 그룹 사용 예시
log_entry = "2024-01-15 14:30:25 INFO: User login successful"
log_pattern = r'(?P<date>\d{4}-\d{2}-\d{2})\s+(?P<time>\d{2}:\d{2}:\d{2})\s+(?P<level>\w+):\s+(?P<message>.*)'
match = re.match(log_pattern, log_entry)

if match:
    print(f"날짜: {match.group('date')}")
    print(f"시간: {match.group('time')}")
    print(f"레벨: {match.group('level')}")
    print(f"메시지: {match.group('message')}")
```

### 판다스와 정규표현식 활용

#### 1. 데이터 클리닝
```python
# 문자열 컬럼에서 숫자만 추출
df['price_clean'] = df['price_text'].str.extract(r'(\d+)')
df['price_clean'] = df['price_clean'].astype(int)

# 전화번호 형식 통일
df['phone_clean'] = df['phone'].str.replace(r'[^\d]', '', regex=True)
df['phone_format'] = df['phone_clean'].str.replace(r'(\d{3})(\d{4})(\d{4})', r'\1-\2-\3', regex=True)

# 이메일 도메인 추출
df['domain'] = df['email'].str.extract(r'@([a-zA-Z0-9.-]+)')
```

#### 2. 데이터 검증
```python
# 이메일 형식 검증
email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}

---

## 📊 Excel 파일 처리

### 기본 Excel 읽기 및 저장

```python
# Excel 파일 읽기
df = pd.read_excel('data.xlsx')  # 첫 번째 시트
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')  # 특정 시트

# Excel 파일 저장
df.to_excel('output.xlsx', index=False)
```

### 다중 시트 처리

```python
# 여러 시트 동시 읽기
excel_dict = pd.read_excel('data.xlsx', sheet_name=None)  # 모든 시트
print(excel_dict.keys())  # 시트 이름들

# 특정 시트들만 읽기
sheets_dict = pd.read_excel('data.xlsx', sheet_name=['Sheet1', 'Sheet2'])

# 여러 시트로 저장
with pd.ExcelWriter('multi_sheet.xlsx') as writer:
    df1.to_excel(writer, sheet_name='데이터1', index=False)
    df2.to_excel(writer, sheet_name='데이터2', index=False)
```

### 실무 예제: 제품 데이터 Excel 처리

```python
# 제품 데이터 생성
product_data = {
    'name': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(product_data)

# Excel로 저장 (다양한 옵션)
df.to_excel('products.xlsx', 
           index=False,           # 인덱스 제외
           sheet_name='제품목록',  # 시트명 지정
           encoding='utf-8')      # 인코딩 지정

# Excel에서 읽기
df_read = pd.read_excel('products.xlsx', sheet_name='제품목록')
print(df_read)
```

---

## 🌐 JSON 파일 처리

### JSON 기본 처리

```python
# JSON 파일로 저장
df.to_json('data.json')
df.to_json('data.json', orient='records')  # 레코드 형태
df.to_json('data.json', orient='index')    # 인덱스 기반

# JSON 파일 읽기
df = pd.read_json('data.json')

# 문자열 JSON 처리
json_string = df.to_json(orient='records')
df_from_json = pd.read_json(json_string)
```

### JSON 형태별 저장 방식

```python
import pandas as pd

# 샘플 데이터
data = {'apple': {'count': 10, 'price': 1500},
        'orange': {'count': 4, 'price': 700}}
df = pd.DataFrame(data)

print("1. 기본 형태:")
print(df.to_json())

print("\n2. 레코드 형태 (웹 API에 적합):")
print(df.to_json(orient='records'))

print("\n3. 인덱스 형태:")
print(df.to_json(orient='index'))

print("\n4. 값만 저장:")
print(df.to_json(orient='values'))
```

---

## 🌍 HTML 및 웹 데이터 처리

### 웹 테이블 읽기

```python
# 웹 페이지에서 테이블 읽기
url = "https://ko.wikipedia.org/wiki/리눅스"
tables = pd.read_html(url, encoding='utf-8')
print(f"총 {len(tables)}개의 테이블 발견")

# 첫 번째 테이블 확인
if tables:
    first_table = tables[0]
    print(first_table.head())
```

### HTML로 저장

```python
# HTML 형태로 저장
html_string = df.to_html()
print(html_string)

# 파일로 저장
df.to_html('table.html', index=False)
```

---

## 📋 클립보드 활용

### 클립보드와 데이터 교환

```python
# 클립보드로 복사 (Excel에서 바로 붙여넣기 가능)
df.to_clipboard(index=False)
print("데이터가 클립보드에 복사되었습니다.")

# 클립보드에서 읽기 (Excel에서 복사한 데이터)
# df_from_clipboard = pd.read_clipboard()
```

---

## 🔄 대용량 데이터: 청크(Chunk) 처리

### 청크 처리 기본

```python
# 대용량 CSV 파일을 청크 단위로 처리
chunk_size = 1000
chunks = []

# 청크 단위로 읽어서 처리
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # 각 청크에 대한 처리
    processed_chunk = chunk[chunk['score'] > 80]  # 예시 필터링
    chunks.append(processed_chunk)

# 모든 청크 결합
final_df = pd.concat(chunks, ignore_index=True)
```

### 메모리 효율적인 처리

```python
import time

# 일반적인 방법 vs 청크 처리 성능 비교
def process_normal(filepath):
    start = time.time()
    df = pd.read_csv(filepath)
    result = df.groupby('category').mean()
    end = time.time()
    return result, end - start

def process_chunks(filepath, chunk_size=1000):
    start = time.time()
    results = []
    
    for chunk in pd.read_csv(filepath, chunksize=chunk_size):
        chunk_result = chunk.groupby('category').mean()
        results.append(chunk_result)
    
    final_result = pd.concat(results).groupby(level=0).mean()
    end = time.time()
    return final_result, end - start
```

**💡 청크 처리의 장단점**

**장점:**
- 메모리 사용량 절약
- 대용량 파일 처리 가능
- 스트리밍 방식 처리

**단점:**
- 처리 속도 약간 저하
- 복잡한 코드 구조
- 전체 데이터 기반 연산 어려움

---

## 🔧 고정폭 파일(FWF) 처리

### 고정폭 데이터 읽기

```python
# 고정폭 파일 읽기 예시
# 데이터 형태: 이름(10자) + 나이(3자) + 점수(5자)
df = pd.read_fwf('fixed_width.txt',
                 widths=[10, 3, 5],  # 각 컬럼의 폭
                 names=['name', 'age', 'score'],
                 encoding='utf-8')
print(df)
```

---

## 📊 다양한 저장 형식 비교

### 형식별 저장 예제

```python
# 샘플 데이터 생성
sample_data = {
    'product': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(sample_data)

# 1. CSV 저장 (구분자 옵션)
df.to_csv('data.csv', index=False, sep=',')
df.to_csv('data_tab.txt', index=False, sep='\t')  # 탭 구분

# 2. Excel 저장
df.to_excel('data.xlsx', index=False)

# 3. JSON 저장
df.to_json('data.json', orient='records', force_ascii=False)

# 4. 전치(Transpose) 저장
df_transposed = df.T  # 행과 열 바꾸기
df_transposed.to_csv('data_transposed.csv')
```

### 트랜스포즈(Transpose) 활용

```python
# 데이터 구조 변경
items = {
    'apple': {'count': 10, 'price': 1500},
    'orange': {'count': 4, 'price': 700}
}
df = pd.DataFrame(items)
print("원본 데이터:")
print(df)

# 행과 열 바꾸기
df_t = df.T
print("\n전치된 데이터:")
print(df_t)

# 전치된 데이터 저장
df_t.to_csv('transposed_data.csv')
```

---

## ⚙️ 실무 팁 및 모범 사례

### 1. 인코딩 문제 해결

```python
# 한글 파일 처리 시 권장 설정
encodings_to_try = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']

for encoding in encodings_to_try:
    try:
        df = pd.read_csv('korean_data.csv', encoding=encoding)
        print(f"성공: {encoding}")
        break
    except UnicodeDecodeError:
        print(f"실패: {encoding}")
        continue
```

### 2. 파일 존재 여부 확인

```python
import os

def safe_read_csv(filepath):
    if os.path.exists(filepath):
        return pd.read_csv(filepath)
    else:
        print(f"파일을 찾을 수 없습니다: {filepath}")
        return None

# 사용 예시
df = safe_read_csv('data.csv')
if df is not None:
    print(df.head())
```

### 3. 메모리 최적화 데이터 타입

```python
# 메모리 효율적인 데이터 타입 지정
efficient_dtypes = {
    'id': 'int32',      # int64 → int32
    'category': 'category',  # object → category
    'score': 'float32'  # float64 → float32
}

df = pd.read_csv('data.csv', dtype=efficient_dtypes)
print(f"메모리 사용량: {df.memory_usage(deep=True).sum()} bytes")
```

### 4. 배치 파일 처리

```python
import glob

# 여러 CSV 파일을 하나로 합치기
csv_files = glob.glob("data_*.csv")
dataframes = []

for file in csv_files:
    df_temp = pd.read_csv(file)
    df_temp['source_file'] = file  # 출처 파일명 추가
    dataframes.append(df_temp)

# 모든 파일 결합
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df.to_csv('combined_data.csv', index=False)
```

---

## 🎯 성능 비교 및 선택 가이드

### 파일 형식별 성능 특성

| 형식 | 읽기 속도 | 파일 크기 | 호환성 | 사용 사례 |
|------|-----------|-----------|--------|-----------|
| CSV | 빠름 | 보통 | 높음 | 범용 데이터 교환 |
| Excel | 보통 | 큼 | 보통 | 비즈니스 문서 |
| JSON | 보통 | 큼 | 높음 | 웹 API, 설정 파일 |
| Parquet | 매우 빠름 | 작음 | 낮음 | 빅데이터 분석 |

### 상황별 권장 형식

```python
# 상황별 권장 파일 형식

# 1. 범용 데이터 교환 → CSV
df.to_csv('data.csv', index=False)

# 2. 비즈니스 보고서 → Excel
df.to_excel('report.xlsx', index=False)

# 3. 웹 API 데이터 → JSON
df.to_json('api_data.json', orient='records')

# 4. 대용량 분석 데이터 → Parquet (별도 라이브러리 필요)
# df.to_parquet('big_data.parquet')

# 5. 웹 표시용 → HTML
df.to_html('table.html', index=False, table_id='data-table')
```

---

## 📋 종합 실습 예제

```python
def comprehensive_file_io_example():
    """
    판다스 파일 입출력 종합 예제
    """
    
    # 1. 데이터 생성
    print("1. 샘플 데이터 생성")
    data = {
        'product_id': range(1, 101),
        'product_name': [f'Product_{i}' for i in range(1, 101)],
        'price': np.random.randint(1000, 100000, 100),
        'category': np.random.choice(['A', 'B', 'C'], 100),
        'stock': np.random.randint(0, 50, 100)
    }
    df = pd.DataFrame(data)
    print(f"데이터 형태: {df.shape}")
    
    # 2. 다양한 형식으로 저장
    print("\n2. 다양한 형식으로 저장")
    
    # CSV (기본)
    df.to_csv('products.csv', index=False)
    
    # Excel (다중 시트)
    with pd.ExcelWriter('products_multi.xlsx') as writer:
        df.to_excel(writer, sheet_name='전체', index=False)
        df[df['category'] == 'A'].to_excel(writer, sheet_name='카테고리A', index=False)
        df[df['stock'] < 10].to_excel(writer, sheet_name='재고부족', index=False)
    
    # JSON (레코드 형태)
    df.to_json('products.json', orient='records', force_ascii=False)
    
    # 3. 파일 읽기 및 검증
    print("\n3. 파일 읽기 및 검증")
    
    df_csv = pd.read_csv('products.csv')
    df_excel = pd.read_excel('products_multi.xlsx', sheet_name='전체')
    df_json = pd.read_json('products.json')
    
    # 데이터 일치성 확인
    print(f"CSV 읽기: {df_csv.shape}")
    print(f"Excel 읽기: {df_excel.shape}")
    print(f"JSON 읽기: {df_json.shape}")
    
    # 4. 통계 정보 출력
    print("\n4. 통계 정보")
    print(df.groupby('category')['price'].agg(['mean', 'count', 'sum']))
    
    print("\n파일 입출력 예제 완료!")

# 실행
comprehensive_file_io_example()
```

---

## 🔍 문제 해결 가이드

### 자주 발생하는 문제와 해결책

#### 1. 인코딩 오류
```python
# 문제: UnicodeDecodeError
# 해결: 적절한 인코딩 지정
df = pd.read_csv('data.csv', encoding='utf-8-sig')
```

#### 2. 메모리 부족
```python
# 문제: MemoryError
# 해결: 청크 처리 또는 데이터 타입 최적화
for chunk in pd.read_csv('big_file.csv', chunksize=1000):
    process(chunk)
```

#### 3. 날짜 형식 문제
```python
# 문제: 날짜 컬럼이 문자열로 읽힘
# 해결: parse_dates 옵션 사용
df = pd.read_csv('data.csv', parse_dates=['date_column'])
```

#### 4. 구분자 문제
```python
# 문제: 데이터가 제대로 분리되지 않음
# 해결: 정확한 구분자 지정
df = pd.read_csv('data.txt', sep='\t')  # 탭 구분
df = pd.read_csv('data.txt', sep=r'\s+')  # 공백 구분
```

---

## 📚 추천 학습 자료

### 공식 문서
- [Pandas I/O Tools](https://pandas.pydata.org/docs/user_guide/io.html)
- [CSV 파일 처리 가이드](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)

### 실무 활용
- 대용량 데이터 처리 시 청크 단위 처리 활용
- 한글 데이터 처리 시 인코딩 주의
- 웹 스크래핑 시 HTML 테이블 읽기 기능 활용
- API 데이터 처리 시 JSON 형식 활용

---

## 🎯 핵심 요약

### 반드시 기억할 포인트

1. **파일 형식별 특징 이해**
   - CSV: 범용성, 가벼움
   - Excel: 비즈니스 친화적, 다중 시트
   - JSON: 웹 친화적, 구조적

2. **인코딩 처리**
   - 한글 파일: `encoding='utf-8-sig'` 또는 `encoding='cp949'`
   - 웹 데이터: `encoding='utf-8'`

3. **메모리 관리**
   - 대용량 파일: 청크 처리 활용
   - 데이터 타입 최적화로 메모리 절약

4. **실무 활용 패턴**
   - 파일 존재 여부 확인
   - 예외 처리 구현
   - 배치 처리 자동화

판다스의 파일 I/O 기능을 마스터하면 다양한 데이터 소스와의 연동이 가능해지며, 실무에서의 데이터 처리 효율성이 크게 향상됩니다! 💪
df['valid_email'] = df['email'].str.match(email_pattern)

# 날짜 형식 검증
date_pattern = r'^\d{4}-\d{2}-\d{2}

---

## 📊 Excel 파일 처리

### 기본 Excel 읽기 및 저장

```python
# Excel 파일 읽기
df = pd.read_excel('data.xlsx')  # 첫 번째 시트
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')  # 특정 시트

# Excel 파일 저장
df.to_excel('output.xlsx', index=False)
```

### 다중 시트 처리

```python
# 여러 시트 동시 읽기
excel_dict = pd.read_excel('data.xlsx', sheet_name=None)  # 모든 시트
print(excel_dict.keys())  # 시트 이름들

# 특정 시트들만 읽기
sheets_dict = pd.read_excel('data.xlsx', sheet_name=['Sheet1', 'Sheet2'])

# 여러 시트로 저장
with pd.ExcelWriter('multi_sheet.xlsx') as writer:
    df1.to_excel(writer, sheet_name='데이터1', index=False)
    df2.to_excel(writer, sheet_name='데이터2', index=False)
```

### 실무 예제: 제품 데이터 Excel 처리

```python
# 제품 데이터 생성
product_data = {
    'name': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(product_data)

# Excel로 저장 (다양한 옵션)
df.to_excel('products.xlsx', 
           index=False,           # 인덱스 제외
           sheet_name='제품목록',  # 시트명 지정
           encoding='utf-8')      # 인코딩 지정

# Excel에서 읽기
df_read = pd.read_excel('products.xlsx', sheet_name='제품목록')
print(df_read)
```

---

## 🌐 JSON 파일 처리

### JSON 기본 처리

```python
# JSON 파일로 저장
df.to_json('data.json')
df.to_json('data.json', orient='records')  # 레코드 형태
df.to_json('data.json', orient='index')    # 인덱스 기반

# JSON 파일 읽기
df = pd.read_json('data.json')

# 문자열 JSON 처리
json_string = df.to_json(orient='records')
df_from_json = pd.read_json(json_string)
```

### JSON 형태별 저장 방식

```python
import pandas as pd

# 샘플 데이터
data = {'apple': {'count': 10, 'price': 1500},
        'orange': {'count': 4, 'price': 700}}
df = pd.DataFrame(data)

print("1. 기본 형태:")
print(df.to_json())

print("\n2. 레코드 형태 (웹 API에 적합):")
print(df.to_json(orient='records'))

print("\n3. 인덱스 형태:")
print(df.to_json(orient='index'))

print("\n4. 값만 저장:")
print(df.to_json(orient='values'))
```

---

## 🌍 HTML 및 웹 데이터 처리

### 웹 테이블 읽기

```python
# 웹 페이지에서 테이블 읽기
url = "https://ko.wikipedia.org/wiki/리눅스"
tables = pd.read_html(url, encoding='utf-8')
print(f"총 {len(tables)}개의 테이블 발견")

# 첫 번째 테이블 확인
if tables:
    first_table = tables[0]
    print(first_table.head())
```

### HTML로 저장

```python
# HTML 형태로 저장
html_string = df.to_html()
print(html_string)

# 파일로 저장
df.to_html('table.html', index=False)
```

---

## 📋 클립보드 활용

### 클립보드와 데이터 교환

```python
# 클립보드로 복사 (Excel에서 바로 붙여넣기 가능)
df.to_clipboard(index=False)
print("데이터가 클립보드에 복사되었습니다.")

# 클립보드에서 읽기 (Excel에서 복사한 데이터)
# df_from_clipboard = pd.read_clipboard()
```

---

## 🔄 대용량 데이터: 청크(Chunk) 처리

### 청크 처리 기본

```python
# 대용량 CSV 파일을 청크 단위로 처리
chunk_size = 1000
chunks = []

# 청크 단위로 읽어서 처리
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # 각 청크에 대한 처리
    processed_chunk = chunk[chunk['score'] > 80]  # 예시 필터링
    chunks.append(processed_chunk)

# 모든 청크 결합
final_df = pd.concat(chunks, ignore_index=True)
```

### 메모리 효율적인 처리

```python
import time

# 일반적인 방법 vs 청크 처리 성능 비교
def process_normal(filepath):
    start = time.time()
    df = pd.read_csv(filepath)
    result = df.groupby('category').mean()
    end = time.time()
    return result, end - start

def process_chunks(filepath, chunk_size=1000):
    start = time.time()
    results = []
    
    for chunk in pd.read_csv(filepath, chunksize=chunk_size):
        chunk_result = chunk.groupby('category').mean()
        results.append(chunk_result)
    
    final_result = pd.concat(results).groupby(level=0).mean()
    end = time.time()
    return final_result, end - start
```

**💡 청크 처리의 장단점**

**장점:**
- 메모리 사용량 절약
- 대용량 파일 처리 가능
- 스트리밍 방식 처리

**단점:**
- 처리 속도 약간 저하
- 복잡한 코드 구조
- 전체 데이터 기반 연산 어려움

---

## 🔧 고정폭 파일(FWF) 처리

### 고정폭 데이터 읽기

```python
# 고정폭 파일 읽기 예시
# 데이터 형태: 이름(10자) + 나이(3자) + 점수(5자)
df = pd.read_fwf('fixed_width.txt',
                 widths=[10, 3, 5],  # 각 컬럼의 폭
                 names=['name', 'age', 'score'],
                 encoding='utf-8')
print(df)
```

---

## 📊 다양한 저장 형식 비교

### 형식별 저장 예제

```python
# 샘플 데이터 생성
sample_data = {
    'product': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(sample_data)

# 1. CSV 저장 (구분자 옵션)
df.to_csv('data.csv', index=False, sep=',')
df.to_csv('data_tab.txt', index=False, sep='\t')  # 탭 구분

# 2. Excel 저장
df.to_excel('data.xlsx', index=False)

# 3. JSON 저장
df.to_json('data.json', orient='records', force_ascii=False)

# 4. 전치(Transpose) 저장
df_transposed = df.T  # 행과 열 바꾸기
df_transposed.to_csv('data_transposed.csv')
```

### 트랜스포즈(Transpose) 활용

```python
# 데이터 구조 변경
items = {
    'apple': {'count': 10, 'price': 1500},
    'orange': {'count': 4, 'price': 700}
}
df = pd.DataFrame(items)
print("원본 데이터:")
print(df)

# 행과 열 바꾸기
df_t = df.T
print("\n전치된 데이터:")
print(df_t)

# 전치된 데이터 저장
df_t.to_csv('transposed_data.csv')
```

---

## ⚙️ 실무 팁 및 모범 사례

### 1. 인코딩 문제 해결

```python
# 한글 파일 처리 시 권장 설정
encodings_to_try = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']

for encoding in encodings_to_try:
    try:
        df = pd.read_csv('korean_data.csv', encoding=encoding)
        print(f"성공: {encoding}")
        break
    except UnicodeDecodeError:
        print(f"실패: {encoding}")
        continue
```

### 2. 파일 존재 여부 확인

```python
import os

def safe_read_csv(filepath):
    if os.path.exists(filepath):
        return pd.read_csv(filepath)
    else:
        print(f"파일을 찾을 수 없습니다: {filepath}")
        return None

# 사용 예시
df = safe_read_csv('data.csv')
if df is not None:
    print(df.head())
```

### 3. 메모리 최적화 데이터 타입

```python
# 메모리 효율적인 데이터 타입 지정
efficient_dtypes = {
    'id': 'int32',      # int64 → int32
    'category': 'category',  # object → category
    'score': 'float32'  # float64 → float32
}

df = pd.read_csv('data.csv', dtype=efficient_dtypes)
print(f"메모리 사용량: {df.memory_usage(deep=True).sum()} bytes")
```

### 4. 배치 파일 처리

```python
import glob

# 여러 CSV 파일을 하나로 합치기
csv_files = glob.glob("data_*.csv")
dataframes = []

for file in csv_files:
    df_temp = pd.read_csv(file)
    df_temp['source_file'] = file  # 출처 파일명 추가
    dataframes.append(df_temp)

# 모든 파일 결합
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df.to_csv('combined_data.csv', index=False)
```

---

## 🎯 성능 비교 및 선택 가이드

### 파일 형식별 성능 특성

| 형식 | 읽기 속도 | 파일 크기 | 호환성 | 사용 사례 |
|------|-----------|-----------|--------|-----------|
| CSV | 빠름 | 보통 | 높음 | 범용 데이터 교환 |
| Excel | 보통 | 큼 | 보통 | 비즈니스 문서 |
| JSON | 보통 | 큼 | 높음 | 웹 API, 설정 파일 |
| Parquet | 매우 빠름 | 작음 | 낮음 | 빅데이터 분석 |

### 상황별 권장 형식

```python
# 상황별 권장 파일 형식

# 1. 범용 데이터 교환 → CSV
df.to_csv('data.csv', index=False)

# 2. 비즈니스 보고서 → Excel
df.to_excel('report.xlsx', index=False)

# 3. 웹 API 데이터 → JSON
df.to_json('api_data.json', orient='records')

# 4. 대용량 분석 데이터 → Parquet (별도 라이브러리 필요)
# df.to_parquet('big_data.parquet')

# 5. 웹 표시용 → HTML
df.to_html('table.html', index=False, table_id='data-table')
```

---

## 📋 종합 실습 예제

```python
def comprehensive_file_io_example():
    """
    판다스 파일 입출력 종합 예제
    """
    
    # 1. 데이터 생성
    print("1. 샘플 데이터 생성")
    data = {
        'product_id': range(1, 101),
        'product_name': [f'Product_{i}' for i in range(1, 101)],
        'price': np.random.randint(1000, 100000, 100),
        'category': np.random.choice(['A', 'B', 'C'], 100),
        'stock': np.random.randint(0, 50, 100)
    }
    df = pd.DataFrame(data)
    print(f"데이터 형태: {df.shape}")
    
    # 2. 다양한 형식으로 저장
    print("\n2. 다양한 형식으로 저장")
    
    # CSV (기본)
    df.to_csv('products.csv', index=False)
    
    # Excel (다중 시트)
    with pd.ExcelWriter('products_multi.xlsx') as writer:
        df.to_excel(writer, sheet_name='전체', index=False)
        df[df['category'] == 'A'].to_excel(writer, sheet_name='카테고리A', index=False)
        df[df['stock'] < 10].to_excel(writer, sheet_name='재고부족', index=False)
    
    # JSON (레코드 형태)
    df.to_json('products.json', orient='records', force_ascii=False)
    
    # 3. 파일 읽기 및 검증
    print("\n3. 파일 읽기 및 검증")
    
    df_csv = pd.read_csv('products.csv')
    df_excel = pd.read_excel('products_multi.xlsx', sheet_name='전체')
    df_json = pd.read_json('products.json')
    
    # 데이터 일치성 확인
    print(f"CSV 읽기: {df_csv.shape}")
    print(f"Excel 읽기: {df_excel.shape}")
    print(f"JSON 읽기: {df_json.shape}")
    
    # 4. 통계 정보 출력
    print("\n4. 통계 정보")
    print(df.groupby('category')['price'].agg(['mean', 'count', 'sum']))
    
    print("\n파일 입출력 예제 완료!")

# 실행
comprehensive_file_io_example()
```

---

## 🔍 문제 해결 가이드

### 자주 발생하는 문제와 해결책

#### 1. 인코딩 오류
```python
# 문제: UnicodeDecodeError
# 해결: 적절한 인코딩 지정
df = pd.read_csv('data.csv', encoding='utf-8-sig')
```

#### 2. 메모리 부족
```python
# 문제: MemoryError
# 해결: 청크 처리 또는 데이터 타입 최적화
for chunk in pd.read_csv('big_file.csv', chunksize=1000):
    process(chunk)
```

#### 3. 날짜 형식 문제
```python
# 문제: 날짜 컬럼이 문자열로 읽힘
# 해결: parse_dates 옵션 사용
df = pd.read_csv('data.csv', parse_dates=['date_column'])
```

#### 4. 구분자 문제
```python
# 문제: 데이터가 제대로 분리되지 않음
# 해결: 정확한 구분자 지정
df = pd.read_csv('data.txt', sep='\t')  # 탭 구분
df = pd.read_csv('data.txt', sep=r'\s+')  # 공백 구분
```

---

## 📚 추천 학습 자료

### 공식 문서
- [Pandas I/O Tools](https://pandas.pydata.org/docs/user_guide/io.html)
- [CSV 파일 처리 가이드](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)

### 실무 활용
- 대용량 데이터 처리 시 청크 단위 처리 활용
- 한글 데이터 처리 시 인코딩 주의
- 웹 스크래핑 시 HTML 테이블 읽기 기능 활용
- API 데이터 처리 시 JSON 형식 활용

---

## 🎯 핵심 요약

### 반드시 기억할 포인트

1. **파일 형식별 특징 이해**
   - CSV: 범용성, 가벼움
   - Excel: 비즈니스 친화적, 다중 시트
   - JSON: 웹 친화적, 구조적

2. **인코딩 처리**
   - 한글 파일: `encoding='utf-8-sig'` 또는 `encoding='cp949'`
   - 웹 데이터: `encoding='utf-8'`

3. **메모리 관리**
   - 대용량 파일: 청크 처리 활용
   - 데이터 타입 최적화로 메모리 절약

4. **실무 활용 패턴**
   - 파일 존재 여부 확인
   - 예외 처리 구현
   - 배치 처리 자동화

판다스의 파일 I/O 기능을 마스터하면 다양한 데이터 소스와의 연동이 가능해지며, 실무에서의 데이터 처리 효율성이 크게 향상됩니다! 💪
df['valid_date'] = df['date_string'].str.match(date_pattern)

# 잘못된 형식 필터링
invalid_emails = df[~df['valid_email']]
```

#### 3. 데이터 분할
```python
# 이름을 성과 이름으로 분할
df[['first_name', 'last_name']] = df['full_name'].str.extract(r'(\w+)\s+(\w+)')

# 주소에서 우편번호 추출
df['zipcode'] = df['address'].str.extract(r'(\d{5})')

# 제품코드에서 카테고리와 번호 분리
df[['category', 'number']] = df['product_code'].str.extract(r'([A-Z]+)(\d+)')
```

### 고급 정규표현식 기법

#### 1. 전방탐색(Lookahead)과 후방탐색(Lookbehind)
```python
advanced_patterns = {
    r'(?=.*\d)(?=.*[a-z])(?=.*[A-Z]).{8,}': '강력한 비밀번호 (대소문자+숫자, 8자 이상)',
    r'\d+(?=원)': '원 앞의 숫자',
    r'(?<=\$)\d+': '달러 기호 뒤의 숫자',
    r'\b\w+(?=@gmail\.com)': 'gmail 앞의 사용자명',
}

# 사용 예시
text = "비밀번호: MyPass123, 가격: 15000원, 급여: $3000"
price_won = re.findall(r'\d+(?=원)', text)      # ['15000']
salary_dollar = re.findall(r'(?<=\$)\d+', text) # ['3000']
```

#### 2. 탐욕적(Greedy) vs 비탐욕적(Non-greedy) 매칭
```python
text = "<div>첫 번째</div><div>두 번째</div>"

greedy = re.findall(r'<div>.*</div>', text)      # ['<div>첫 번째</div><div>두 번째</div>']
non_greedy = re.findall(r'<div>.*?</div>', text) # ['<div>첫 번째</div>', '<div>두 번째</div>']
```

### 실무 정규표현식 패턴 라이브러리

```python
class RegexPatterns:
    """실무에서 자주 사용하는 정규표현식 패턴 모음"""
    
    # 기본 데이터 타입
    INTEGER = r'-?\d+'
    FLOAT = r'-?\d+\.?\d*'
    PERCENTAGE = r'-?\d+\.?\d*%'
    
    # 날짜/시간
    DATE_YYYY_MM_DD = r'\d{4}-\d{2}-\d{2}'
    DATE_MM_DD_YYYY = r'\d{2}/\d{2}/\d{4}'
    TIME_HH_MM_SS = r'\d{2}:\d{2}:\d{2}'
    DATETIME_ISO = r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}'
    
    # 연락처 정보
    EMAIL = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
    PHONE_KR = r'\d{2,3}-\d{3,4}-\d{4}'
    URL = r'https?://[^\s]+'
    
    # 식별자
    UUID = r'[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}'
    IP_ADDRESS = r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'
    MAC_ADDRESS = r'[0-9a-fA-F]{2}:[0-9a-fA-F]{2}:[0-9a-fA-F]{2}:[0-9a-fA-F]{2}:[0-9a-fA-F]{2}:[0-9a-fA-F]{2}'
    
    # 금융/화폐
    CURRENCY_KRW = r'\d{1,3}(,\d{3})*원?'
    CURRENCY_USD = r'\$\d{1,3}(,\d{3})*\.?\d{0,2}'
    CREDIT_CARD = r'\d{4}-?\d{4}-?\d{4}-?\d{4}'
    
    # 구분자
    CSV_DELIMITER = r','
    TSV_DELIMITER = r'\t'
    MULTI_DELIMITER = r'[,;\t|]'
    WHITESPACE = r'\s+'
    WHITESPACE_OR_COMMA = r'[,\s]+'

# 사용 예시
def clean_phone_numbers(df, column_name):
    """전화번호 컬럼 정리"""
    # 숫자만 추출
    df[f'{column_name}_digits'] = df[column_name].str.replace(r'[^\d]', '', regex=True)
    
    # 한국 전화번호 형식으로 변환
    df[f'{column_name}_formatted'] = df[f'{column_name}_digits'].str.replace(
        r'(\d{3})(\d{4})(\d{4})', r'\1-\2-\3', regex=True
    )
    
    # 유효성 검증
    df[f'{column_name}_valid'] = df[f'{column_name}_formatted'].str.match(RegexPatterns.PHONE_KR)
    
    return df

# 이메일 도메인별 통계
def analyze_email_domains(df, email_column):
    """이메일 도메인별 분석"""
    df['domain'] = df[email_column].str.extract(f'@([a-zA-Z0-9.-]+)')
    domain_stats = df['domain'].value_counts()
    return domain_stats
```

### 성능 최적화 팁

```python
import re
import time

# 1. 컴파일된 패턴 사용 (반복 사용 시 성능 향상)
compiled_email = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}')

def extract_emails_compiled(text_list):
    return [compiled_email.findall(text) for text in text_list]

# 2. 판다스 벡터화 연산 활용
def clean_data_vectorized(df):
    # ✅ 벡터화된 연산 (빠름)
    df['clean_phone'] = df['phone'].str.replace(r'[^\d]', '', regex=True)
    
    # ❌ 반복문 사용 (느림)
    # for idx, row in df.iterrows():
    #     df.loc[idx, 'clean_phone'] = re.sub(r'[^\d]', '', row['phone'])

# 3. 필요한 경우에만 정규표현식 사용
def smart_cleaning(df):
    # 간단한 경우: 문자열 메서드 사용
    df['upper_name'] = df['name'].str.upper()
    
    # 복잡한 경우: 정규표현식 사용
    df['extracted_numbers'] = df['mixed_text'].str.extract(r'(\d+)')
```

---

## 📊 Excel 파일 처리

### 기본 Excel 읽기 및 저장

```python
# Excel 파일 읽기
df = pd.read_excel('data.xlsx')  # 첫 번째 시트
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')  # 특정 시트

# Excel 파일 저장
df.to_excel('output.xlsx', index=False)
```

### 다중 시트 처리

```python
# 여러 시트 동시 읽기
excel_dict = pd.read_excel('data.xlsx', sheet_name=None)  # 모든 시트
print(excel_dict.keys())  # 시트 이름들

# 특정 시트들만 읽기
sheets_dict = pd.read_excel('data.xlsx', sheet_name=['Sheet1', 'Sheet2'])

# 여러 시트로 저장
with pd.ExcelWriter('multi_sheet.xlsx') as writer:
    df1.to_excel(writer, sheet_name='데이터1', index=False)
    df2.to_excel(writer, sheet_name='데이터2', index=False)
```

### 실무 예제: 제품 데이터 Excel 처리

```python
# 제품 데이터 생성
product_data = {
    'name': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(product_data)

# Excel로 저장 (다양한 옵션)
df.to_excel('products.xlsx', 
           index=False,           # 인덱스 제외
           sheet_name='제품목록',  # 시트명 지정
           encoding='utf-8')      # 인코딩 지정

# Excel에서 읽기
df_read = pd.read_excel('products.xlsx', sheet_name='제품목록')
print(df_read)
```

---

## 🌐 JSON 파일 처리

### JSON 기본 처리

```python
# JSON 파일로 저장
df.to_json('data.json')
df.to_json('data.json', orient='records')  # 레코드 형태
df.to_json('data.json', orient='index')    # 인덱스 기반

# JSON 파일 읽기
df = pd.read_json('data.json')

# 문자열 JSON 처리
json_string = df.to_json(orient='records')
df_from_json = pd.read_json(json_string)
```

### JSON 형태별 저장 방식

```python
import pandas as pd

# 샘플 데이터
data = {'apple': {'count': 10, 'price': 1500},
        'orange': {'count': 4, 'price': 700}}
df = pd.DataFrame(data)

print("1. 기본 형태:")
print(df.to_json())

print("\n2. 레코드 형태 (웹 API에 적합):")
print(df.to_json(orient='records'))

print("\n3. 인덱스 형태:")
print(df.to_json(orient='index'))

print("\n4. 값만 저장:")
print(df.to_json(orient='values'))
```

---

## 🌍 HTML 및 웹 데이터 처리

### 웹 테이블 읽기

```python
# 웹 페이지에서 테이블 읽기
url = "https://ko.wikipedia.org/wiki/리눅스"
tables = pd.read_html(url, encoding='utf-8')
print(f"총 {len(tables)}개의 테이블 발견")

# 첫 번째 테이블 확인
if tables:
    first_table = tables[0]
    print(first_table.head())
```

### HTML로 저장

```python
# HTML 형태로 저장
html_string = df.to_html()
print(html_string)

# 파일로 저장
df.to_html('table.html', index=False)
```

---

## 📋 클립보드 활용

### 클립보드와 데이터 교환

```python
# 클립보드로 복사 (Excel에서 바로 붙여넣기 가능)
df.to_clipboard(index=False)
print("데이터가 클립보드에 복사되었습니다.")

# 클립보드에서 읽기 (Excel에서 복사한 데이터)
# df_from_clipboard = pd.read_clipboard()
```

---

## 🔄 대용량 데이터: 청크(Chunk) 처리

### 청크 처리 기본

```python
# 대용량 CSV 파일을 청크 단위로 처리
chunk_size = 1000
chunks = []

# 청크 단위로 읽어서 처리
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # 각 청크에 대한 처리
    processed_chunk = chunk[chunk['score'] > 80]  # 예시 필터링
    chunks.append(processed_chunk)

# 모든 청크 결합
final_df = pd.concat(chunks, ignore_index=True)
```

### 메모리 효율적인 처리

```python
import time

# 일반적인 방법 vs 청크 처리 성능 비교
def process_normal(filepath):
    start = time.time()
    df = pd.read_csv(filepath)
    result = df.groupby('category').mean()
    end = time.time()
    return result, end - start

def process_chunks(filepath, chunk_size=1000):
    start = time.time()
    results = []
    
    for chunk in pd.read_csv(filepath, chunksize=chunk_size):
        chunk_result = chunk.groupby('category').mean()
        results.append(chunk_result)
    
    final_result = pd.concat(results).groupby(level=0).mean()
    end = time.time()
    return final_result, end - start
```

**💡 청크 처리의 장단점**

**장점:**
- 메모리 사용량 절약
- 대용량 파일 처리 가능
- 스트리밍 방식 처리

**단점:**
- 처리 속도 약간 저하
- 복잡한 코드 구조
- 전체 데이터 기반 연산 어려움

---

## 🔧 고정폭 파일(FWF) 처리

### 고정폭 데이터 읽기

```python
# 고정폭 파일 읽기 예시
# 데이터 형태: 이름(10자) + 나이(3자) + 점수(5자)
df = pd.read_fwf('fixed_width.txt',
                 widths=[10, 3, 5],  # 각 컬럼의 폭
                 names=['name', 'age', 'score'],
                 encoding='utf-8')
print(df)
```

---

## 📊 다양한 저장 형식 비교

### 형식별 저장 예제

```python
# 샘플 데이터 생성
sample_data = {
    'product': ['Mouse', 'Keyboard', 'Monitor'],
    'price': [25000, 45000, 250000],
    'stock': [15, 8, 3]
}
df = pd.DataFrame(sample_data)

# 1. CSV 저장 (구분자 옵션)
df.to_csv('data.csv', index=False, sep=',')
df.to_csv('data_tab.txt', index=False, sep='\t')  # 탭 구분

# 2. Excel 저장
df.to_excel('data.xlsx', index=False)

# 3. JSON 저장
df.to_json('data.json', orient='records', force_ascii=False)

# 4. 전치(Transpose) 저장
df_transposed = df.T  # 행과 열 바꾸기
df_transposed.to_csv('data_transposed.csv')
```

### 트랜스포즈(Transpose) 활용

```python
# 데이터 구조 변경
items = {
    'apple': {'count': 10, 'price': 1500},
    'orange': {'count': 4, 'price': 700}
}
df = pd.DataFrame(items)
print("원본 데이터:")
print(df)

# 행과 열 바꾸기
df_t = df.T
print("\n전치된 데이터:")
print(df_t)

# 전치된 데이터 저장
df_t.to_csv('transposed_data.csv')
```

---

## ⚙️ 실무 팁 및 모범 사례

### 1. 인코딩 문제 해결

```python
# 한글 파일 처리 시 권장 설정
encodings_to_try = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']

for encoding in encodings_to_try:
    try:
        df = pd.read_csv('korean_data.csv', encoding=encoding)
        print(f"성공: {encoding}")
        break
    except UnicodeDecodeError:
        print(f"실패: {encoding}")
        continue
```

### 2. 파일 존재 여부 확인

```python
import os

def safe_read_csv(filepath):
    if os.path.exists(filepath):
        return pd.read_csv(filepath)
    else:
        print(f"파일을 찾을 수 없습니다: {filepath}")
        return None

# 사용 예시
df = safe_read_csv('data.csv')
if df is not None:
    print(df.head())
```

### 3. 메모리 최적화 데이터 타입

```python
# 메모리 효율적인 데이터 타입 지정
efficient_dtypes = {
    'id': 'int32',      # int64 → int32
    'category': 'category',  # object → category
    'score': 'float32'  # float64 → float32
}

df = pd.read_csv('data.csv', dtype=efficient_dtypes)
print(f"메모리 사용량: {df.memory_usage(deep=True).sum()} bytes")
```

### 4. 배치 파일 처리

```python
import glob

# 여러 CSV 파일을 하나로 합치기
csv_files = glob.glob("data_*.csv")
dataframes = []

for file in csv_files:
    df_temp = pd.read_csv(file)
    df_temp['source_file'] = file  # 출처 파일명 추가
    dataframes.append(df_temp)

# 모든 파일 결합
combined_df = pd.concat(dataframes, ignore_index=True)
combined_df.to_csv('combined_data.csv', index=False)
```

---

## 🎯 성능 비교 및 선택 가이드

### 파일 형식별 성능 특성

| 형식 | 읽기 속도 | 파일 크기 | 호환성 | 사용 사례 |
|------|-----------|-----------|--------|-----------|
| CSV | 빠름 | 보통 | 높음 | 범용 데이터 교환 |
| Excel | 보통 | 큼 | 보통 | 비즈니스 문서 |
| JSON | 보통 | 큼 | 높음 | 웹 API, 설정 파일 |
| Parquet | 매우 빠름 | 작음 | 낮음 | 빅데이터 분석 |

### 상황별 권장 형식

```python
# 상황별 권장 파일 형식

# 1. 범용 데이터 교환 → CSV
df.to_csv('data.csv', index=False)

# 2. 비즈니스 보고서 → Excel
df.to_excel('report.xlsx', index=False)

# 3. 웹 API 데이터 → JSON
df.to_json('api_data.json', orient='records')

# 4. 대용량 분석 데이터 → Parquet (별도 라이브러리 필요)
# df.to_parquet('big_data.parquet')

# 5. 웹 표시용 → HTML
df.to_html('table.html', index=False, table_id='data-table')
```

---

## 📋 종합 실습 예제

```python
def comprehensive_file_io_example():
    """
    판다스 파일 입출력 종합 예제
    """
    
    # 1. 데이터 생성
    print("1. 샘플 데이터 생성")
    data = {
        'product_id': range(1, 101),
        'product_name': [f'Product_{i}' for i in range(1, 101)],
        'price': np.random.randint(1000, 100000, 100),
        'category': np.random.choice(['A', 'B', 'C'], 100),
        'stock': np.random.randint(0, 50, 100)
    }
    df = pd.DataFrame(data)
    print(f"데이터 형태: {df.shape}")
    
    # 2. 다양한 형식으로 저장
    print("\n2. 다양한 형식으로 저장")
    
    # CSV (기본)
    df.to_csv('products.csv', index=False)
    
    # Excel (다중 시트)
    with pd.ExcelWriter('products_multi.xlsx') as writer:
        df.to_excel(writer, sheet_name='전체', index=False)
        df[df['category'] == 'A'].to_excel(writer, sheet_name='카테고리A', index=False)
        df[df['stock'] < 10].to_excel(writer, sheet_name='재고부족', index=False)
    
    # JSON (레코드 형태)
    df.to_json('products.json', orient='records', force_ascii=False)
    
    # 3. 파일 읽기 및 검증
    print("\n3. 파일 읽기 및 검증")
    
    df_csv = pd.read_csv('products.csv')
    df_excel = pd.read_excel('products_multi.xlsx', sheet_name='전체')
    df_json = pd.read_json('products.json')
    
    # 데이터 일치성 확인
    print(f"CSV 읽기: {df_csv.shape}")
    print(f"Excel 읽기: {df_excel.shape}")
    print(f"JSON 읽기: {df_json.shape}")
    
    # 4. 통계 정보 출력
    print("\n4. 통계 정보")
    print(df.groupby('category')['price'].agg(['mean', 'count', 'sum']))
    
    print("\n파일 입출력 예제 완료!")

# 실행
comprehensive_file_io_example()
```

---

## 🔍 문제 해결 가이드

### 자주 발생하는 문제와 해결책

#### 1. 인코딩 오류
```python
# 문제: UnicodeDecodeError
# 해결: 적절한 인코딩 지정
df = pd.read_csv('data.csv', encoding='utf-8-sig')
```

#### 2. 메모리 부족
```python
# 문제: MemoryError
# 해결: 청크 처리 또는 데이터 타입 최적화
for chunk in pd.read_csv('big_file.csv', chunksize=1000):
    process(chunk)
```

#### 3. 날짜 형식 문제
```python
# 문제: 날짜 컬럼이 문자열로 읽힘
# 해결: parse_dates 옵션 사용
df = pd.read_csv('data.csv', parse_dates=['date_column'])
```

#### 4. 구분자 문제
```python
# 문제: 데이터가 제대로 분리되지 않음
# 해결: 정확한 구분자 지정
df = pd.read_csv('data.txt', sep='\t')  # 탭 구분
df = pd.read_csv('data.txt', sep=r'\s+')  # 공백 구분
```

---

## 📚 추천 학습 자료

### 공식 문서
- [Pandas I/O Tools](https://pandas.pydata.org/docs/user_guide/io.html)
- [CSV 파일 처리 가이드](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)

### 실무 활용
- 대용량 데이터 처리 시 청크 단위 처리 활용
- 한글 데이터 처리 시 인코딩 주의
- 웹 스크래핑 시 HTML 테이블 읽기 기능 활용
- API 데이터 처리 시 JSON 형식 활용

---

## 🎯 핵심 요약

### 반드시 기억할 포인트

1. **파일 형식별 특징 이해**
   - CSV: 범용성, 가벼움
   - Excel: 비즈니스 친화적, 다중 시트
   - JSON: 웹 친화적, 구조적

2. **인코딩 처리**
   - 한글 파일: `encoding='utf-8-sig'` 또는 `encoding='cp949'`
   - 웹 데이터: `encoding='utf-8'`

3. **메모리 관리**
   - 대용량 파일: 청크 처리 활용
   - 데이터 타입 최적화로 메모리 절약

4. **실무 활용 패턴**
   - 파일 존재 여부 확인
   - 예외 처리 구현
   - 배치 처리 자동화

판다스의 파일 I/O 기능을 마스터하면 다양한 데이터 소스와의 연동이 가능해지며, 실무에서의 데이터 처리 효율성이 크게 향상됩니다! 💪