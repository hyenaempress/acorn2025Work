# NumPy ë¡œê·¸ ë³€í™˜, ì •ê·œí™”ì™€ ê²½ì‚¬í•˜ê°•ë²• ì™„ì „ ê°€ì´ë“œ

## ê°œìš”

NumPyë¥¼ ì‚¬ìš©í•œ ë¡œê·¸ ë³€í™˜, ì •ê·œí™”, í‘œì¤€í™”ì— ëŒ€í•œ ì™„ì „ ê°€ì´ë“œì…ë‹ˆë‹¤. íŠ¹íˆ **ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)**ê³¼ì˜ ì—°ê´€ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## 1. NumPy ì¶œë ¥ ì˜µì…˜ ì„¤ì •

### 1.1 ê¸°ë³¸ ì¶œë ¥ ì˜µì…˜
```python
import numpy as np
import matplotlib.pyplot as plt

# ì¶œë ¥ ì˜µì…˜ ì„¤ì •
np.set_printoptions(suppress=True, precision=6)
# suppress=True: ê³¼í•™ì  í‘œê¸°ë²• ì–µì œ (1e-5 â†’ 0.00001)
# precision=6: ì†Œìˆ˜ì  ì´í•˜ 6ìë¦¬ê¹Œì§€ í‘œì‹œ
```

### 1.2 ë‹¤ì–‘í•œ ì¶œë ¥ ì˜µì…˜ë“¤
```python
# ìƒì„¸í•œ ì¶œë ¥ ì˜µì…˜ ì„¤ì •
np.set_printoptions(
    suppress=True,        # ê³¼í•™ì  í‘œê¸°ë²• ì–µì œ
    precision=4,          # ì†Œìˆ˜ì  ì´í•˜ ìë¦¿ìˆ˜
    threshold=10,         # ìƒëµí•˜ì§€ ì•Šê³  ì¶œë ¥í•  ìµœëŒ€ ìš”ì†Œ ìˆ˜
    linewidth=120,        # í•œ ì¤„ ìµœëŒ€ ë¬¸ì ìˆ˜
    edgeitems=3          # ë°°ì—´ ì‹œì‘/ëì—ì„œ ì¶œë ¥í•  ìš”ì†Œ ìˆ˜
)

# ì¶œë ¥ ì˜ˆì‹œ
large_array = np.random.randn(20)
print("í° ë°°ì—´ ì¶œë ¥:", large_array)

# ì›ë˜ ì„¤ì •ìœ¼ë¡œ ë³µì›
np.set_printoptions()  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µì›
```

## 2. ê²½ì‚¬í•˜ê°•ë²•ê³¼ ë°ì´í„° ì „ì²˜ë¦¬ì˜ ê´€ê³„

### 2.1 ê²½ì‚¬í•˜ê°•ë²•ì´ë€?
```python
def gradient_descent_concept():
    """ê²½ì‚¬í•˜ê°•ë²• ê¸°ë³¸ ê°œë…"""
    print("=== ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent) ê°œë… ===")
    print("ëª©ì : ì†ì‹¤ í•¨ìˆ˜(Loss Function)ì˜ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜")
    print()
    print("ê¸°ë³¸ ì›ë¦¬:")
    print("1. í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê¸°ìš¸ê¸°(gradient) ê³„ì‚°")
    print("2. ê¸°ìš¸ê¸° ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì¡°ê¸ˆì”© ì´ë™")
    print("3. ìµœì†Ÿê°’ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ")
    print()
    print("ìˆ˜ì‹: Î¸_new = Î¸_old - Î± Ã— âˆ‡J(Î¸)")
    print("  Î¸: íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜)")
    print("  Î±: í•™ìŠµë¥ (learning rate)")
    print("  âˆ‡J(Î¸): ì†ì‹¤í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°")

gradient_descent_concept()
```

### 2.2 ì „ì²˜ë¦¬ê°€ ê²½ì‚¬í•˜ê°•ë²•ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
```python
def preprocessing_impact_on_gradient_descent():
    """ì „ì²˜ë¦¬ê°€ ê²½ì‚¬í•˜ê°•ë²•ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì‹œì—°"""
    print("=== ì „ì²˜ë¦¬ê°€ ê²½ì‚¬í•˜ê°•ë²•ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ===")
    
    # 1. ìŠ¤ì¼€ì¼ ì°¨ì´ê°€ í° ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 100
    
    # íŠ¹ì„± 1: ë‚˜ì´ (20-60)
    age = np.random.uniform(20, 60, n_samples)
    # íŠ¹ì„± 2: ì—°ë´‰ (2000-8000ë§Œì›) - ìŠ¤ì¼€ì¼ì´ ë§¤ìš° ë‹¤ë¦„!
    salary = np.random.uniform(2000, 8000, n_samples)
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ (ì§‘ê°’: ë‚˜ì´ì™€ ì—°ë´‰ì— ë¹„ë¡€)
    target = 0.5 * age + 0.001 * salary + np.random.normal(0, 2, n_samples)
    
    print("ì›ë³¸ ë°ì´í„° ìŠ¤ì¼€ì¼:")
    print(f"ë‚˜ì´ ë²”ìœ„: {age.min():.1f} ~ {age.max():.1f}")
    print(f"ì—°ë´‰ ë²”ìœ„: {salary.min():.0f} ~ {salary.max():.0f}")
    print(f"ìŠ¤ì¼€ì¼ ë¹„ìœ¨: {salary.max()/age.max():.0f}:1")
    
    return age, salary, target

def demonstrate_gradient_descent_without_scaling():
    """ìŠ¤ì¼€ì¼ë§ ì—†ëŠ” ê²½ì‚¬í•˜ê°•ë²• ë¬¸ì œì """
    age, salary, target = preprocessing_impact_on_gradient_descent()
    
    # íŠ¹ì„± í–‰ë ¬ êµ¬ì„± (bias í¬í•¨)
    X_raw = np.column_stack([np.ones(len(age)), age, salary])
    y = target
    
    print("\n=== ìŠ¤ì¼€ì¼ë§ ì—†ëŠ” ê²½ì‚¬í•˜ê°•ë²• ===")
    
    # ê°„ë‹¨í•œ ê²½ì‚¬í•˜ê°•ë²• êµ¬í˜„
    def gradient_descent(X, y, learning_rate=0.01, iterations=1000):
        m, n = X.shape
        theta = np.zeros(n)
        cost_history = []
        
        for i in range(iterations):
            # ì˜ˆì¸¡ê°’ ê³„ì‚°
            predictions = X.dot(theta)
            # ì˜¤ì°¨ ê³„ì‚°
            errors = predictions - y
            # ë¹„ìš© í•¨ìˆ˜ (MSE)
            cost = np.sum(errors**2) / (2*m)
            cost_history.append(cost)
            
            # ê¸°ìš¸ê¸° ê³„ì‚°
            gradients = X.T.dot(errors) / m
            # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
            theta = theta - learning_rate * gradients
            
            # ë¹„ìš©ì´ ë°œì‚°í•˜ë©´ ì¤‘ë‹¨
            if cost > 1e10:
                print(f"  ë°œì‚° ë°œìƒ! {i+1}ë²ˆì§¸ ë°˜ë³µì—ì„œ ì¤‘ë‹¨")
                break
                
            if i % 200 == 0:
                print(f"  ë°˜ë³µ {i}: ë¹„ìš© = {cost:.6f}")
        
        return theta, cost_history
    
    # ë†’ì€ í•™ìŠµë¥ ë¡œ í…ŒìŠ¤íŠ¸ (ë¬¸ì œ ìƒí™© ì¬í˜„)
    try:
        theta_raw, costs_raw = gradient_descent(X_raw, y, learning_rate=0.01)
        print(f"  ìµœì¢… íŒŒë¼ë¯¸í„°: {theta_raw}")
    except:
        print("  âŒ í•™ìŠµ ì‹¤íŒ¨: ê¸°ìš¸ê¸° í­ë°œ ë°œìƒ!")

demonstrate_gradient_descent_without_scaling()
```

### 2.3 ì „ì²˜ë¦¬ í›„ ê²½ì‚¬í•˜ê°•ë²• ê°œì„ 
```python
def demonstrate_gradient_descent_with_scaling():
    """ìŠ¤ì¼€ì¼ë§ í›„ ê²½ì‚¬í•˜ê°•ë²• ê°œì„  íš¨ê³¼"""
    age, salary, target = preprocessing_impact_on_gradient_descent()
    
    print("\n=== ìŠ¤ì¼€ì¼ë§ í›„ ê²½ì‚¬í•˜ê°•ë²• ===")
    
    # íŠ¹ì„± ì •ê·œí™” (Min-Max)
    age_scaled = (age - age.min()) / (age.max() - age.min())
    salary_scaled = (salary - salary.min()) / (salary.max() - salary.min())
    
    # íŠ¹ì„± í–‰ë ¬ êµ¬ì„± (ì •ê·œí™”ëœ ë°ì´í„°)
    X_scaled = np.column_stack([np.ones(len(age)), age_scaled, salary_scaled])
    y = target
    
    print("ì •ê·œí™”ëœ ë°ì´í„° ìŠ¤ì¼€ì¼:")
    print(f"ë‚˜ì´ ë²”ìœ„: {age_scaled.min():.3f} ~ {age_scaled.max():.3f}")
    print(f"ì—°ë´‰ ë²”ìœ„: {salary_scaled.min():.3f} ~ {salary_scaled.max():.3f}")
    
    # ê²½ì‚¬í•˜ê°•ë²• (ë™ì¼í•œ í•¨ìˆ˜ ì¬ì‚¬ìš©)
    def gradient_descent(X, y, learning_rate=0.01, iterations=1000):
        m, n = X.shape
        theta = np.zeros(n)
        cost_history = []
        
        for i in range(iterations):
            predictions = X.dot(theta)
            errors = predictions - y
            cost = np.sum(errors**2) / (2*m)
            cost_history.append(cost)
            
            gradients = X.T.dot(errors) / m
            theta = theta - learning_rate * gradients
            
            if i % 200 == 0:
                print(f"  ë°˜ë³µ {i}: ë¹„ìš© = {cost:.6f}")
        
        return theta, cost_history
    
    # ë™ì¼í•œ í•™ìŠµë¥ ë¡œ í…ŒìŠ¤íŠ¸
    theta_scaled, costs_scaled = gradient_descent(X_scaled, y, learning_rate=0.1)
    print(f"  âœ… í•™ìŠµ ì„±ê³µ!")
    print(f"  ìµœì¢… íŒŒë¼ë¯¸í„°: {theta_scaled}")
    print(f"  ìµœì¢… ë¹„ìš©: {costs_scaled[-1]:.6f}")
    
    return costs_scaled

costs = demonstrate_gradient_descent_with_scaling()
```

## 3. ë¡œê·¸ ë³€í™˜ (Logarithmic Transformation)

### 3.1 ë‹¤ì–‘í•œ ë¡œê·¸ í•¨ìˆ˜ë“¤
```python
def demonstrate_log_functions():
    """ë‹¤ì–‘í•œ ë¡œê·¸ í•¨ìˆ˜ ì‹œì—°"""
    value = 3.45
    
    print("=== ë¡œê·¸ í•¨ìˆ˜ ë¹„êµ ===")
    print(f"ì›ë³¸ ê°’: {value}")
    print(f"logâ‚‚({value}) = {np.log2(value):.6f}")   # ë°‘ì´ 2ì¸ ë¡œê·¸
    print(f"logâ‚â‚€({value}) = {np.log10(value):.6f}") # ìƒìš©ë¡œê·¸ (ë°‘ì´ 10)
    print(f"ln({value}) = {np.log(value):.6f}")      # ìì—°ë¡œê·¸ (ë°‘ì´ e)
    
    # ë¡œê·¸ í•¨ìˆ˜ ì •ë¦¬
    print("\n=== ë¡œê·¸ í•¨ìˆ˜ ì •ë¦¬ ===")
    print("np.log2()  : ë°‘ì´ 2ì¸ ë¡œê·¸ (ì»´í“¨í„° ê³¼í•™)")
    print("np.log10() : ìƒìš©ë¡œê·¸, ë°‘ì´ 10 (ì¼ë°˜ì )")
    print("np.log()   : ìì—°ë¡œê·¸, ë°‘ì´ e (ìˆ˜í•™/í†µê³„)")

demonstrate_log_functions()
```

### 3.2 ë¡œê·¸ ë³€í™˜ê³¼ ê²½ì‚¬í•˜ê°•ë²•
```python
def log_transformation_for_gradient_descent():
    """ë¡œê·¸ ë³€í™˜ì´ ê²½ì‚¬í•˜ê°•ë²•ì— ë¯¸ì¹˜ëŠ” ì˜í–¥"""
    print("=== ë¡œê·¸ ë³€í™˜ê³¼ ê²½ì‚¬í•˜ê°•ë²• ===")
    
    # ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ë°ì´í„° (ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸ì ìˆ˜)
    days = np.arange(1, 101)
    visitors = 100 * np.exp(0.05 * days) + np.random.normal(0, 100, 100)
    
    print("ì›ë³¸ ë°ì´í„° (ì§€ìˆ˜ì  ì„±ì¥):")
    print(f"ë°©ë¬¸ì ìˆ˜ ë²”ìœ„: {visitors.min():.0f} ~ {visitors.max():.0f}")
    print(f"ë°ì´í„° ìŠ¤ì¼€ì¼ ë¹„ìœ¨: {visitors.max()/visitors.min():.1f}:1")
    
    # ë¡œê·¸ ë³€í™˜
    log_visitors = np.log(visitors)
    print(f"\në¡œê·¸ ë³€í™˜ í›„:")
    print(f"ë¡œê·¸ ë°©ë¬¸ì ë²”ìœ„: {log_visitors.min():.3f} ~ {log_visitors.max():.3f}")
    print(f"ìŠ¤ì¼€ì¼ ë¹„ìœ¨: {(log_visitors.max()-log_visitors.min()):.1f} (ì°¨ì´)")
    
    print("\nê²½ì‚¬í•˜ê°•ë²•ì—ì„œì˜ ì´ì :")
    print("âœ… ê¸°ìš¸ê¸°ê°€ ì•ˆì •ì ì´ ë¨")
    print("âœ… í•™ìŠµë¥  ì„¤ì •ì´ ì‰¬ì›Œì§") 
    print("âœ… ìˆ˜ì¹˜ì  ì•ˆì •ì„± í–¥ìƒ")
    print("âœ… ì´ìƒì¹˜ì˜ ì˜í–¥ ê°ì†Œ")
    
    return days, visitors, log_visitors

log_transformation_for_gradient_descent()
```

### 3.3 ë°°ì—´ì— ëŒ€í•œ ë¡œê·¸ ë³€í™˜
```python
def log_transformation_example():
    """ë°°ì—´ ë¡œê·¸ ë³€í™˜ ì˜ˆì œ"""
    # ì›ë³¸ ë°ì´í„° (ë‹¤ì–‘í•œ í¬ê¸°ì˜ ê°’ë“¤)
    values = np.array([3.45, 34.5, 0.01, 10, 100, 1000])
    print("=== ë¡œê·¸ ë³€í™˜ ì˜ˆì œ ===")
    print(f"ì›ë³¸ ìë£Œ: {values}")
    print(f"ì›ë³¸ ë²”ìœ„: {values.min():.2f} ~ {values.max():.2f}")
    
    # ìƒìš©ë¡œê·¸ ë³€í™˜
    log10_values = np.log10(values)
    print(f"ìƒìš©ë¡œê·¸ ë³€í™˜: {log10_values}")
    print(f"ë¡œê·¸ ë³€í™˜ ë²”ìœ„: {log10_values.min():.2f} ~ {log10_values.max():.2f}")
    
    # ìì—°ë¡œê·¸ ë³€í™˜
    ln_values = np.log(values)  # ì£¼ì˜: np.logëŠ” ìì—°ë¡œê·¸!
    print(f"ìì—°ë¡œê·¸ ë³€í™˜: {ln_values}")
    print(f"ìì—°ë¡œê·¸ ë²”ìœ„: {ln_values.min():.2f} ~ {ln_values.max():.2f}")
    
    print(f"ë°°ì—´ í˜•íƒœ: {log10_values.shape}")
    
    return values, log10_values, ln_values

values, log10_vals, ln_vals = log_transformation_example()
```

## 4. ì •ê·œí™” (Normalization)

### 4.1 Min-Max ì •ê·œí™”ì™€ ê²½ì‚¬í•˜ê°•ë²•
```python
def min_max_normalization_for_gradient_descent(data):
    """Min-Max ì •ê·œí™”ì™€ ê²½ì‚¬í•˜ê°•ë²• ê´€ê³„"""
    print("=== Min-Max ì •ê·œí™”ì™€ ê²½ì‚¬í•˜ê°•ë²• ===")
    
    # ê³µì‹: (x - min) / (max - min)
    min_val = np.min(data)
    max_val = np.max(data)
    normalized = (data - min_val) / (max_val - min_val)
    
    print(f"ì›ë³¸ ë°ì´í„°: {data}")
    print(f"ì›ë³¸ ë²”ìœ„: [{min_val:.2f}, {max_val:.2f}]")
    print(f"ì •ê·œí™” ê²°ê³¼: {normalized}")
    print(f"ì •ê·œí™” ë²”ìœ„: [{normalized.min():.2f}, {normalized.max():.2f}]")
    
    print("\nê²½ì‚¬í•˜ê°•ë²•ì—ì„œì˜ ì´ì :")
    print("âœ… ëª¨ë“  íŠ¹ì„±ì´ ë™ì¼í•œ ìŠ¤ì¼€ì¼ (0~1)")
    print("âœ… ê¸°ìš¸ê¸° í¬ê¸°ê°€ ê· í˜•ì ")
    print("âœ… í•™ìŠµë¥  ì„¤ì •ì´ ìš©ì´")
    print("âœ… ë¹ ë¥¸ ìˆ˜ë ´")
    
    return normalized

# ë¡œê·¸ ë³€í™˜ëœ ë°ì´í„°ë¥¼ ì •ê·œí™”
normalized_log = min_max_normalization_for_gradient_descent(log10_vals)
```

### 4.2 ë‹¤ì–‘í•œ ë²”ìœ„ë¡œ ì •ê·œí™”
```python
def custom_range_normalization(data, target_min=0, target_max=1):
    """ì‚¬ìš©ì ì •ì˜ ë²”ìœ„ë¡œ ì •ê·œí™”"""
    # ê³µì‹: target_min + (x - min) * (target_max - target_min) / (max - min)
    min_val = np.min(data)
    max_val = np.max(data)
    
    normalized = target_min + (data - min_val) * (target_max - target_min) / (max_val - min_val)
    
    print(f"=== ì‚¬ìš©ì ì •ì˜ ë²”ìœ„ ì •ê·œí™” [{target_min}, {target_max}] ===")
    print(f"ì •ê·œí™” ê²°ê³¼: {normalized}")
    
    return normalized

# -1 ~ 1 ë²”ìœ„ë¡œ ì •ê·œí™” (tanh í™œì„±í™” í•¨ìˆ˜ì— ì í•©)
normalized_custom = custom_range_normalization(values, -1, 1)
```

## 5. í‘œì¤€í™” (Standardization/Z-score Normalization)

### 5.1 Z-score í‘œì¤€í™”ì™€ ê²½ì‚¬í•˜ê°•ë²•
```python
def z_score_standardization_for_gradient_descent(data):
    """Z-score í‘œì¤€í™”ì™€ ê²½ì‚¬í•˜ê°•ë²• ê´€ê³„"""
    print("=== Z-score í‘œì¤€í™”ì™€ ê²½ì‚¬í•˜ê°•ë²• ===")
    
    # ê³µì‹: (x - í‰ê· ) / í‘œì¤€í¸ì°¨
    mean_val = np.mean(data)
    std_val = np.std(data)
    standardized = (data - mean_val) / std_val
    
    print(f"ì›ë³¸ ë°ì´í„°: {data}")
    print(f"ì›ë³¸ í‰ê· : {mean_val:.2f}, í‘œì¤€í¸ì°¨: {std_val:.2f}")
    print(f"í‘œì¤€í™” ê²°ê³¼: {standardized}")
    print(f"í‘œì¤€í™” í‰ê· : {np.mean(standardized):.6f}")
    print(f"í‘œì¤€í™” í‘œì¤€í¸ì°¨: {np.std(standardized):.6f}")
    
    print("\nê²½ì‚¬í•˜ê°•ë²•ì—ì„œì˜ ì´ì :")
    print("âœ… í‰ê· ì´ 0, í‘œì¤€í¸ì°¨ê°€ 1")
    print("âœ… ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ë°ì´í„°ì— ìµœì ")
    print("âœ… ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì™€ ì˜ ë§ìŒ")
    print("âœ… ë°°ì¹˜ ì •ê·œí™”ì™€ ìœ ì‚¬í•œ íš¨ê³¼")
    
    return standardized

# ì›ë³¸ ë°ì´í„° í‘œì¤€í™”
standardized_data = z_score_standardization_for_gradient_descent(values)
```

### 5.2 ë¡œë²„ìŠ¤íŠ¸ í‘œì¤€í™”
```python
def robust_standardization(data):
    """ë¡œë²„ìŠ¤íŠ¸ í‘œì¤€í™”: ì¤‘ì•™ê°’ê³¼ IQR ì‚¬ìš© (ì´ìƒì¹˜ì— ê°•í•¨)"""
    print("=== ë¡œë²„ìŠ¤íŠ¸ í‘œì¤€í™” ===")
    
    # ê³µì‹: (x - ì¤‘ì•™ê°’) / IQR
    median_val = np.median(data)
    q75 = np.percentile(data, 75)
    q25 = np.percentile(data, 25)
    iqr = q75 - q25
    
    robust_scaled = (data - median_val) / iqr
    
    print(f"ì¤‘ì•™ê°’: {median_val:.2f}")
    print(f"IQR (Q3-Q1): {iqr:.2f}")
    print(f"ë¡œë²„ìŠ¤íŠ¸ í‘œì¤€í™” ê²°ê³¼: {robust_scaled}")
    
    print("\nê²½ì‚¬í•˜ê°•ë²•ì—ì„œì˜ ì´ì :")
    print("âœ… ì´ìƒì¹˜ì— ê°•í•¨")
    print("âœ… ì•ˆì •ì ì¸ ê¸°ìš¸ê¸°")
    print("âœ… ë¹„ì •ê·œë¶„í¬ ë°ì´í„°ì— ì í•©")
    
    return robust_scaled

# ì´ìƒì¹˜ê°€ ìˆëŠ” ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
data_with_outliers = np.array([1, 2, 3, 4, 5, 100])  # 100ì´ ì´ìƒì¹˜
robust_scaled = robust_standardization(data_with_outliers)
```

## 6. ê²½ì‚¬í•˜ê°•ë²•ì„ ìœ„í•œ ì „ì²˜ë¦¬ ì „ëµ

### 6.1 ê²½ì‚¬í•˜ê°•ë²• ìµœì í™”ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ ì„ íƒ
```python
def preprocessing_strategy_for_gradient_descent():
    """ê²½ì‚¬í•˜ê°•ë²•ì„ ìœ„í•œ ì „ì²˜ë¦¬ ì „ëµ"""
    print("=== ê²½ì‚¬í•˜ê°•ë²•ì„ ìœ„í•œ ì „ì²˜ë¦¬ ì „ëµ ===")
    
    print("1. íŠ¹ì„± ìŠ¤ì¼€ì¼ ë¶„ì„:")
    print("   ğŸ“Š ëª¨ë“  íŠ¹ì„±ì˜ ë²”ìœ„ì™€ ë¶„í¬ í™•ì¸")
    print("   ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„")
    print("   ğŸ“Š ì´ìƒì¹˜ íƒì§€")
    
    print("\n2. ì „ì²˜ë¦¬ ë°©ë²• ì„ íƒ:")
    print("   ğŸ”„ ìŠ¤ì¼€ì¼ ì°¨ì´ > 100ë°° â†’ ë¡œê·¸ ë³€í™˜ ê³ ë ¤")
    print("   ğŸ”„ ì •ê·œë¶„í¬ â†’ Z-score í‘œì¤€í™”")
    print("   ğŸ”„ ê· ë“±ë¶„í¬ â†’ Min-Max ì •ê·œí™”")
    print("   ğŸ”„ ì´ìƒì¹˜ ë§ìŒ â†’ Robust Scaling")
    
    print("\n3. ê²½ì‚¬í•˜ê°•ë²• ê´€ì ì—ì„œ:")
    print("   âš¡ ë¹ ë¥¸ ìˆ˜ë ´: Min-Max (0~1)")
    print("   âš¡ ì•ˆì •ì  í•™ìŠµ: Z-score")
    print("   âš¡ ì´ìƒì¹˜ ëŒ€ì‘: Robust")
    print("   âš¡ ì§€ìˆ˜ì  ë°ì´í„°: Log + Standard")

preprocessing_strategy_for_gradient_descent()
```

### 6.2 ì „ì²˜ë¦¬ë³„ í•™ìŠµë¥  ì„¤ì • ê°€ì´ë“œ
```python
def learning_rate_guide_by_preprocessing():
    """ì „ì²˜ë¦¬ ë°©ë²•ë³„ í•™ìŠµë¥  ì„¤ì • ê°€ì´ë“œ"""
    print("=== ì „ì²˜ë¦¬ë³„ í•™ìŠµë¥  ì„¤ì • ê°€ì´ë“œ ===")
    
    preprocessing_methods = {
        "ì›ë³¸ ë°ì´í„° (ìŠ¤ì¼€ì¼ ì°¨ì´ í¼)": {
            "í•™ìŠµë¥ ": "0.00001 ~ 0.0001",
            "ë¬¸ì œì ": "ìˆ˜ë ´ ëŠë¦¼, ë°œì‚° ìœ„í—˜",
            "ê¶Œì¥": "ì „ì²˜ë¦¬ í•„ìˆ˜"
        },
        "Min-Max ì •ê·œí™” (0~1)": {
            "í•™ìŠµë¥ ": "0.01 ~ 0.1",
            "ì¥ì ": "ì•ˆì •ì , ë¹ ë¥¸ ìˆ˜ë ´",
            "ê¶Œì¥": "ì‹ ê²½ë§, ê±°ë¦¬ ê¸°ë°˜"
        },
        "Z-score í‘œì¤€í™”": {
            "í•™ìŠµë¥ ": "0.001 ~ 0.01",
            "ì¥ì ": "ì´ë¡ ì  ê·¼ê±° ê°•í•¨",
            "ê¶Œì¥": "ì„ í˜• ëª¨ë¸, ì •ê·œë¶„í¬"
        },
        "ë¡œê·¸ + í‘œì¤€í™”": {
            "í•™ìŠµë¥ ": "0.001 ~ 0.01",
            "ì¥ì ": "ì™œê³¡ ë¶„í¬ ì²˜ë¦¬",
            "ê¶Œì¥": "ì§€ìˆ˜ì  ì„±ì¥ ë°ì´í„°"
        }
    }
    
    for method, info in preprocessing_methods.items():
        print(f"\n{method}:")
        for key, value in info.items():
            print(f"  {key}: {value}")

learning_rate_guide_by_preprocessing()
```

### 6.3 ì‹¤ì œ ê²½ì‚¬í•˜ê°•ë²• êµ¬í˜„ ì˜ˆì œ
```python
def complete_gradient_descent_example():
    """ì™„ì „í•œ ê²½ì‚¬í•˜ê°•ë²• êµ¬í˜„ ì˜ˆì œ"""
    print("=== ì™„ì „í•œ ê²½ì‚¬í•˜ê°•ë²• ì˜ˆì œ ===")
    
    # ê°€ìƒì˜ ë°ì´í„° ìƒì„± (ì§‘ê°’ ì˜ˆì¸¡)
    np.random.seed(42)
    n_samples = 200
    
    # ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ íŠ¹ì„±ë“¤
    size = np.random.uniform(20, 200, n_samples)      # í‰ë°©ë¯¸í„° (20-200)
    age = np.random.uniform(1, 50, n_samples)         # ì—°ì‹ (1-50ë…„)
    distance = np.random.uniform(0.1, 20, n_samples)  # ê±°ë¦¬ (0.1-20km)
    
    # ì‹¤ì œ ê´€ê³„ì‹ (ì§‘ê°’ = í¬ê¸°*2 - ì—°ì‹*0.5 - ê±°ë¦¬*3 + ë…¸ì´ì¦ˆ)
    true_price = size * 2 - age * 0.5 - distance * 3 + np.random.normal(0, 10, n_samples)
    
    print("ì›ë³¸ ë°ì´í„° ìŠ¤ì¼€ì¼:")
    print(f"í¬ê¸°: {size.min():.1f} ~ {size.max():.1f}")
    print(f"ì—°ì‹: {age.min():.1f} ~ {age.max():.1f}")  
    print(f"ê±°ë¦¬: {distance.min():.1f} ~ {distance.max():.1f}")
    
    # ì „ì²˜ë¦¬: Min-Max ì •ê·œí™”
    def normalize_feature(feature):
        return (feature - feature.min()) / (feature.max() - feature.min())
    
    size_norm = normalize_feature(size)
    age_norm = normalize_feature(age)
    distance_norm = normalize_feature(distance)
    
    # íŠ¹ì„± í–‰ë ¬ êµ¬ì„± (bias í¬í•¨)
    X = np.column_stack([np.ones(n_samples), size_norm, age_norm, distance_norm])
    y = true_price
    
    # ê²½ì‚¬í•˜ê°•ë²• êµ¬í˜„
    def gradient_descent_with_history(X, y, learning_rate=0.1, iterations=1000):
        m, n = X.shape
        theta = np.random.normal(0, 0.01, n)  # ì‘ì€ ëœë¤ ì´ˆê¸°í™”
        
        cost_history = []
        theta_history = []
        
        for i in range(iterations):
            # ìˆœì „íŒŒ
            predictions = X.dot(theta)
            errors = predictions - y
            cost = np.sum(errors**2) / (2*m)
            
            # ì—­ì „íŒŒ (ê¸°ìš¸ê¸° ê³„ì‚°)
            gradients = X.T.dot(errors) / m
            
            # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
            theta = theta - learning_rate * gradients
            
            # ê¸°ë¡
            cost_history.append(cost)
            theta_history.append(theta.copy())
            
            if i % 100 == 0:
                print(f"ë°˜ë³µ {i:4d}: ë¹„ìš© = {cost:10.2f}, ê¸°ìš¸ê¸° í¬ê¸° = {np.linalg.norm(gradients):.6f}")
        
        return theta, cost_history, theta_history
    
    # í•™ìŠµ ì‹¤í–‰
    final_theta, costs, theta_hist = gradient_descent_with_history(X, y)
    
    print(f"\nìµœì¢… íŒŒë¼ë¯¸í„°: {final_theta}")
    print(f"ìµœì¢… ë¹„ìš©: {costs[-1]:.2f}")
    print(f"ìˆ˜ë ´ ì—¬ë¶€: {'âœ… ìˆ˜ë ´' if costs[-1] < costs[10] else 'âŒ ë°œì‚°'}")
    
    return X, y, final_theta, costs

# ì‹¤í–‰
X, y, theta, costs = complete_gradient_descent_example()
```

## 7. ë¡œê·¸ ë³€í™˜ì˜ ì—­ë³€í™˜ (Inverse Transformation) ì™„ì „ ê°€ì´ë“œ

### 7.1 ì—­ë³€í™˜ì˜ ê¸°ë³¸ ê°œë…
```python
def log_inverse_transformation_basics():
    """ë¡œê·¸ ë³€í™˜ ì—­ë³€í™˜ ê¸°ë³¸ ê°œë…"""
    print("=== ë¡œê·¸ ë³€í™˜ ì—­ë³€í™˜ ê¸°ë³¸ ê°œë… ===")
    
    # ì›ë³¸ ë°ì´í„°
    original_data = np.array([1, 10, 100, 1000, 10000])
    print(f"ì›ë³¸ ë°ì´í„°: {original_data}")
    
    # ë¡œê·¸ ë³€í™˜ (ìƒìš©ë¡œê·¸)
    log_transformed = np.log10(original_data)
    print(f"ë¡œê·¸10 ë³€í™˜: {log_transformed}")
    
    # ì—­ë³€í™˜ (10ì˜ ê±°ë“­ì œê³±)
    inverse_transformed = 10 ** log_transformed
    # ë˜ëŠ” np.power(10, log_transformed)
    print(f"ì—­ë³€í™˜ ê²°ê³¼: {inverse_transformed}")
    
    # ì •í™•ì„± ê²€ì¦
    is_equal = np.allclose(original_data, inverse_transformed)
    print(f"ì›ë³¸ê³¼ ì—­ë³€í™˜ ê²°ê³¼ ì¼ì¹˜: {is_equal}")
    
    print("\n=== ë‹¤ì–‘í•œ ë¡œê·¸ í•¨ìˆ˜ì˜ ì—­ë³€í™˜ ===")
    value = 100
    
    # ìì—°ë¡œê·¸ì™€ ì—­ë³€í™˜
    ln_val = np.log(value)
    exp_val = np.exp(ln_val)
    print(f"ìì—°ë¡œê·¸: ln({value}) = {ln_val:.6f}")
    print(f"ì§€ìˆ˜í•¨ìˆ˜: e^{ln_val:.6f} = {exp_val:.6f}")
    
    # ìƒìš©ë¡œê·¸ì™€ ì—­ë³€í™˜  
    log10_val = np.log10(value)
    power10_val = 10 ** log10_val
    print(f"ìƒìš©ë¡œê·¸: log10({value}) = {log10_val:.6f}")
    print(f"ê±°ë“­ì œê³±: 10^{log10_val:.6f} = {power10_val:.6f}")
    
    # ë°‘ì´ 2ì¸ ë¡œê·¸ì™€ ì—­ë³€í™˜
    log2_val = np.log2(value)
    power2_val = 2 ** log2_val
    print(f"ë¡œê·¸2: log2({value}) = {log2_val:.6f}")
    print(f"ê±°ë“­ì œê³±: 2^{log2_val:.6f} = {power2_val:.6f}")

log_inverse_transformation_basics()
```

### 7.2 0ê°’ê³¼ ìŒìˆ˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì˜¤í”„ì…‹ ì—­ë³€í™˜
```python
def offset_inverse_transformation():
    """ì˜¤í”„ì…‹ì„ ì‚¬ìš©í•œ ë¡œê·¸ ë³€í™˜ê³¼ ì—­ë³€í™˜"""
    print("=== ì˜¤í”„ì…‹ ë¡œê·¸ ë³€í™˜ê³¼ ì—­ë³€í™˜ ===")
    
    # 0ê³¼ ìŒìˆ˜ê°€ í¬í•¨ëœ ë°ì´í„°
    problematic_data = np.array([0, 0.1, 1, 10, 100, -5])
    print(f"ë¬¸ì œê°€ ìˆëŠ” ì›ë³¸ ë°ì´í„°: {problematic_data}")
    
    # ë°©ë²• 1: ë‹¨ìˆœ ì˜¤í”„ì…‹ ì¶”ê°€
    offset = 10  # ìµœì†Ÿê°’ë³´ë‹¤ í° ì–‘ìˆ˜
    offset_data = problematic_data + offset
    print(f"ì˜¤í”„ì…‹ ì¶”ê°€ ({offset}): {offset_data}")
    
    # ë¡œê·¸ ë³€í™˜
    log_offset = np.log(offset_data)
    print(f"ë¡œê·¸ ë³€í™˜: {log_offset}")
    
    # ì—­ë³€í™˜
    exp_result = np.exp(log_offset)
    original_recovered = exp_result - offset
    print(f"ì—­ë³€í™˜ (exp): {exp_result}")
    print(f"ì˜¤í”„ì…‹ ì œê±°: {original_recovered}")
    print(f"ì›ë³¸ ë³µì› ì •í™•ë„: {np.allclose(problematic_data, original_recovered)}")
    
    print("\në°©ë²• 2: np.log1pì™€ np.expm1 ì‚¬ìš© (ë” ì •í™•)")
    # log1p(x) = log(1+x), expm1(x) = exp(x)-1
    positive_data = np.array([0, 0.1, 1, 10, 100])  # ì–‘ìˆ˜ë§Œ
    
    log1p_result = np.log1p(positive_data)
    expm1_result = np.expm1(log1p_result)
    
    print(f"ì›ë³¸: {positive_data}")
    print(f"log1p: {log1p_result}")
    print(f"expm1: {expm1_result}")
    print(f"ì •í™•ë„: {np.allclose(positive_data, expm1_result)}")

offset_inverse_transformation()
```

### 7.3 ì™„ì „í•œ ë¡œê·¸ ë³€í™˜ í´ë˜ìŠ¤ êµ¬í˜„
```python
class LogTransformer:
    """ì™„ì „í•œ ë¡œê·¸ ë³€í™˜ ë° ì—­ë³€í™˜ í´ë˜ìŠ¤"""
    
    def __init__(self, method='log10', offset='auto'):
        """
        Parameters:
        -----------
        method : str
            'log10', 'ln', 'log2', 'log1p' ì¤‘ ì„ íƒ
        offset : float or 'auto'
            ì˜¤í”„ì…‹ ê°’, 'auto'ë©´ ìë™ ê³„ì‚°
        """
        self.method = method
        self.offset = offset
        self.fitted_offset = None
        self.is_fitted = False
        
    def fit(self, X):
        """ë³€í™˜ íŒŒë¼ë¯¸í„° í•™ìŠµ"""
        X = np.array(X)
        
        if self.method == 'log1p':
            # log1pëŠ” ì˜¤í”„ì…‹ì´ í•„ìš” ì—†ìŒ
            self.fitted_offset = 0
        else:
            if self.offset == 'auto':
                min_val = np.min(X)
                if min_val <= 0:
                    # ìµœì†Ÿê°’ì´ 0 ì´í•˜ë©´ ì ì ˆí•œ ì˜¤í”„ì…‹ ì„¤ì •
                    self.fitted_offset = abs(min_val) + 1
                else:
                    self.fitted_offset = 0
            else:
                self.fitted_offset = self.offset
                
        self.is_fitted = True
        print(f"LogTransformer í•™ìŠµ ì™„ë£Œ - ë°©ë²•: {self.method}, ì˜¤í”„ì…‹: {self.fitted_offset}")
        return self
    
    def transform(self, X):
        """ë¡œê·¸ ë³€í™˜ ìˆ˜í–‰"""
        if not self.is_fitted:
            raise ValueError("ë¨¼ì € fit()ì„ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")
            
        X = np.array(X)
        X_shifted = X + self.fitted_offset
        
        if self.method == 'log10':
            return np.log10(X_shifted)
        elif self.method == 'ln':
            return np.log(X_shifted)
        elif self.method == 'log2':
            return np.log2(X_shifted)
        elif self.method == 'log1p':
            return np.log1p(X)  # log1pëŠ” ì˜¤í”„ì…‹ ë¶ˆí•„ìš”
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•: {self.method}")
    
    def inverse_transform(self, X_transformed):
        """ì—­ë³€í™˜ ìˆ˜í–‰"""
        if not self.is_fitted:
            raise ValueError("ë¨¼ì € fit()ì„ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")
            
        X_transformed = np.array(X_transformed)
        
        if self.method == 'log10':
            X_exp = 10 ** X_transformed
        elif self.method == 'ln':
            X_exp = np.exp(X_transformed)
        elif self.method == 'log2':
            X_exp = 2 ** X_transformed
        elif self.method == 'log1p':
            return np.expm1(X_transformed)  # expm1ì€ ì˜¤í”„ì…‹ ë¶ˆí•„ìš”
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•: {self.method}")
            
        return X_exp - self.fitted_offset
    
    def fit_transform(self, X):
        """í•™ìŠµê³¼ ë³€í™˜ì„ ë™ì‹œì— ìˆ˜í–‰"""
        return self.fit(X).transform(X)

def test_log_transformer():
    """LogTransformer í…ŒìŠ¤íŠ¸"""
    print("=== LogTransformer í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ ===")
    
    # ë‹¤ì–‘í•œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    test_cases = {
        "ì–‘ìˆ˜ë§Œ": np.array([1, 10, 100, 1000]),
        "0 í¬í•¨": np.array([0, 1, 10, 100]),
        "ìŒìˆ˜ í¬í•¨": np.array([-5, 0, 5, 50, 500]),
        "ì†Œìˆ˜ì ": np.array([0.01, 0.1, 1.5, 15.7, 157.3])
    }
    
    methods = ['log10', 'ln', 'log1p']
    
    for case_name, data in test_cases.items():
        print(f"\nğŸ“Š {case_name}: {data}")
        
        for method in methods:
            try:
                # ë³€í™˜ê¸° ìƒì„± ë° í•™ìŠµ
                transformer = LogTransformer(method=method, offset='auto')
                
                # ë³€í™˜
                transformed = transformer.fit_transform(data)
                
                # ì—­ë³€í™˜
                recovered = transformer.inverse_transform(transformed)
                
                # ì •í™•ë„ ê²€ì¦
                accuracy = np.allclose(data, recovered, rtol=1e-10)
                
                print(f"  {method:6s}: ë³€í™˜ ë²”ìœ„ {transformed.min():.3f}~{transformed.max():.3f}, "
                      f"ë³µì› ì •í™•ë„ {'âœ…' if accuracy else 'âŒ'}")
                
                if not accuracy:
                    max_error = np.max(np.abs(data - recovered))
                    print(f"          ìµœëŒ€ ì˜¤ì°¨: {max_error:.2e}")
                    
            except Exception as e:
                print(f"  {method:6s}: âŒ ì˜¤ë¥˜ - {str(e)}")

test_log_transformer()
```

### 7.4 ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì—­ë³€í™˜ì˜ ì‹¤ì œ í™œìš©
```python
def ml_inverse_transformation_example():
    """ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì—­ë³€í™˜ í™œìš© ì˜ˆì œ"""
    print("=== ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì—­ë³€í™˜ í™œìš© ===")
    
    # ì§‘ê°’ ì˜ˆì¸¡ ë¬¸ì œ (íƒ€ê²Ÿì´ ë¡œê·¸ ë³€í™˜ëœ ê²½ìš°)
    np.random.seed(42)
    
    # ì‹¤ì œ ì§‘ê°’ (ë°±ë§Œì› ë‹¨ìœ„, ë„“ì€ ë²”ìœ„)
    actual_prices = np.array([
        150, 280, 450, 680, 920, 1200, 1800, 2500, 3200, 4500,
        6000, 8500, 12000, 18000, 25000, 35000, 50000
    ])
    
    print("1. ì›ë³¸ ì§‘ê°’ ë°ì´í„°:")
    print(f"   ë²”ìœ„: {actual_prices.min():,} ~ {actual_prices.max():,} ë§Œì›")
    print(f"   ìŠ¤ì¼€ì¼ ë¹„ìœ¨: {actual_prices.max()/actual_prices.min():.1f}:1")
    
    # ë¡œê·¸ ë³€í™˜ (ê²½ì‚¬í•˜ê°•ë²•ì„ ìœ„í•´)
    log_transformer = LogTransformer(method='log10', offset='auto')
    log_prices = log_transformer.fit_transform(actual_prices)
    
    print(f"\n2. ë¡œê·¸ ë³€í™˜ í›„:")
    print(f"   ë¡œê·¸ ë²”ìœ„: {log_prices.min():.3f} ~ {log_prices.max():.3f}")
    print(f"   ë¡œê·¸ ìŠ¤ì¼€ì¼ ì°¨ì´: {log_prices.max() - log_prices.min():.3f}")
    
    # ê°€ìƒì˜ ëª¨ë¸ ì˜ˆì¸¡ (ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€)
    predicted_log_prices = log_prices + np.random.normal(0, 0.05, len(log_prices))
    
    print(f"\n3. ëª¨ë¸ ì˜ˆì¸¡ (ë¡œê·¸ ê³µê°„):")
    print(f"   ì˜ˆì¸¡ ë¡œê·¸ ë²”ìœ„: {predicted_log_prices.min():.3f} ~ {predicted_log_prices.max():.3f}")
    
    # ì—­ë³€í™˜ìœ¼ë¡œ ì‹¤ì œ ê°€ê²© ë³µì›
    predicted_actual_prices = log_transformer.inverse_transform(predicted_log_prices)
    
    print(f"\n4. ì—­ë³€í™˜ëœ ì˜ˆì¸¡ ê°€ê²©:")
    print(f"   ì˜ˆì¸¡ ë²”ìœ„: {predicted_actual_prices.min():,.0f} ~ {predicted_actual_prices.max():,.0f} ë§Œì›")
    
    # ì„±ëŠ¥ í‰ê°€ (ì‹¤ì œ ìŠ¤ì¼€ì¼ì—ì„œ)
    mae_actual = np.mean(np.abs(actual_prices - predicted_actual_prices))
    mape = np.mean(np.abs((actual_prices - predicted_actual_prices) / actual_prices)) * 100
    
    print(f"\n5. ëª¨ë¸ ì„±ëŠ¥ (ì‹¤ì œ ìŠ¤ì¼€ì¼):")
    print(f"   í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE): {mae_actual:,.0f} ë§Œì›")
    print(f"   í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨ (MAPE): {mape:.2f}%")
    
    # ë¡œê·¸ ìŠ¤ì¼€ì¼ì—ì„œì˜ ì„±ëŠ¥ê³¼ ë¹„êµ
    mae_log = np.mean(np.abs(log_prices - predicted_log_prices))
    print(f"   ë¡œê·¸ ìŠ¤ì¼€ì¼ MAE: {mae_log:.4f}")
    
    print(f"\n6. ì—­ë³€í™˜ì˜ ì¤‘ìš”ì„±:")
    print(f"   âœ… ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸ë¡œ í•´ì„ ê°€ëŠ¥")
    print(f"   âœ… ì‹¤ì œ ìŠ¤ì¼€ì¼ì—ì„œ ì„±ëŠ¥ í‰ê°€")
    print(f"   âœ… ì˜ì‚¬ê²°ì •ì— ì§ì ‘ í™œìš© ê°€ëŠ¥")

ml_inverse_transformation_example()
```

### 7.5 ê²½ì‚¬í•˜ê°•ë²•ì—ì„œ ì—­ë³€í™˜ì˜ ì¤‘ìš”ì„±
```python
def gradient_descent_with_inverse_transformation():
    """ê²½ì‚¬í•˜ê°•ë²•ì—ì„œ ì—­ë³€í™˜ í™œìš© ì˜ˆì œ"""
    print("=== ê²½ì‚¬í•˜ê°•ë²•ê³¼ ì—­ë³€í™˜ ===")
    
    # ì§€ìˆ˜ì  ì„±ì¥ ë°ì´í„° ìƒì„± (ì›¹ì‚¬ì´íŠ¸ íŠ¸ë˜í”½)
    days = np.arange(1, 101)
    actual_traffic = 100 * np.exp(0.05 * days) + np.random.normal(0, 200, 100)
    actual_traffic = np.maximum(actual_traffic, 1)  # ìµœì†Œê°’ 1ë¡œ ì œí•œ
    
    print("1. ì›ë³¸ íŠ¸ë˜í”½ ë°ì´í„°:")
    print(f"   ë²”ìœ„: {actual_traffic.min():.0f} ~ {actual_traffic.max():.0f} ë°©ë¬¸ì")
    print(f"   í‰ê· : {actual_traffic.mean():.0f}, í‘œì¤€í¸ì°¨: {actual_traffic.std():.0f}")
    
    # ë¡œê·¸ ë³€í™˜
    log_transformer = LogTransformer(method='ln', offset='auto')
    log_traffic = log_transformer.fit_transform(actual_traffic)
    
    print(f"\n2. ë¡œê·¸ ë³€í™˜ í›„:")
    print(f"   ë¡œê·¸ ë²”ìœ„: {log_traffic.min():.3f} ~ {log_traffic.max():.3f}")
    print(f"   ë¡œê·¸ í‰ê· : {log_traffic.mean():.3f}, í‘œì¤€í¸ì°¨: {log_traffic.std():.3f}")
    
    # ê°„ë‹¨í•œ ì„ í˜• íšŒê·€ (ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ)
    X = np.column_stack([np.ones(len(days)), days])  # [1, day]
    y_log = log_traffic
    
    def simple_gradient_descent(X, y, learning_rate=0.01, iterations=1000):
        theta = np.zeros(X.shape[1])
        costs = []
        
        for i in range(iterations):
            predictions = X.dot(theta)
            errors = predictions - y
            cost = np.sum(errors**2) / (2*len(y))
            costs.append(cost)
            
            gradients = X.T.dot(errors) / len(y)
            theta = theta - learning_rate * gradients
            
            if i % 200 == 0:
                print(f"   ë°˜ë³µ {i}: ë¡œê·¸ ìŠ¤ì¼€ì¼ ë¹„ìš© = {cost:.6f}")
        
        return theta, costs
    
    print(f"\n3. ê²½ì‚¬í•˜ê°•ë²• í•™ìŠµ (ë¡œê·¸ ìŠ¤ì¼€ì¼):")
    theta, costs = simple_gradient_descent(X, y_log, learning_rate=0.001)
    
    # ë¡œê·¸ ìŠ¤ì¼€ì¼ì—ì„œ ì˜ˆì¸¡
    log_predictions = X.dot(theta)
    
    # ì—­ë³€í™˜ìœ¼ë¡œ ì‹¤ì œ ìŠ¤ì¼€ì¼ ì˜ˆì¸¡
    actual_predictions = log_transformer.inverse_transform(log_predictions)
    
    print(f"\n4. ê²°ê³¼ ë¹„êµ:")
    print(f"   ë¡œê·¸ ìŠ¤ì¼€ì¼ MSE: {np.mean((log_traffic - log_predictions)**2):.6f}")
    print(f"   ì‹¤ì œ ìŠ¤ì¼€ì¼ MSE: {np.mean((actual_traffic - actual_predictions)**2):,.0f}")
    print(f"   ì‹¤ì œ ìŠ¤ì¼€ì¼ MAPE: {np.mean(np.abs((actual_traffic - actual_predictions)/actual_traffic))*100:.2f}%")
    
    print(f"\n5. ì—­ë³€í™˜ì˜ ê°€ì¹˜:")
    print(f"   ğŸ“ˆ ì‹¤ì œ íŠ¸ë˜í”½ìœ¼ë¡œ ê²°ê³¼ í•´ì„")
    print(f"   ğŸ“Š ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œë¡œ ì„±ëŠ¥ ì¸¡ì •") 
    print(f"   ğŸ¯ ì‹¤ì œ ëª©í‘œì¹˜ì™€ ë¹„êµ ê°€ëŠ¥")
    
    return actual_traffic, actual_predictions, log_transformer

traffic, predictions, transformer = gradient_descent_with_inverse_transformation()
```

### 7.6 ì—­ë³€í™˜ ì‹œ ì£¼ì˜ì‚¬í•­ê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
```python
def inverse_transformation_best_practices():
    """ì—­ë³€í™˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤"""
    print("=== ì—­ë³€í™˜ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ===")
    
    print("1. ğŸ” ìˆ˜ì¹˜ì  ì •í™•ë„ ê´€ë¦¬:")
    
    # ë¶€ë™ì†Œìˆ˜ì  ì •ë°€ë„ ë¬¸ì œ
    original = np.array([1e-10, 1e-5, 1, 1e5, 1e10])
    
    # ì •ë°€ë„ê°€ ë‚®ì€ ë³€í™˜
    log_vals = np.log10(original)
    recovered_low = 10 ** log_vals
    
    # ì •ë°€ë„ê°€ ë†’ì€ ë³€í™˜ (log1p/expm1 ì‚¬ìš©)
    adjusted_original = original - 1  # log1pë¥¼ ìœ„í•´ ì¡°ì •
    log1p_vals = np.log1p(adjusted_original)
    recovered_high = np.expm1(log1p_vals) + 1
    
    print(f"   ì›ë³¸: {original}")
    print(f"   ë‚®ì€ ì •ë°€ë„ ë³µì›: {recovered_low}")
    print(f"   ë†’ì€ ì •ë°€ë„ ë³µì›: {recovered_high}")
    print(f"   ë‚®ì€ ì •ë°€ë„ ì •í™•ë„: {np.allclose(original, recovered_low)}")
    print(f"   ë†’ì€ ì •ë°€ë„ ì •í™•ë„: {np.allclose(original, recovered_high)}")
    
    print(f"\n2. âš ï¸  ì˜¤ë²„í”Œë¡œìš°/ì–¸ë”í”Œë¡œìš° ë°©ì§€:")
    
    # í° ë¡œê·¸ ê°’ë“¤
    large_log_values = np.array([-50, -10, 0, 10, 50, 100])
    
    try:
        # ì§ì ‘ ì§€ìˆ˜ ê³„ì‚° (ì˜¤ë²„í”Œë¡œìš° ìœ„í—˜)
        direct_exp = np.exp(large_log_values)
        print(f"   ì§ì ‘ exp(): {direct_exp}")
    except:
        print(f"   ì§ì ‘ exp(): ì˜¤ë²„í”Œë¡œìš° ë°œìƒ")
    
    # ì•ˆì „í•œ ë°©ë²•: í´ë¦¬í•‘
    clipped_log = np.clip(large_log_values, -50, 50)
    safe_exp = np.exp(clipped_log)
    print(f"   í´ë¦¬í•‘ í›„ exp(): {safe_exp}")
    
    print(f"\n3. ğŸ“ ë³€í™˜ íŒŒë¼ë¯¸í„° ì €ì¥:")
    
    class SafeLogTransformer:
        def __init__(self):
            self.transform_params = {}
            
        def fit_transform_with_save(self, data, method='log10'):
            # ë³€í™˜ íŒŒë¼ë¯¸í„° ì €ì¥
            if method == 'log10':
                min_val = np.min(data)
                offset = max(0, -min_val + 1e-8) if min_val <= 0 else 0
                
                self.transform_params = {
                    'method': method,
                    'offset': offset,
                    'original_min': min_val,
                    'original_max': np.max(data)
                }
                
                transformed = np.log10(data + offset)
                
            return transformed
        
        def inverse_transform_with_params(self, transformed_data):
            params = self.transform_params
            
            if params['method'] == 'log10':
                recovered = 10 ** transformed_data - params['offset']
                
            return recovered
        
        def get_transform_info(self):
            return self.transform_params
    
    # ì‚¬ìš© ì˜ˆì œ
    test_data = np.array([0.001, 0.1, 1, 10, 100])
    transformer = SafeLogTransformer()
    
    transformed = transformer.fit_transform_with_save(test_data)
    recovered = transformer.inverse_transform_with_params(transformed)
    
    print(f"   ì›ë³¸: {test_data}")
    print(f"   ë³€í™˜: {transformed}")
    print(f"   ë³µì›: {recovered}")
    print(f"   ë³€í™˜ ì •ë³´: {transformer.get_transform_info()}")
    
    print(f"\n4. ğŸš« í”í•œ ì‹¤ìˆ˜ë“¤:")
    print(f"   âŒ ë³€í™˜ íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ")
    print(f"   âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ë‹¤ë¥¸ ë³€í™˜ ì ìš©")
    print(f"   âŒ ì˜¤ë²„í”Œë¡œìš°/ì–¸ë”í”Œë¡œìš° ë¬´ì‹œ")
    print(f"   âŒ ë¶€ë™ì†Œìˆ˜ì  ì •ë°€ë„ ë¬¸ì œ ê°„ê³¼")
    
    print(f"\n5. âœ… ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤:")
    print(f"   âœ… ë³€í™˜ íŒŒë¼ë¯¸í„° í•­ìƒ ì €ì¥")
    print(f"   âœ… ì•ˆì „í•œ ìˆ˜ì¹˜ ë²”ìœ„ ìœ ì§€")
    print(f"   âœ… ì •í™•ë„ ê²€ì¦ í¬í•¨")
    print(f"   âœ… ì˜ˆì™¸ ì²˜ë¦¬ êµ¬í˜„")

inverse_transformation_best_practices()
```

## 8. ì •ê·œí™” vs í‘œì¤€í™” vs ë¡œê·¸ë³€í™˜ ë¹„êµ

### 8.1 ê²½ì‚¬í•˜ê°•ë²• ê´€ì ì—ì„œ ì¢…í•© ë¹„êµ
```python
def comprehensive_comparison_for_gradient_descent():
    """ê²½ì‚¬í•˜ê°•ë²• ê´€ì ì—ì„œ ì „ì²˜ë¦¬ ë°©ë²• ì¢…í•© ë¹„êµ"""
    print("=== ê²½ì‚¬í•˜ê°•ë²•ì„ ìœ„í•œ ì „ì²˜ë¦¬ ë°©ë²• ë¹„êµ ===")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (ë‹¤ì–‘í•œ íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„°)
    np.random.seed(42)
    
    # íŠ¹ì„±ë“¤ (ë‹¤ì–‘í•œ ë¶„í¬ì™€ ìŠ¤ì¼€ì¼)
    normal_data = np.random.normal(50, 15, 100)           # ì •ê·œë¶„í¬
    uniform_data = np.random.uniform(0, 100, 100)         # ê· ë“±ë¶„í¬  
    exponential_data = np.random.exponential(10, 100)     # ì§€ìˆ˜ë¶„í¬
    heavy_tail = np.random.pareto(1, 100) * 10           # íŒŒë ˆí† ë¶„í¬ (heavy tail)
    
    datasets = {
        "ì •ê·œë¶„í¬": normal_data,
        "ê· ë“±ë¶„í¬": uniform_data, 
        "ì§€ìˆ˜ë¶„í¬": exponential_data,
        "ê¸´ ê¼¬ë¦¬ ë¶„í¬": heavy_tail
    }
    
    methods = {
        "Min-Max": lambda x: (x - x.min()) / (x.max() - x.min()),
        "Z-score": lambda x: (x - x.mean()) / x.std(),
        "Robust": lambda x: (x - np.median(x)) / (np.percentile(x, 75) - np.percentile(x, 25)),
        "Log+Z-score": lambda x: (np.log1p(x) - np.log1p(x).mean()) / np.log1p(x).std()
    }
    
    print("ë°ì´í„°ë³„ ì „ì²˜ë¦¬ ê²°ê³¼ ë¹„êµ:")
    print("-" * 80)
    
    for data_name, data in datasets.items():
        print(f"\nğŸ“Š {data_name} (ì›ë³¸ ë²”ìœ„: {data.min():.1f}~{data.max():.1f})")
        
        for method_name, method_func in methods.items():
            try:
                processed = method_func(data)
                range_str = f"{processed.min():.2f}~{processed.max():.2f}"
                std_str = f"Ïƒ={processed.std():.2f}"
                print(f"  {method_name:12s}: ë²”ìœ„={range_str:12s}, {std_str}")
            except:
                print(f"  {method_name:12s}: âŒ ë³€í™˜ ì‹¤íŒ¨")

comprehensive_comparison_for_gradient_descent()
```

## 9. ì™„ì „í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

### 9.1 ì—­ë³€í™˜ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```python
class ComprehensivePreprocessor:
    """ì—­ë³€í™˜ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì™„ì „í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.transformers = {}
        self.feature_names = []
        self.is_fitted = False
    
    def fit_transform(self, X, feature_names=None, methods=None):
        """ë°ì´í„° í•™ìŠµ ë° ë³€í™˜"""
        X = np.array(X)
        
        if X.ndim == 1:
            X = X.reshape(-1, 1)
            
        if feature_names is None:
            feature_names = [f'feature_{i}' for i in range(X.shape[1])]
            
        if methods is None:
            methods = ['minmax'] * X.shape[1]
            
        self.feature_names = feature_names
        transformed_X = np.zeros_like(X, dtype=float)
        
        for i, (